{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "burning-clinic",
   "metadata": {
    "id": "Eg62Pmz3o83v"
   },
   "source": [
    "## Creating a Logistic Regression Model\n",
    "\n",
    "Logistic Regression is classification algorithm commonly used in machine learning. It allows us to categorize data into discrete classes (binary categories) by learning the relationship from a given set of labeled data. A logistic regression model learns a linear relationship from the given dataset and then introduces a non-linearity in the form of the Sigmoid function, described below, to return a probability value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bulgarian-collection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "  \n",
    "def sigmoid(z): \n",
    "    return 1 / (1 + np.exp( - z)) \n",
    "  \n",
    "plt.plot(np.arange(-5, 5, 0.1), sigmoid(np.arange(-5, 5, 0.1))) \n",
    "plt.title('Visualization of the Sigmoid Function') \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-transaction",
   "metadata": {
    "id": "iAsKG535pHep"
   },
   "source": [
    "## Load the dataset\n",
    "\n",
    "The Iris flower dataset is available on [Keras Dataset API](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html). \n",
    "\n",
    "The following code loads the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "extreme-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# import iris data from sklearn datasets library\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "introductory-dining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                  5.1               3.5                1.4               0.2   \n",
      "1                  4.9               3.0                1.4               0.2   \n",
      "2                  4.7               3.2                1.3               0.2   \n",
      "3                  4.6               3.1                1.5               0.2   \n",
      "4                  5.0               3.6                1.4               0.2   \n",
      "..                 ...               ...                ...               ...   \n",
      "145                6.7               3.0                5.2               2.3   \n",
      "146                6.3               2.5                5.0               1.9   \n",
      "147                6.5               3.0                5.2               2.0   \n",
      "148                6.2               3.4                5.4               2.3   \n",
      "149                5.9               3.0                5.1               1.8   \n",
      "\n",
      "     target  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "..      ...  \n",
      "145       2  \n",
      "146       2  \n",
      "147       2  \n",
      "148       2  \n",
      "149       2  \n",
      "\n",
      "[150 rows x 5 columns]\n",
      "(150, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# To use tenforflow 1.x functions, import compact v1\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.enable_eager_execution()\n",
    "# # # make unable to use tensorflow v2.x functions to avoid crash\n",
    "# tf.disable_v2_behavior()\n",
    "\n",
    "# change to pandas dataframe\n",
    "iris = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "iris = iris.astype({\"target\": int })\n",
    "\n",
    "print(iris)\n",
    "print(iris.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-premiere",
   "metadata": {
    "id": "l50X3GfjpU4r"
   },
   "source": [
    "## Explore the data \n",
    "\n",
    "Let's take a moment to understand the format of the data. Each data contains sepal length, sepal width, petal length, petal width and a corresponding species label. The label is an integer value of either 0 or 1, where 0 is a `Iris-setosa`, and 1 is a `Iris-versicolo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "coated-excerpt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the head of dataframe\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "representative-regard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f19329d3a30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a scatter plot with sepal length and width between two iris species in the dataframe\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(iris['sepal length (cm)'][:50], iris['sepal width (cm)'][:50], label='Iris-setosa')\n",
    "plt.scatter(iris['sepal length (cm)'][51:], iris['sepal width (cm)'][51:], label='Iris-versicolo')\n",
    "# plt.scatter(iris['sepal length (cm)'][101:], iris['sepal width (cm)'][101:], label='Iris-virginica')\n",
    "plt.xlabel('sepal length (cm)')\n",
    "plt.ylabel('sepal width (cm)')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-replication",
   "metadata": {
    "id": "h5bmxzE0Fobd"
   },
   "source": [
    "## Processing Data\n",
    "\n",
    "We need to process our data to split the overall dataset into a training set and a test set.\n",
    "\n",
    "The x-value from our dataset will become the features (sepal length, sepal width, petal length, petal width) and y value as species (labels -- `Iris-setosa` or `Iris-versicolo`) from the Iris dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ordinary-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = iris.drop(labels=['target'], axis=1).values\n",
    "y = iris['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-algorithm",
   "metadata": {},
   "source": [
    "Set a seed to get reproducibility for numpy and tensorflow so for the next step, our training and test dataset are split the same way each time you run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "protected-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 23\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-passion",
   "metadata": {},
   "source": [
    "Next, we split the dataset into a training set (60%, used to train our model and familiarize the model with the kind of data it will be classifying) and a test set (40%, evaluate the model and how effective it is as classifying new, unseen before data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "changed-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use random choice from numpy library to set dataset randomly\n",
    "train_data = np.random.choice(len(x), round(len(x) * 0.6), replace=False)\n",
    "test_data = np.array(list(set(range(len(x))) - set(train_data)))\n",
    "\n",
    "# separate the dataset into features and labels\n",
    "x_train = x[train_data]\n",
    "y_train = y[train_data]\n",
    "x_test = x[test_data]\n",
    "y_test = y[test_data]\n",
    "\n",
    "# the number of labels\n",
    "num_labels = 3 \n",
    "\n",
    "# the number of features: sepal length & width, petal length & width\n",
    "num_features = 4\n",
    "\n",
    "# Convert to float32.\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "\n",
    "x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-submission",
   "metadata": {},
   "source": [
    "Now, we normalize the feature values in the dataset. Normalization is optional for logistic regression, however, the main goal of normalizing features is to help the convergence of the technique used for optimization. \n",
    "\n",
    "(TODO: explain a bit more about normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "confidential-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the normalization function\n",
    "def min_max_normalization(data):\n",
    "    col_max = np.max(data, axis=0)\n",
    "    col_min = np.min(data, axis=0)\n",
    "    return np.divide(data - col_min, col_max - col_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "original-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized processing, must be placed after the data set segmentation, \n",
    "# otherwise the test set will be affected by the training set\n",
    "x_train = min_max_normalization(x_train)\n",
    "x_test = min_max_normalization(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "considered-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make y dataset shape to fit on model dimensions as multi-class classification\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, 3)\n",
    "y_test = to_categorical(y_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adopted-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-helicopter",
   "metadata": {
    "id": "LLC02j2g-llC"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "With the processed dataset, we can now start to build the model with Tensorflow and Keras. You'll notice that our model has an activation function, which defines the output range of our model. As mentioned, we will be using the Sigmoid function as our activation function to normalize our output between 0 and 1. \n",
    "\n",
    "There are a few other activation functions that you can try out and see how they affect the model. Learn about more activation functions through the [Keras documentation](https://keras.io/api/layers/activations/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sitting-april",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 15\n",
      "Trainable params: 15\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import L1L2\n",
    "\n",
    "# Set up the logistic regression model\n",
    "model = Sequential()\n",
    "# the number of class, Iris dataset has 3 classes\n",
    "output_dim = num_labels\n",
    "# input dimension = number of features your data has\n",
    "input_dim = num_features\n",
    "\n",
    "model.add(Dense(output_dim,\n",
    "                input_dim = input_dim,\n",
    "                activation='sigmoid'\n",
    "                )) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-spanish",
   "metadata": {},
   "source": [
    "Before training our model, we also need to set a few parameters: learning rate, batch size, and the number of epoch interations.\n",
    "* <b>Learning rate</b> is a hyper-parameter that controls how much we are adjusting the weights of our network with respect the loss gradient.\n",
    "* <b>Batch size</b> defines the number of samples that will be propagated through the network.\n",
    "* An <b>epoch</b> is a full iteration over our samples during model training. \n",
    "\n",
    "Feel free to adjust these parameters, especially the learning rate and number of epochs. How do they change the model's accuracy and loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "furnished-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.03\n",
    "batch_size = 32\n",
    "epochs_num = 10"
   ]
  },
  {
   "attachments": {
    "log_loss.PNG": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAABXCAYAAADf7eU1AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAEnQAABJ0Ad5mH3gAACHCSURBVHhe7Z2HuxRFtsDfn/I2mdaEgpFgAkFBJSkmQCQYSIogIkgSRIkiGRFBQDArKBkk5wUkSHYRVknu6i7IU3Hrza+mz6Wmb/dMz9w7M32H8/u++u6d6uru6qpTVaeqTlX9j1EURVEURYkRqpwoiqIoihIrVDlRFEVRFCVWqHKiKIqiKEqsUOVEURRFUZRYocqJoiiKoiixQpUTRVEURVFihSoniqIoiqLEClVOFEVRFEWJFaqcKIqiKIoSK1Q5URRFURQlVqhyoiiKoihKrFDlRFEURVGUWKHKiRIr/vvf/5rVq9fk5FatWh3oH9UpiqIo8UCVEyVWrFmz1vzvH/5ccFezVh3z+++/e7FQFEVRiokqJ0qsQEF4umPnFMXh1tvqmjVr15pt27abvXv3msOHvzUnTpww//73v82vv/5qfvvtN3sfzv2fa+fOnTM//PCD+dvWreazz+aaMWPGmTaPtzMXX/LXlHfgZs9534uFoiiKUkxUOVFix+HDh80tt96eojh07NTFKhyVAVNHP//8s1m4cJHp1Llr2TsefPCRSnuHoiiKkjuqnCixZO68eSnKCW7ChIlWsahMGFl57bVhZe/YsGGjd0WpajCi9txzPcyIkaM8H0VR8sWBAwdMiwcfNosWLfZ8KhdVTpRYghLyyiuvpignf/zTRXkzXP322yP2HYMGvZJ4t+epFIw9e/eaPXv2eL9y4+1p020eUmG6IEtMC1alUTHivHbtOu9XabMm8Z0HDhz0flVdyDNGYyu7AxVXXh40OLC8rVu/vlLKmyonSmxBuB96+NEy5QTXqNF91tYkHyxYsNDcdns9O5pS6vCNpG8cGuwzZ84k0r2u+eSTTz2f7KFBePbZ56yMLFmy1PNNwjUq0NFvjKkSCgpxnJZQtIYNH3FBNHQ7d+4yV151jbUXywaSRuS42BCHTz/9zDS/v0WVkLGKglze3fCe0PLWrNkD5o0xYyuUFnlXTujpUsiGjRhp3XDvr7ihw4bHMjOrarxLDQxZr7yqWoqC0rNnr7xU2jyzwxNPmQ8++NDzKT2Q6wdaPGQba9zy5V/mJS2jgtFytWtqmLFjx3s+uUFZvPGmmtZWKahcbty4yVx08WXm/fc/KOr3ZoK4U9fcdXejhBL+H883nFIYXSE/Xn/9DdO4SbNICgrZR90sMoxDwS0mxKduvQbmp59+8nzCKYU8++WXX8xfLro0tLwtTigs1NWzZ8/xfLKnIMqJbdgTmec2MOIXZ+WkKsa7FJk27Z2UPMDNnDnLu1q5UFHGufGqKMj1gIGD7NJp0nHR4iVF+95z53433bs/b3tZFS1LR44kp+X69u0f+j3jJ0w01apVNzt27PB84seGjRvtd8yZ857nkwrfRlp9/fXXti4ibCmM9KGk3te4qek/YGBGWSB3qYNlpAz3448/Ji8WgY2bNtk4vPnmFM8nFcmz/fsPmjenvFUSebZ37z77HZS3MHr16l2h8lawaR0yhx4bH8TfqtIAVNV4lxLkwXOJRow8EHdt9evNrl27vRBKtojSPW/e50WT6WXLlts4zJ33heeTO7NmvWuftXTZMs+nPMhRy1aP2UaN/+MGcXryqY42jkHxI8+e79nLNGx4b1k5wJXKNOT8+Qvs96xfv8HzSQ/fzagJ9xw/fsLzLSzk0xNPPm1uvKmW+ec//+n5nofOAApX48bNSirP3p09x37HkqXh5Y0OwzXX1jB9XuqbUx2jykkGqmq8S42TJ0+a+g3uLivcuMfbtg+sxJXMSK/7w48+LopMk2/tOzxpGtzVsMJ5SPxp1HlWpm+ZNGmy/e4vV6zwfOLD5s1bbNxmh4yakGcY/bIT8ooVK8vKQakoJ8jBzTXrmKef7hRJJvhuqZuPHD3q+RYWybN+/Qd4PqmgnEycOMnm2cqVq0oizyhjbMHAflGZyluP53uay6+42nzzzd89n+iocpKBqhrvUmTJ0uQ8puteH/2G5kkOiHLy7ruzi5J+Mn2Rblg4KsSf4WO+KRMsN+a9bdq0jZXcEBfJk2PHjnu+4dDoERZXKsoJdO/R037Tl19mVh6pm+9/4EEb/uDBwq/2cfMsymjP6tXnd7+uynlGul9z7XWRlhDLjt9Dh43wfKJT5ZQTnkPG8jcKvMd9F/9mc38x4k0YcengOvGJ8sxSgG8dNWp0WQEXR6/kQsQv29kg0zpvvz0t9Bny/KjvEXl0wS9I7mUZIpVXOqK8+9SpU/ZZUZYi86ymze63q0POnj3r+RYf0gebi6h1TFVQTqLknZ8VK5MjQlEMpEkzqZt37/7a8y1PUgajr0wLineQDPO7SdPmdtVKtDyLv3ISJc9OnDhp7r2vSaT0JAwj3oyyZLsaq8ooJ9z/yaefmVeGvGq15cGvDLGaW1gCkflLly4z/foNsCswGCplXpLKGKtqDAKjaucVjTdLzIgv8Sb+YfEmzkNefc2Ge+CBh+yGUtzbrv0TKeF37dplOnXqYsM82rK1ndNbsHCRGTGi9DefIh2YzpFCjqOCYMfXCwXSYNu2baZr12etfITJEg1YGKKcTJgwqZxM8/vnn89a4z3egRs/fqI5c+bnQPnH7/jx42bKlKmmbSJvhgx5zW5mR2+WqRuGdZ95plvZvcTtkUdb2dUzYRVW8pknbHkZ9frowMqc7+YbCbtmzbrAuAXBkmK+fdOmzZ5P8WFDK+JE3KIQZ+WEfGEYv1u37lY+g+KHX1CngnuRi3btOgTKtQvXpW7maAs/XKcNQNYfa9PWvNi7j3n/gw9D04vwWxPlir2OaDPenZ1sM1jWTZtx3fU3pYzOUecQ15GjXvd80hNn5YRvJ89oc6Ym2siwPDtf3tJ3KgTCUn7pDGD0nA1VQjmhAntr6tv2XipThJrdQhvc1chMTfj7P5pEJJFRQJjvI7HZvwIBGz58ZNncX5SdJCsSb+LlxpuMDYs334hhFcZ6VOrfff+9mfzmlDKDL+IBbPJDIUHZOnbsmO0x0Bj89fKrIisnfENFXTGhh3z9DTfZdBH38suDy9KolCHtyWd6ImvXrbPfPjDx7W5lIhvKXVv9utA0EeXEPy3G/1RSTZo0t/JI47J6zRrT84Veid5SU3vNDc/zv5i/wJavni+8aFauWmWNNnn2Pfc0NnPnzisnw8QVhaVmrVtCZYkyig3JBx9+ZO/nfCVXkeFZYmR48OAhzzca7733vr1v0uQ3PZ/iQx1FnIhbFOKqnJAvLJVt1bqN3YiL+CGfvzlxPHLkqPUPkk/kAbmgMfNf88N1qZv90yq//vqbmfJWsu5FsWYvDtK4QYOGpteLfcq1GTxL5JYdo+kIU8ZoM1onvmXxkiX2WovE+wQ6CPhRT0chrsoJac6qPcrbh155ox1yyxvxbZyoE4h7pnzxQ/pw31dfZbdqp2jKSdQPJIHojXEfFZ0LAkZDzXVJSJ7LUknCz3EKuhRmhA2r6okTJ0eKQ2XEmwPnXLgm8ZZCInPwhw59Y38LoxONB/68FwFheRZKl79SZ7QlqnKC4PEtubqZs971nlQ82K+CdHHd559/EdrYlQpU7CLDLCeVb3crO+QNvw4dngxND1FO6AlKGP6yUyv+GJi69yJ/KNZcc6dPduzcaarXuMHKpFs2ZJkn7yE+bhn417/+Za8xvRIE38L1VatXm3OJZ9auc5v9TRkWpIG7ulp1GyYbZOqgf/+Bnk/xIS7EKaqhrquc+BvaYsKS3jvr32UP24win/76FJlr1vwBex05SQf3Uh8R1pUNt+59c8pUzzcJ1yg/KEaSbrxzxoyZNryraFDG2G9GyhvxdvcxmT9/ob2HlWJRcJWTOOWZlEc6IeSTlDc3z6S8he1rkg7s2rh3/oKFnk80Yj9yMmNmcolg23YdAu9pk9Buuf5OQriABJU9HFyBlQrP75+JXONNA849meI9K5Fx4DYWrlBQmPAnHm5caJzdcNyPi8KqxPeTBmGOkakgf3EYFRYb0pQeGWkhjkJ16FB2veiqhHyzDC1Pm57c/8WvSLB0Ef/Bg4d4PuURQz5XOUHWZDSCVQh+ZGUCYQgLyC9+/rIh8kwvzJVTYKtyrj31dCfPJxWWGPM85J3eFmH9vWxp4Jj7dt8bBU625t4uXZ/N+t58wHcRF+JE3KJAOSQ8zp++xYR8p3MI09+ZYeOHfLp5l0k+kQuu+ztqfvhuqQ9JD0Hq3lq1b015ryCy2bvPS/Y69ijyHHeqiWviz+igC9dE9lmOH4XVMR054ZgQySNZBebPMylvL73UL+syw6AC90pbF5VYKicID3NaZGDXZ7rZe1544UXvaipSyRKOZ/Ke88rJ+XkxnoUfDo0+KtnFe2RO8eYd27d/VRa/Vq3a2DRAIDD2k3fyVxokHMPujABt2LjJvjNd3EoRvlms9cVh4+AWqrhAnMjzXBxyIDBvvmfPXpvXHTt2tt/MnLjAe0TBYHVTGDyXMPwVuflqR1IRwDFl6MctQ4yYgOx34C8b8nxOk/bL5ZYtf7PXBgx42fNJhe+lQuM+GRJmnxvJV/yjKGBh0LvnXqYe/HELgpEiyYtsndtohkEciAtxirqZWEWUE+4NimsUl6lsMW0udRFLgokftksC/iKfYas9kAuuIyfpIC5S/iWd8aMBxa9dyMihTKHJMnactBluWeNeqfv9ygnXxo6bYK8tW77c801PRZSTiuRZJkNxRty3b99uv6nbcz1s/CZPTs0zKW90irOF9OHeseOy2wU6lsoJvS0ywxUaEjkIGnGuy7AyTuYOXS1YRiDoXZ8+fdrzzUwh4838Pltw4y+O63yHvBeh7ucNAbuOd0tv9kKC9PanxbZEQYsb5I1bYWTj3ApT4Hwh5uWRZ3eo+ejRf5Slw+nT4Vt6o0gThueLbGHHJPcij36kDOFkLw4MCPlN2XArXJFv/vqRERjpYYdBHMT42d+jlQYO25RsIb24N6pysnv37nJ5EtVJo5kOvocOCXEqhHJCmgXFNYoLkosgRD6J3/79BzzfVPkMWy0l0/JRlBOpmyWd8cOYFj/iG5S/knbETxQpFk7gN2bsOC9UUt6pf5mCp5PownvGjB1v7+EIiChURDmpSJ5FWcUGxEnaLXfUim9lJRn+0inJBtKHe8eNnxCpvAmxU04QasLs37/f3sMcF7+DKjnAqJXr0sgD83mc11G3Xn1rUIeB1hNPdrR+7CAp4aJQyHjzbIwZGbp/+JGWdi6fMBQOd/dBhIjGgZET5ncvufRyG+6xNu0iCT1heGdFXFwgLuMSGjnfj/v73w97V+IH+Zur80NPju+l8nHzA0UG/5Ytg3cZFbhP7pfnU1YkHYPuxU+ui3KCLA0bNsIaZKPwoESgZBOGnnOQwizTOoyqpIMVQITzTw3J/DdyfzqHM1Uoo9wfdVrHzYdcXCYI07lzVxsn4haFiign/vhl46Ii8kk9FiSfjHiEdaaQC8JkMnTmuUEjJ9iy4Bc2ai17b4hyArQZtevcav1ZxID9hZSRKW9NDSwPM2bOstexdYtCRZSToLyI6qLClDhxI03d+MlW9biwPEsH017cy3RbNvGJnXKCIBCG3g33yCgIghJEt+e62+syrQMUgE6du9hTTp99trvVpGnIWa6XRdpYcok3YbKJN+9AWXEVGZ6BvxQQCjt+Tz3V0Xz99XlNGD8aZJZmEu7777/3roTjnwrJ1oUpXMWANKK3Q7w+/vgTz7e0Ic+Z3uGbp09/x/NNInKXaQhV5Iq/PA+k0cYFVZ6ygRlOelCEw2CQlQs8C0WC+Wrml8mbIGRfEspTOqQho+GWOMJHH31s/du3fyLFPyrSsMfJIFamIqSRzURFlJN848onHS0X8cfQPwjulfo2k0Es3y1hJd24X6aFUNCDENl3dyf+/vvjpsfzL9iGlDgykjVw4CCzdevWUDmel1BKeI4o6pmoiHJSCKS8denyjOeTREwJsJPMpbzJiKx/aiwTRVNOgjKczV0QCpYZynWWznIPhm/+e0goGQ4VYxvCoIiIYBJGXC4UKt6cdHzPvU3KCS2KB+HeeWdGWVz8BR7YaZNwUexpeEdFXFAaFAPSUSzB30ooh7nmcVWD9BcF0z1Lhryhp4r/hg3pd6xEwSScq5xwvxhqs629n3Xr1ttrrkEsUx740RuV52TKB97DPbfcekfasOwqSTi/XYk0Lhzklwui3LD6KC7INxG3KMRdORH5dBsk4il1KaeNB8G9yAUjcZlWtPA8lGGe5yp1UvfedHOtRJjydS+2KFwXg1iYOnWaadjoXvs/YcSlQ/KAJctRiLNywrdKncCeXC6Sl9g35oJ03DdnmKbzk3flhI8mIxA0t5HHD4dw8JdGusZ1yWmMO+rWLxMarkmFieGfCAx/Z3v7FbR1lhLjL6s4KPAkuDgaeA4jitK4Vka8ZaiWeP/uxJslzvizgZrEG6Mk/NgkyEXCYkVNWASFBsidA+WZ+DNFdKFsRsY3UwmxCRJ5y+8LBWRM5siRaYEpFfyQA+QvCNKJ+6UxZJRD5BmkYmeO+T//OX9kP9elHLgrShipw49Gwi1vGEYuWrzYPtufN/IsGqCweIIY2zL9IlAG8MPluomaLImOsgljoRCDePZDCkPyDucqJ9RRpGO6tCwkxJMRPeLmKpbSM2c6LiyufBtygXzwfxiSDqKcIPsia/xNqXud50h9yuovqXtB9jEJajOYag+KC7aL3PPGmLGeT3lS8+y8cuLmmb98FAvJM7e8SZ7hci1vsulhtjsy5105wZCHhhNhkI8Ux+maYoDjOgTTzTBWDoiw8RcBkooSS3BXyIAdVN3nue6qq681L/Xtn3FutzLijeCJ1uiPN7/dKRgqc4bHObPghV697WqFQYNfMbVq3WKnfETAuZ8CWb9BQzNy5Ot25KBvv/72XnaJvVDYuHGTuf6Gm+1ZHEEVR6mDzDdqdJ+VBxpZhqRFDtmZM6zCoxIXGRSHLCN/QFpikc/+IUwVMt3AniUcyIbtEyMlbnoj4x2eeDrlea7jXTNnziqXR6++OtReTzfSx7PZ2K1m4t10Amgs5Lm331HPXs8W0kVsEmh04gL5SRlGWUyXdy1aPFzWIIsj/0jn5ve3yClN8gHxYISNuFFHybQVDju5sDK7c2dyJK53n/CTbPHne8XWDnfpZVeYRvc0TqRRcgSF90vdi+0Ty2VltJr61d9m/PLLL2XP8jvajL6JzoDfsJRvoI3IlGfENTzPwm1vCo0/z2Q0Hscmi2F5lg7ShXS//Y47s74/78oJw71kEJp+ZOcMEQv8Zhc7NFkqKYxrGE3wfzAaactEYlDQGVKUZxIHDPWYTyOxEZZ0BTmneCc046B4E0/iK5o43+GPN4oLu70Sf/YhQeAJT49KwvKXERbC4NBq2eHz408+taf2+t9dqmA7xHkNHC1POlyoYCTNtI7YK8noG7IWBnKN4i0yi4yLnLtgU8IUA6MMOHaO9CsmIEa0vP/885J/GQZGyeG6f15ejOSQ+3RQRlFEJ06abL+PuHNfd2dpcTacOXPGjraheOVyf76g7GID4xpp+knmXXidRNrE6ZtYeWTlM5Fv5LPIJ8pBWF3FSAth0hmZcq9bN4sM40gjQepeZI96EkNXRgb9acRuspgC8F67I7LzTMJLx9jfZvD8gQMHR8iz8+XN77iWrh0qNNhMsvR3+PBkng0dNrxC5Q2jZu5npDfb+wtmc1IoECgSY33InDsJxFk1hNm3b5/nW3UJK+SlCg3yQw8/antf335b/M3gigFK6bhxE1IqYpRTFIErrqxWsHShx8dSX8pTGJRDyhqjL66sUg7ZIZbeY5AM00tF6eJbBe5Baed5UTe+8jPHM87LZb+GfLNx0yYbty/mZ7eTZtwgz2jYkE/JWzvSd8999vvYZC8IwmKkytEJhRxNkDZj/PgJnk8qyJ3seOzfJO98nmVn7Bk3GMGkvPnzDJvJdHmWCY5W4X6UxGwpOeWE8zJIjE2byu9wKaAFEibKyhYlPlBJsPHYtdWvz7gHQq5QMIMay7ggNh44RiuA+PI/fhxvUKiKnR4fvUa/db+LGHSzd4W/h8j5JVzbsiW1rFrFpen99hrKi7BgQXK7cBq5XL6RdGKZKs8sZOMXFVG+mKqMswymgzwmj3Ain7BgQdKWibo3bKRANufD1qiQyGoSlJQwxP7pu+++83ySlEqeSXlLzbNkeUuXZ+kgbZgOyrW8lZxywvz7pZddbucCSVBXYJhXFAMfKqlcElwpDgg689DkHYd45QNkBdsL9k2JK8dPnLBp0CNRGWIzQZxlXxGGpgvZ6FJ+nu6Y3JPi00S5Io8E4sXmW8SJ8uiuKhIIz6gLCqdbTvFPnmlS3545BbIqiOnabHZ4dpGpJIbT4wq7phLHXHqacQCZwGaBHjf5RL5K3iELYXUu4WjksZcqdL3MqCPyRiPKQgO/HG/evNkudiB+v/1WPm6lkmeULUwLouZZJsaMSW7xkE7pS0fJKSckLHPUjZs0s0OEHNXO0FLbth1sL+/mmrXt70IXACV3yFOGiRH0fA7Hy5krbBIWV6g4WYFQ784GNk06d3nGTnGxYsCtVAsF76Q8YTDL0QEokBjSUfb+/JdLbDmkPJKHQRw/ftLUuO7GlGkawm7dus0ah2O8zneyIo6dkaPudumHEafbbq9re4bFSKeo8O0vDxps2rRpF+t4hkGcmToj35kmQNnnFPZM8slpzDSQnHpdDE6ePGUbYmSOs336D3jZMwSvbQ1iMylWVT3PZs9+z9YpfDd5xmIDyl2u34NywxRzRcpbySknAgmCcQ+9JAz6MO5jhQ4CFlZRKvGDvGIPApQGjCLzlXcnTpyw9hMY2ca9gkGGqcSpPJDvYss07yYOlC8xoiVespFiJjD6vuHGmmbHjvNbY/NMjFenTZ9h5/OxNcr1G7nvue49vJ5v/KZz/JCWHNMfd0UqDOJ87Nhx83ZCiabXnEk+2SeK8j1r1uxEOM+zCIgccyQDCxdw/B+lfEmeYUBa1fOsMspbp05drEJXkfJWssqJUhrIlEWfHE7DjApH899R9077Hg4FUwoLFePixUtSjF8rk3379lvFJF/ykw9o7Nj5ePfu3EaKqhKMsOBkL6iqCg3x6NFjDAdzXsjs3LnLvPra0AoraaqcKLFl5apVdkOmx9q0rfTeCA0VmwqxRfXFl/zVKiYsMY168JqiKIqSP1Q5UWIJ2net2rfYEQ16JCgn4vgtfvQw3WuukzD8ZdUIy+TYIZLhcgzcUEhch81EVepdK4qilCqqnCixA+NFdntkRINVV1jRuy7IL8ixky+7mvqVkDCX61p+RVEUpXJR5USJFSz3btU6uc10IR0ruRhlURRFUYqPKidKrMB4cdiw5MFxletY1ug5u52236XfRl1RFEUpHKqcKIqiKIoSK1Q5URRFURQlVqhyoiiKoihKrFDlRCkIJ0+dMmvXrfN+5ZclS8uf46IoiqJUHVQ5UfLOoW++sec2uEf85xNW31Tm4W5r166zcRfHs93f7LWiKIqiVB6qnCh5hYb7qac65u0k4SA4DZcDvNjIrTJYvXqt3TelxYMPW8VH/hfHniwc964buCmKolQOqpwoeWX2nPdMy5atC95wc7ZDzxdejPxewokL4ty5361i0qjRfeXCcNgd15YsLZwCpiiKUsqocqLkDRrxVq3amEmT3/R8Csd3331nLrn0cnv0fhCnT582Q4cON+3bP2GP9mcn2Tvr32WPSQ/i4MFDVgGZMuUtz+c8bN7WuvXj5pZbbzdnz571fBVFUZRcUeVEyRtHj/7DNugHDhz0fFKRM3BcouzSmm6EQ+B63XoNzHvvfeD5nGfDxo12BGTEiFFm85Yt9p1yRk+Y/cgXX8y337Jv3z7PJxU2cUte3+/5KIqiKLmiyomSN+bOnWdHL4IUjhUrVpq77m5kG/SOnbqY1WvXmsZNmlmlIZ2BKbYd3EM4niGsXLnKHovv0ubxdna3WVeRIS6MkmRzrDn393qxj/nLRZeGKk+8i3ht3RY8UqMoiqJER5UTJW8wmnDDjTcnGvTUUY5Tp06Z6jVusFMlHPJHo96kaXPzww8/2P/DtpL/6aefzPUJxYJThus3uLssHMoD00cYqrqKSL9+A6zC4io7jJbMn7/A+xUN7E1q17kt0N4EROG56OJLdVpHURSlElDlRMkbI0aOsnYc/gYdBYRRFUDh4DeGs4Rj9cuePXvsNT+TJ0+xysWRI0dTnnH27P/ZEZohQ16zv4Xp02fYqR0Z7eD5KDD8/j3xf5jzw3Jh3tfnpX6eTypLly231xn5CVJeFEVRlOxQ5UTJG4xsJJWB8AabZbo07IcOHfJ8MvP551/Ye06cOGl/i/KwfPmX9rcg/jJyguLA7z/+6SLzhz/+JcXhj0M58iP2JJ988qnnk0rfvv3TXlcURVGyQ5UTJW/Mnj3H1KvXIO1oAg1/7Tq3Rh5xYNSjW7fuZSMgMGnSZHPFldXMjz/+aH8Lb0+bbm68qWaZckL4atfUsL95X5hz4TfvQvlgxMbPjh07zZVXXWPtZvz3KoqiKLmhyomSN778coWpcd2NZUqEgFHr423bWyXh3nubpBiyspR33fr13i9jtm3fbg4f/tb7ldzUrdo11c2LvfvY3ygE7dp1sAapfuWgd5++pnHjZinvZ1po5MjXvV+ZYZqnZq061qbF//wzZ87Yd2OLsndv8CoeRVEUJXtUOVHyxq5du+0eIq5ywP+MQjS65z4zfvxEaxMihq3vv/+BnVYRJQDbE8I2adK87Bn8RRno0uUZ+3v79q/sMxgl8fNoy9Z2lMVVKlBueOa8eZ97PuEQltU3hEehkTgcO3bMbNy4yTRr/oBVik6eTE4vKYqiKJWDKidK3jh1Krn6ZvPmLZ5PEgxlGY1A6di1e7ddqYNSgh8KgYBywiZpXBN/FI1ly5ebhx5+1Po3bXq/fcf+/an7ixCuzi23lyk+LqKgPPjgI2b1mjVmzdq1ZX9xwCGFl19xtZ3SwdVv0NCuyEHZYuSGpcUsX/aPpiiKoigVR5UTJW8w0sDIwtBhwz2fJDToXJOGnf/d335QJGTUYt269WbRosX2N0rGqFGjzSOPtC67LqAQXXTxZaEbwPEuVvv06z/AdOrc1XR9ppvp0aOn6d3nJS9EajzlrzhFURQlf6hyouSVRYuX2IPx/MpDVFAuUE5QCOR8G1lRwx4pjGYEKSAcNjho0Cs5v1dRFEUpHqqcKHkFpYJpkSCbkCigfCxdtsz+j6LxfM9eZtKkKdZmBEVlcUL58Ssgn302t0IKkaIoilJcVDlR8g5KAgaw7DuSLexp4rJ3714zfMQou8InTPnomVBgFi5c5P1SFEVRqhqqnCgFgRGUQh2KxxJmRVEUpeqiyomiKIqiKLFClRNFURRFUWKFKieKoiiKosQKVU4URVEURYkVqpwoiqIoihIrVDlRFEVRFCVWqHKiKIqiKEqsUOVEURRFUZRYocqJoiiKoiixQpUTRVEURVFihSoniqIoiqLEClVOFEVRFEWJFaqcKIqiKIoSK1Q5URRFURQlVqhyoiiKoihKrFDlRFEURVGUGGHM/wNgkglqoZrjEgAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "id": "corresponding-invite",
   "metadata": {},
   "source": [
    "### Loss function and optimizer\n",
    "\n",
    "Here, we declare a loss function for our linear regression. Why does we need one?\n",
    "\n",
    "The loss function is critical for machine & deep learning models.\n",
    "The loss function for linear regression is squared loss, but the loss function for logistic regression is Log Loss, which is defined as follows:\n",
    "\n",
    "![log_loss.PNG](attachment:log_loss.PNG)\n",
    "\n",
    "where:\n",
    "\n",
    "* (x,y)  D is the data set containing many labeled examples, which are  (x,y) pairs.\n",
    "* y is the label in a labeled example. Since this is logistic regression, every value of y must either be 0 or 1.\n",
    "* y' is the predicted value (somewhere between 0 and 1), given the set of features in x.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "thirty-possible",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Setup Callback function\n",
    "# Requires: model and validation_data (X and Y values of test data)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=[0, 1])\n",
    "\n",
    "# confusion_matrix_updates = ConfMatrixCallbackPlotter(\n",
    "#                                 model = model,\n",
    "#                                 scaler = scaler,\n",
    "#                                 validation_data = (x_test, y_test),\n",
    "#                                 original_input = x_test)\n",
    "\n",
    "import libraries.extractioncallback as excb\n",
    "\n",
    "extractor = excb.CallbackDataExtractor(\n",
    "    model = model,\n",
    "    layer = 0,\n",
    "    validation_data = (x_test, y_test),\n",
    "    rec_int_values = False,\n",
    "    is_bin = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-desert",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "We can start to train our model. We use the parameters that we set above. The `validation_split` parameter selects what fraction of our training data will be held as validation data, used to evaluate model metrics per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "living-movement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 1s 363ms/step - loss: 1.0547 - accuracy: 0.3196 - val_loss: 1.0380 - val_accuracy: 0.3929\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 1.0531 - accuracy: 0.3300 - val_loss: 1.0345 - val_accuracy: 0.3929\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.0569 - accuracy: 0.3196 - val_loss: 1.0311 - val_accuracy: 0.3929\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 1.0383 - accuracy: 0.3404 - val_loss: 1.0277 - val_accuracy: 0.3929\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1.0448 - accuracy: 0.3091 - val_loss: 1.0244 - val_accuracy: 0.3929\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 1.0182 - accuracy: 0.3300 - val_loss: 1.0212 - val_accuracy: 0.3929\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1.0296 - accuracy: 0.3300 - val_loss: 1.0180 - val_accuracy: 0.3929\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 1.0078 - accuracy: 0.3508 - val_loss: 1.0149 - val_accuracy: 0.4286\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.9936 - accuracy: 0.3824 - val_loss: 1.0118 - val_accuracy: 0.4643\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.9995 - accuracy: 0.4257 - val_loss: 1.0088 - val_accuracy: 0.4643\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs_num, validation_split=0.3, callbacks=[extractor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "distinct-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_output = extractor.get_testing_results()\n",
    "\n",
    "# all_predictions = []\n",
    "# for epoch in extractor.get_stored_predictions():\n",
    "#     epoch_predictions = []\n",
    "#     for dp in epoch:\n",
    "#         epoch_predictions.append(int(dp.argmax()))                       \n",
    "#     all_predictions.append(epoch_predictions)\n",
    "\n",
    "# extractor.set_stored_predictions(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "exotic-rebate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[    epoch           actual  prediction                       confidence_score  \\\n",
       " 0       0  [1.0, 0.0, 0.0]           2    [0.57454216, 0.6223644, 0.12807369]   \n",
       " 1       0  [0.0, 0.0, 1.0]           2    [0.23719323, 0.08595872, 0.7933723]   \n",
       " 2       0  [1.0, 0.0, 0.0]           2    [0.6273726, 0.7810868, 0.056068063]   \n",
       " 3       0  [0.0, 0.0, 1.0]           2     [0.3594209, 0.41234744, 0.5423151]   \n",
       " 4       0  [1.0, 0.0, 0.0]           2    [0.7562312, 0.79076946, 0.17297232]   \n",
       " 5       0  [1.0, 0.0, 0.0]           1   [0.80528533, 0.93925107, 0.06834459]   \n",
       " 6       0  [1.0, 0.0, 0.0]           2    [0.3963002, 0.35292006, 0.15004766]   \n",
       " 7       0  [1.0, 0.0, 0.0]           2  [0.24685133, 0.31281543, 0.062628865]   \n",
       " 8       0  [0.0, 0.0, 1.0]           2   [0.24081767, 0.16020393, 0.70473707]   \n",
       " 9       0  [0.0, 0.0, 1.0]           2      [0.4362613, 0.5112761, 0.5777887]   \n",
       " 10      0  [0.0, 0.0, 1.0]           2    [0.15821755, 0.15850103, 0.6942388]   \n",
       " 11      0  [0.0, 0.0, 1.0]           2    [0.13861644, 0.1728481, 0.65169275]   \n",
       " 12      0  [0.0, 0.0, 1.0]           2   [0.27370012, 0.22483504, 0.69164693]   \n",
       " 13      0  [1.0, 0.0, 0.0]           2   [0.49398553, 0.58874786, 0.09327054]   \n",
       " 14      0  [1.0, 0.0, 0.0]           2            [0.5982479, 0.7531699, 0.0]   \n",
       " 15      0  [0.0, 0.0, 1.0]           2     [0.29672968, 0.2747965, 0.6512865]   \n",
       " 16      0  [0.0, 0.0, 1.0]           2    [0.34223235, 0.4345764, 0.53425896]   \n",
       " 17      0  [1.0, 0.0, 0.0]           2     [0.7257587, 0.7312156, 0.21916902]   \n",
       " 18      0  [1.0, 0.0, 0.0]           2     [0.56189096, 0.5928234, 0.1504525]   \n",
       " 19      0  [1.0, 0.0, 0.0]           2     [0.5744959, 0.5551702, 0.21137273]   \n",
       " 20      0  [1.0, 0.0, 0.0]           2  [0.37697375, 0.49334133, 0.026460886]   \n",
       " 21      0  [1.0, 0.0, 0.0]           2    [0.6916424, 0.70410717, 0.16373336]   \n",
       " 22      0  [1.0, 0.0, 0.0]           2      [0.5619222, 0.513106, 0.19988358]   \n",
       " 23      0  [1.0, 0.0, 0.0]           1        [0.7798556, 0.90774, 0.0954808]   \n",
       " 24      0  [1.0, 0.0, 0.0]           2    [0.5990037, 0.63741505, 0.15216267]   \n",
       " 25      0  [1.0, 0.0, 0.0]           2    [0.5825578, 0.65268743, 0.10703766]   \n",
       " 26      0  [1.0, 0.0, 0.0]           2     [1.0000001, 1.0000001, 0.25956118]   \n",
       " 27      0  [1.0, 0.0, 0.0]           2     [0.54584706, 0.6496991, 0.1216315]   \n",
       " 28      0  [1.0, 0.0, 0.0]           2   [0.49317586, 0.53206646, 0.12821531]   \n",
       " 29      0  [1.0, 0.0, 0.0]           2   [0.64393103, 0.68394434, 0.15031064]   \n",
       " 30      0  [0.0, 1.0, 0.0]           2     [0.48210347, 0.5282272, 0.5099648]   \n",
       " 31      0  [0.0, 1.0, 0.0]           2     [0.2686442, 0.28552377, 0.5311178]   \n",
       " 32      0  [0.0, 1.0, 0.0]           2    [0.75706613, 0.8385409, 0.39515817]   \n",
       " 33      0  [0.0, 1.0, 0.0]           2    [0.39846456, 0.26922667, 0.6598517]   \n",
       " 34      0  [0.0, 1.0, 0.0]           2      [0.8672925, 0.8749193, 0.5029696]   \n",
       " 35      0  [0.0, 1.0, 0.0]           2    [0.3415779, 0.21620452, 0.63251126]   \n",
       " 36      0  [0.0, 1.0, 0.0]           2     [0.41493714, 0.5241946, 0.4529835]   \n",
       " 37      0  [0.0, 1.0, 0.0]           2    [0.3024875, 0.40714622, 0.48289788]   \n",
       " 38      0  [0.0, 1.0, 0.0]           2    [0.43067896, 0.32629454, 0.6126715]   \n",
       " 39      0  [0.0, 1.0, 0.0]           2     [0.4264902, 0.4313215, 0.54815996]   \n",
       " 40      0  [0.0, 1.0, 0.0]           2     [0.6358167, 0.57556474, 0.5114745]   \n",
       " 41      0  [0.0, 1.0, 0.0]           2     [0.6829659, 0.66049874, 0.5267571]   \n",
       " 42      0  [0.0, 1.0, 0.0]           2    [0.5588201, 0.52620065, 0.52650535]   \n",
       " 43      0  [0.0, 1.0, 0.0]           2   [0.44002354, 0.45472443, 0.61090267]   \n",
       " 44      0  [0.0, 1.0, 0.0]           2    [0.26707113, 0.35197318, 0.4474722]   \n",
       " 45      0  [0.0, 1.0, 0.0]           2     [0.3150934, 0.21227717, 0.6442176]   \n",
       " 46      0  [0.0, 1.0, 0.0]           2      [0.5679976, 0.6178161, 0.5085205]   \n",
       " 47      0  [0.0, 1.0, 0.0]           2   [0.38736022, 0.38614035, 0.54869115]   \n",
       " 48      0  [0.0, 1.0, 0.0]           2    [0.54152954, 0.5772716, 0.50105655]   \n",
       " 49      0  [0.0, 1.0, 0.0]           2    [0.4729973, 0.51666963, 0.47893965]   \n",
       " 50      0  [0.0, 1.0, 0.0]           2      [0.7326709, 0.7701129, 0.4029113]   \n",
       " 51      0  [0.0, 0.0, 1.0]           2   [0.31693852, 0.33559108, 0.64592874]   \n",
       " 52      0  [0.0, 0.0, 1.0]           2     [0.2179426, 0.06997788, 0.8495666]   \n",
       " 53      0  [0.0, 0.0, 1.0]           2    [0.40684235, 0.29002452, 0.8039342]   \n",
       " 54      0  [0.0, 0.0, 1.0]           2                 [0.0, 0.0, 0.69705045]   \n",
       " 55      0  [0.0, 0.0, 1.0]           2    [0.2837876, 0.26897907, 0.65912616]   \n",
       " 56      0  [0.0, 0.0, 1.0]           2   [0.22455156, 0.010720968, 0.9999999]   \n",
       " 57      0  [0.0, 0.0, 1.0]           2     [0.6113354, 0.53738105, 0.7060081]   \n",
       " 58      0  [0.0, 0.0, 1.0]           2    [0.43092763, 0.37829387, 0.6616436]   \n",
       " 59      0  [0.0, 0.0, 1.0]           2   [0.15591753, 0.17048109, 0.64657485]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction                       confidence_score  \\\n",
       " 0       1  [1.0, 0.0, 0.0]           2      [0.5833194, 0.6187848, 0.1274811]   \n",
       " 1       1  [0.0, 0.0, 1.0]           2    [0.23526883, 0.08579016, 0.7926129]   \n",
       " 2       1  [1.0, 0.0, 0.0]           2   [0.63541126, 0.7785548, 0.056262493]   \n",
       " 3       1  [0.0, 0.0, 1.0]           2    [0.35794878, 0.41403925, 0.5425767]   \n",
       " 4       1  [1.0, 0.0, 0.0]           2     [0.7618356, 0.7885121, 0.17305005]   \n",
       " 5       1  [1.0, 0.0, 0.0]           1    [0.8113084, 0.93737566, 0.06904435]   \n",
       " 6       1  [1.0, 0.0, 0.0]           2   [0.40827894, 0.34717596, 0.14820516]   \n",
       " 7       1  [1.0, 0.0, 0.0]           2    [0.2607236, 0.30689573, 0.06080389]   \n",
       " 8       1  [0.0, 0.0, 1.0]           2   [0.23830771, 0.16109717, 0.70449483]   \n",
       " 9       1  [0.0, 0.0, 1.0]           2      [0.4322486, 0.5144347, 0.5786544]   \n",
       " 10      1  [0.0, 0.0, 1.0]           2    [0.15609622, 0.15985215, 0.6940564]   \n",
       " 11      1  [0.0, 0.0, 1.0]           2    [0.13684797, 0.17435598, 0.6515993]   \n",
       " 12      1  [0.0, 0.0, 1.0]           2    [0.27048683, 0.22642565, 0.6916925]   \n",
       " 13      1  [1.0, 0.0, 0.0]           2    [0.5035417, 0.58516467, 0.09268451]   \n",
       " 14      1  [1.0, 0.0, 0.0]           1             [0.6082349, 0.749688, 0.0]   \n",
       " 15      1  [0.0, 0.0, 1.0]           2    [0.2943492, 0.27623868, 0.65128744]   \n",
       " 16      1  [0.0, 0.0, 1.0]           2   [0.34064507, 0.43665028, 0.53464425]   \n",
       " 17      1  [1.0, 0.0, 0.0]           2   [0.73087025, 0.72904766, 0.21908605]   \n",
       " 18      1  [1.0, 0.0, 0.0]           2   [0.57055354, 0.58919084, 0.14973748]   \n",
       " 19      1  [1.0, 0.0, 0.0]           2    [0.58181214, 0.5518695, 0.21062934]   \n",
       " 20      1  [1.0, 0.0, 0.0]           2    [0.3899827, 0.4881884, 0.025290608]   \n",
       " 21      1  [1.0, 0.0, 0.0]           2    [0.69845796, 0.7011782, 0.16347516]   \n",
       " 22      1  [1.0, 0.0, 0.0]           2     [0.5705695, 0.5089325, 0.19878924]   \n",
       " 23      1  [1.0, 0.0, 0.0]           1   [0.78543234, 0.9060911, 0.096126914]   \n",
       " 24      1  [1.0, 0.0, 0.0]           2      [0.6070111, 0.6341368, 0.1516459]   \n",
       " 25      1  [1.0, 0.0, 0.0]           2      [0.5911751, 0.649346, 0.10663974]   \n",
       " 26      1  [1.0, 0.0, 0.0]           2           [1.0, 1.0000001, 0.26069486]   \n",
       " 27      1  [1.0, 0.0, 0.0]           2     [0.5534506, 0.6471456, 0.12145281]   \n",
       " 28      1  [1.0, 0.0, 0.0]           2      [0.5031774, 0.527856, 0.12723172]   \n",
       " 29      1  [1.0, 0.0, 0.0]           2     [0.6513388, 0.6809436, 0.14998758]   \n",
       " 30      1  [0.0, 1.0, 0.0]           2     [0.48075962, 0.52976596, 0.510332]   \n",
       " 31      1  [0.0, 1.0, 0.0]           2   [0.26976943, 0.28554857, 0.53062904]   \n",
       " 32      1  [0.0, 1.0, 0.0]           2    [0.75456834, 0.8407134, 0.39640605]   \n",
       " 33      1  [0.0, 1.0, 0.0]           2    [0.39742684, 0.26910722, 0.6593169]   \n",
       " 34      1  [0.0, 1.0, 0.0]           2   [0.86160207, 0.87803996, 0.50452554]   \n",
       " 35      1  [0.0, 1.0, 0.0]           2    [0.3417909, 0.21546853, 0.63171446]   \n",
       " 36      1  [0.0, 1.0, 0.0]           2     [0.4145677, 0.5257782, 0.45338857]   \n",
       " 37      1  [0.0, 1.0, 0.0]           2    [0.30249763, 0.40851963, 0.4830469]   \n",
       " 38      1  [0.0, 1.0, 0.0]           2    [0.42988467, 0.32631075, 0.6122724]   \n",
       " 39      1  [0.0, 1.0, 0.0]           2    [0.42522883, 0.4325018, 0.54828274]   \n",
       " 40      1  [0.0, 1.0, 0.0]           2     [0.63435555, 0.5762886, 0.5117384]   \n",
       " 41      1  [0.0, 1.0, 0.0]           2    [0.6796558, 0.66240275, 0.52751935]   \n",
       " 42      1  [0.0, 1.0, 0.0]           2    [0.55729556, 0.5271994, 0.52675164]   \n",
       " 43      1  [0.0, 1.0, 0.0]           2    [0.4367137, 0.45700037, 0.61137354]   \n",
       " 44      1  [0.0, 1.0, 0.0]           2     [0.2691307, 0.3521273, 0.44714892]   \n",
       " 45      1  [0.0, 1.0, 0.0]           2     [0.3148892, 0.21191251, 0.6435262]   \n",
       " 46      1  [0.0, 1.0, 0.0]           2    [0.56558084, 0.6198827, 0.50920546]   \n",
       " 47      1  [0.0, 1.0, 0.0]           2     [0.3869915, 0.38680482, 0.5485543]   \n",
       " 48      1  [0.0, 1.0, 0.0]           2     [0.5396559, 0.5789839, 0.50159466]   \n",
       " 49      1  [0.0, 1.0, 0.0]           2    [0.47255015, 0.5177499, 0.47917593]   \n",
       " 50      1  [0.0, 1.0, 0.0]           2      [0.730737, 0.7717391, 0.40390146]   \n",
       " 51      1  [0.0, 0.0, 1.0]           2    [0.31421614, 0.3375635, 0.64611447]   \n",
       " 52      1  [0.0, 0.0, 1.0]           2  [0.21452427, 0.070501685, 0.84905875]   \n",
       " 53      1  [0.0, 0.0, 1.0]           2     [0.40090632, 0.2923963, 0.8042716]   \n",
       " 54      1  [0.0, 0.0, 1.0]           2                   [0.0, 0.0, 0.696252]   \n",
       " 55      1  [0.0, 0.0, 1.0]           2    [0.28176737, 0.27027404, 0.6590191]   \n",
       " 56      1  [0.0, 0.0, 1.0]           2   [0.21728683, 0.012348771, 0.9999999]   \n",
       " 57      1  [0.0, 0.0, 1.0]           2    [0.6045859, 0.54062426, 0.70694506]   \n",
       " 58      1  [0.0, 0.0, 1.0]           2      [0.42708135, 0.380275, 0.6619736]   \n",
       " 59      1  [0.0, 0.0, 1.0]           2    [0.15513015, 0.17130768, 0.6461977]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction                       confidence_score  \\\n",
       " 0       2  [1.0, 0.0, 0.0]           2   [0.59191585, 0.61517954, 0.12688935]   \n",
       " 1       2  [0.0, 0.0, 1.0]           2   [0.23338246, 0.085629225, 0.7918848]   \n",
       " 2       2  [1.0, 0.0, 0.0]           2    [0.6432829, 0.77599645, 0.05644512]   \n",
       " 3       2  [0.0, 0.0, 1.0]           2     [0.3564961, 0.41570592, 0.5428852]   \n",
       " 4       2  [1.0, 0.0, 0.0]           2     [0.7673235, 0.78624034, 0.1730684]   \n",
       " 5       2  [1.0, 0.0, 0.0]           1     [0.8172066, 0.9354875, 0.06967628]   \n",
       " 6       2  [1.0, 0.0, 0.0]           2     [0.42001867, 0.3414024, 0.1464206]   \n",
       " 7       2  [1.0, 0.0, 0.0]           2  [0.27432537, 0.30093598, 0.059090137]   \n",
       " 8       2  [0.0, 0.0, 1.0]           2    [0.23583817, 0.16198325, 0.7043232]   \n",
       " 9       2  [0.0, 0.0, 1.0]           2     [0.42830896, 0.5175688, 0.5795399]   \n",
       " 10      2  [0.0, 0.0, 1.0]           2   [0.15401411, 0.16119564, 0.69396484]   \n",
       " 11      2  [0.0, 0.0, 1.0]           2     [0.13510919, 0.17585015, 0.651616]   \n",
       " 12      2  [0.0, 0.0, 1.0]           2   [0.26732898, 0.22800553, 0.69180095]   \n",
       " 13      2  [1.0, 0.0, 0.0]           2    [0.51290107, 0.5815487, 0.09212911]   \n",
       " 14      2  [1.0, 0.0, 0.0]           1           [0.6180179, 0.74617743, 0.0]   \n",
       " 15      2  [0.0, 0.0, 1.0]           2   [0.29200864, 0.27766645, 0.65134203]   \n",
       " 16      2  [0.0, 0.0, 1.0]           2     [0.3390808, 0.43869758, 0.5350791]   \n",
       " 17      2  [1.0, 0.0, 0.0]           2     [0.73587465, 0.7268641, 0.2189523]   \n",
       " 18      2  [1.0, 0.0, 0.0]           2    [0.5790391, 0.58553267, 0.14902544]   \n",
       " 19      2  [1.0, 0.0, 0.0]           2    [0.5889728, 0.54854393, 0.20988715]   \n",
       " 20      2  [1.0, 0.0, 0.0]           2  [0.40273833, 0.48299932, 0.024181962]   \n",
       " 21      2  [1.0, 0.0, 0.0]           2    [0.70513105, 0.6982298, 0.16318238]   \n",
       " 22      2  [1.0, 0.0, 0.0]           2    [0.57903814, 0.5047362, 0.19769824]   \n",
       " 23      2  [1.0, 0.0, 0.0]           1     [0.7908919, 0.9044268, 0.09671199]   \n",
       " 24      2  [1.0, 0.0, 0.0]           2     [0.6148529, 0.6308346, 0.15111995]   \n",
       " 25      2  [1.0, 0.0, 0.0]           2     [0.5996134, 0.6459768, 0.10624409]   \n",
       " 26      2  [1.0, 0.0, 0.0]           2                 [1.0, 1.0, 0.26169908]   \n",
       " 27      2  [1.0, 0.0, 0.0]           2    [0.56089234, 0.64455986, 0.1212914]   \n",
       " 28      2  [1.0, 0.0, 0.0]           2    [0.5129764, 0.52361655, 0.12627292]   \n",
       " 29      2  [1.0, 0.0, 0.0]           2    [0.65859246, 0.6779206, 0.14964294]   \n",
       " 30      2  [0.0, 1.0, 0.0]           2     [0.47943544, 0.53128266, 0.510702]   \n",
       " 31      2  [0.0, 1.0, 0.0]           2    [0.2708614, 0.28555143, 0.53021157]   \n",
       " 32      2  [0.0, 1.0, 0.0]           2     [0.7521162, 0.84286785, 0.3975879]   \n",
       " 33      2  [0.0, 1.0, 0.0]           2    [0.39640188, 0.2689799, 0.65879023]   \n",
       " 34      2  [0.0, 1.0, 0.0]           2      [0.8560281, 0.8811519, 0.5059737]   \n",
       " 35      2  [0.0, 1.0, 0.0]           2   [0.34198952, 0.21472275, 0.63095033]   \n",
       " 36      2  [0.0, 1.0, 0.0]           2   [0.41419578, 0.52733254, 0.45382655]   \n",
       " 37      2  [0.0, 1.0, 0.0]           2    [0.3024969, 0.40986288, 0.48326576]   \n",
       " 38      2  [0.0, 1.0, 0.0]           2     [0.4290974, 0.32631457, 0.6118809]   \n",
       " 39      2  [0.0, 1.0, 0.0]           2     [0.42398262, 0.4336605, 0.5484263]   \n",
       " 40      2  [0.0, 1.0, 0.0]           2     [0.63291645, 0.5769975, 0.5119604]   \n",
       " 41      2  [0.0, 1.0, 0.0]           2    [0.6764078, 0.66429186, 0.52822506]   \n",
       " 42      2  [0.0, 1.0, 0.0]           2     [0.55579233, 0.5281799, 0.5269791]   \n",
       " 43      2  [0.0, 1.0, 0.0]           2    [0.43346524, 0.4592588, 0.61185133]   \n",
       " 44      2  [0.0, 1.0, 0.0]           2   [0.27113724, 0.35225105, 0.44690692]   \n",
       " 45      2  [0.0, 1.0, 0.0]           2      [0.3146801, 0.2115382, 0.6428739]   \n",
       " 46      2  [0.0, 1.0, 0.0]           2     [0.56320786, 0.6219287, 0.5098659]   \n",
       " 47      2  [0.0, 1.0, 0.0]           2    [0.38662148, 0.38744903, 0.5484456]   \n",
       " 48      2  [0.0, 1.0, 0.0]           2       [0.537812, 0.5806742, 0.5021211]   \n",
       " 49      2  [0.0, 1.0, 0.0]           2    [0.47210288, 0.5188055, 0.47942293]   \n",
       " 50      2  [0.0, 1.0, 0.0]           2      [0.7288337, 0.7733464, 0.4048375]   \n",
       " 51      2  [0.0, 0.0, 1.0]           2   [0.31154418, 0.33952057, 0.64633906]   \n",
       " 52      2  [0.0, 0.0, 1.0]           2    [0.21117854, 0.0710392, 0.84857833]   \n",
       " 53      2  [0.0, 0.0, 1.0]           2    [0.39509082, 0.29476893, 0.8045958]   \n",
       " 54      2  [0.0, 0.0, 1.0]           2                 [0.0, 0.0, 0.69559515]   \n",
       " 55      2  [0.0, 0.0, 1.0]           2      [0.27978206, 0.27155602, 0.65896]   \n",
       " 56      2  [0.0, 0.0, 1.0]           2   [0.21018267, 0.014009714, 1.0000001]   \n",
       " 57      2  [0.0, 0.0, 1.0]           2     [0.5979719, 0.5438583, 0.70782554]   \n",
       " 58      2  [0.0, 0.0, 1.0]           2    [0.42330503, 0.38224208, 0.6623131]   \n",
       " 59      2  [0.0, 0.0, 1.0]           2    [0.15435386, 0.17212212, 0.6459149]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction                       confidence_score  \\\n",
       " 0       3  [1.0, 0.0, 0.0]           2   [0.60032403, 0.61148703, 0.12630236]   \n",
       " 1       3  [0.0, 0.0, 1.0]           2   [0.23154223, 0.08546817, 0.79115534]   \n",
       " 2       3  [1.0, 0.0, 0.0]           2   [0.65098035, 0.7733699, 0.056622863]   \n",
       " 3       3  [0.0, 0.0, 1.0]           2    [0.35508215, 0.4173814, 0.54317117]   \n",
       " 4       3  [1.0, 0.0, 0.0]           2    [0.77268803, 0.7839149, 0.17308903]   \n",
       " 5       3  [1.0, 0.0, 0.0]           1   [0.82297003, 0.93355405, 0.07030356]   \n",
       " 6       3  [1.0, 0.0, 0.0]           2     [0.4315058, 0.33549833, 0.1446526]   \n",
       " 7       3  [1.0, 0.0, 0.0]           2   [0.2876345, 0.29483378, 0.057388067]   \n",
       " 8       3  [0.0, 0.0, 1.0]           2    [0.23343766, 0.16288006, 0.7041335]   \n",
       " 9       3  [0.0, 0.0, 1.0]           2      [0.4244696, 0.5207392, 0.5804002]   \n",
       " 10      3  [0.0, 0.0, 1.0]           2    [0.15198696, 0.16255665, 0.6938515]   \n",
       " 11      3  [0.0, 0.0, 1.0]           2     [0.1334182, 0.17736113, 0.6516056]   \n",
       " 12      3  [0.0, 0.0, 1.0]           2     [0.2642573, 0.22960567, 0.6918886]   \n",
       " 13      3  [1.0, 0.0, 0.0]           2   [0.52205646, 0.57784045, 0.09157431]   \n",
       " 14      3  [1.0, 0.0, 0.0]           1           [0.6275865, 0.74258053, 0.0]   \n",
       " 15      3  [0.0, 0.0, 1.0]           2     [0.2897297, 0.27910745, 0.6513779]   \n",
       " 16      3  [0.0, 0.0, 1.0]           2   [0.33755648, 0.44075882, 0.53549004]   \n",
       " 17      3  [1.0, 0.0, 0.0]           2   [0.74076784, 0.72462666, 0.21882033]   \n",
       " 18      3  [1.0, 0.0, 0.0]           2     [0.5873376, 0.5817865, 0.14831913]   \n",
       " 19      3  [1.0, 0.0, 0.0]           2   [0.59597886, 0.54513705, 0.20914865]   \n",
       " 20      3  [1.0, 0.0, 0.0]           2  [0.41521657, 0.47768533, 0.023084402]   \n",
       " 21      3  [1.0, 0.0, 0.0]           2     [0.71165645, 0.695212, 0.16289353]   \n",
       " 22      3  [1.0, 0.0, 0.0]           2     [0.5873226, 0.5004438, 0.19661725]   \n",
       " 23      3  [1.0, 0.0, 0.0]           1    [0.7962283, 0.90271986, 0.09729147]   \n",
       " 24      3  [1.0, 0.0, 0.0]           2   [0.62252176, 0.62745225, 0.15059829]   \n",
       " 25      3  [1.0, 0.0, 0.0]           2    [0.60786736, 0.6425241, 0.10584962]   \n",
       " 26      3  [1.0, 0.0, 0.0]           2     [1.0000001, 0.9999999, 0.26269817]   \n",
       " 27      3  [1.0, 0.0, 0.0]           2    [0.56817234, 0.6419014, 0.12112403]   \n",
       " 28      3  [1.0, 0.0, 0.0]           2     [0.5225607, 0.5192758, 0.12532222]   \n",
       " 29      3  [1.0, 0.0, 0.0]           2      [0.665686, 0.6748246, 0.14930117]   \n",
       " 30      3  [0.0, 1.0, 0.0]           2     [0.47814453, 0.5328058, 0.5110562]   \n",
       " 31      3  [0.0, 1.0, 0.0]           2    [0.27192914, 0.2855338, 0.52977824]   \n",
       " 32      3  [0.0, 1.0, 0.0]           2    [0.7497252, 0.84504616, 0.39875126]   \n",
       " 33      3  [0.0, 1.0, 0.0]           2    [0.39540517, 0.26884103, 0.6582608]   \n",
       " 34      3  [0.0, 1.0, 0.0]           2    [0.85058916, 0.8843111, 0.50740814]   \n",
       " 35      3  [0.0, 1.0, 0.0]           2   [0.34218776, 0.21395302, 0.63018227]   \n",
       " 36      3  [0.0, 1.0, 0.0]           2     [0.41383445, 0.5288893, 0.4542427]   \n",
       " 37      3  [0.0, 1.0, 0.0]           2   [0.30249846, 0.41120446, 0.48345995]   \n",
       " 38      3  [0.0, 1.0, 0.0]           2   [0.42833292, 0.32630575, 0.61148405]   \n",
       " 39      3  [0.0, 1.0, 0.0]           2    [0.42277014, 0.43482172, 0.5485544]   \n",
       " 40      3  [0.0, 1.0, 0.0]           2      [0.6315166, 0.5777043, 0.5121753]   \n",
       " 41      3  [0.0, 1.0, 0.0]           2     [0.67324126, 0.6662005, 0.5289197]   \n",
       " 42      3  [0.0, 1.0, 0.0]           2     [0.5543302, 0.52916133, 0.5271964]   \n",
       " 43      3  [0.0, 1.0, 0.0]           2     [0.4302975, 0.46154106, 0.6123128]   \n",
       " 44      3  [0.0, 1.0, 0.0]           2    [0.27309716, 0.35235107, 0.4466455]   \n",
       " 45      3  [0.0, 1.0, 0.0]           2    [0.31447923, 0.21114683, 0.6422155]   \n",
       " 46      3  [0.0, 1.0, 0.0]           2     [0.5608927, 0.62399256, 0.5105109]   \n",
       " 47      3  [0.0, 1.0, 0.0]           2     [0.38626206, 0.3880862, 0.5483241]   \n",
       " 48      3  [0.0, 1.0, 0.0]           2     [0.53601515, 0.5823754, 0.5026319]   \n",
       " 49      3  [0.0, 1.0, 0.0]           2     [0.47166932, 0.5198587, 0.4796543]   \n",
       " 50      3  [0.0, 1.0, 0.0]           2     [0.7269815, 0.7749659, 0.40575695]   \n",
       " 51      3  [0.0, 0.0, 1.0]           2     [0.30893862, 0.3414986, 0.6465461]   \n",
       " 52      3  [0.0, 0.0, 1.0]           2     [0.2079159, 0.07159436, 0.8480954]   \n",
       " 53      3  [0.0, 0.0, 1.0]           2    [0.38942254, 0.2971828, 0.80491304]   \n",
       " 54      3  [0.0, 0.0, 1.0]           2                 [0.0, 0.0, 0.69491553]   \n",
       " 55      3  [0.0, 0.0, 1.0]           2      [0.2778467, 0.2728492, 0.6588857]   \n",
       " 56      3  [0.0, 0.0, 1.0]           2         [0.20326602, 0.015727282, 1.0]   \n",
       " 57      3  [0.0, 0.0, 1.0]           2    [0.59152186, 0.5471419, 0.70869565]   \n",
       " 58      3  [0.0, 0.0, 1.0]           2   [0.41962564, 0.38423264, 0.66263795]   \n",
       " 59      3  [0.0, 0.0, 1.0]           2    [0.15359747, 0.17294025, 0.6456125]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction                      confidence_score  \\\n",
       " 0       4  [1.0, 0.0, 0.0]           2   [0.60855687, 0.6078006, 0.12571192]   \n",
       " 1       4  [0.0, 0.0, 1.0]           2   [0.22972596, 0.08534014, 0.7904705]   \n",
       " 2       4  [1.0, 0.0, 0.0]           1  [0.65852344, 0.77072465, 0.05678594]   \n",
       " 3       4  [0.0, 0.0, 1.0]           2    [0.3536824, 0.41900814, 0.5435411]   \n",
       " 4       4  [1.0, 0.0, 0.0]           2    [0.7779411, 0.7815968, 0.17301774]   \n",
       " 5       4  [1.0, 0.0, 0.0]           1    [0.8286215, 0.9316119, 0.07083118]   \n",
       " 6       4  [1.0, 0.0, 0.0]           2  [0.44275272, 0.32962918, 0.14295816]   \n",
       " 7       4  [1.0, 0.0, 0.0]           2  [0.3006822, 0.28874564, 0.055843353]   \n",
       " 8       4  [0.0, 0.0, 1.0]           2    [0.2310592, 0.16376519, 0.7040614]   \n",
       " 9       4  [0.0, 0.0, 1.0]           2    [0.42069733, 0.5238439, 0.5813068]   \n",
       " 10      4  [0.0, 0.0, 1.0]           2   [0.14999259, 0.16389513, 0.6938878]   \n",
       " 11      4  [0.0, 0.0, 1.0]           2   [0.13175118, 0.17883515, 0.6517793]   \n",
       " 12      4  [0.0, 0.0, 1.0]           2   [0.2612239, 0.23117971, 0.69208324]   \n",
       " 13      4  [1.0, 0.0, 0.0]           2   [0.5310224, 0.57412374, 0.09106398]   \n",
       " 14      4  [1.0, 0.0, 0.0]           1          [0.6369642, 0.73897135, 0.0]   \n",
       " 15      4  [0.0, 0.0, 1.0]           2  [0.28747904, 0.28052092, 0.65150535]   \n",
       " 16      4  [0.0, 0.0, 1.0]           2  [0.33605492, 0.44276297, 0.53599155]   \n",
       " 17      4  [1.0, 0.0, 0.0]           2    [0.7455543, 0.7223972, 0.21860778]   \n",
       " 18      4  [1.0, 0.0, 0.0]           2   [0.59546244, 0.5780498, 0.14761126]   \n",
       " 19      4  [1.0, 0.0, 0.0]           2    [0.6028267, 0.5417403, 0.20840776]   \n",
       " 20      4  [1.0, 0.0, 0.0]           2   [0.42745507, 0.4723779, 0.02207172]   \n",
       " 21      4  [1.0, 0.0, 0.0]           2    [0.7180408, 0.6922029, 0.16254854]   \n",
       " 22      4  [1.0, 0.0, 0.0]           2   [0.5954255, 0.49617636, 0.19553137]   \n",
       " 23      4  [1.0, 0.0, 0.0]           1    [0.8014592, 0.9010004, 0.09778345]   \n",
       " 24      4  [1.0, 0.0, 0.0]           2    [0.6300298, 0.62407625, 0.1500591]   \n",
       " 25      4  [1.0, 0.0, 0.0]           2   [0.61594784, 0.6390692, 0.10545683]   \n",
       " 26      4  [1.0, 0.0, 0.0]           2     [1.0000001, 0.9999999, 0.2635044]   \n",
       " 27      4  [1.0, 0.0, 0.0]           2   [0.57529557, 0.639222, 0.120985866]   \n",
       " 28      4  [1.0, 0.0, 0.0]           2   [0.5319475, 0.51494586, 0.12440193]   \n",
       " 29      4  [1.0, 0.0, 0.0]           2  [0.67262995, 0.67173326, 0.14892375]   \n",
       " 30      4  [0.0, 1.0, 0.0]           2    [0.47687137, 0.53428996, 0.511423]   \n",
       " 31      4  [0.0, 1.0, 0.0]           2   [0.27296054, 0.28549385, 0.5294596]   \n",
       " 32      4  [0.0, 1.0, 0.0]           2     [0.7473792, 0.8471774, 0.3998264]   \n",
       " 33      4  [0.0, 1.0, 0.0]           2    [0.3944062, 0.26871395, 0.6577426]   \n",
       " 34      4  [0.0, 1.0, 0.0]           2    [0.84525955, 0.8874327, 0.5086876]   \n",
       " 35      4  [0.0, 1.0, 0.0]           2    [0.3423561, 0.21319687, 0.6294633]   \n",
       " 36      4  [0.0, 1.0, 0.0]           2  [0.41347277, 0.53038967, 0.45472252]   \n",
       " 37      4  [0.0, 1.0, 0.0]           2   [0.30248892, 0.41249025, 0.4837755]   \n",
       " 38      4  [0.0, 1.0, 0.0]           2    [0.4275607, 0.32629848, 0.6110996]   \n",
       " 39      4  [0.0, 1.0, 0.0]           2  [0.42156613, 0.43594992, 0.54872143]   \n",
       " 40      4  [0.0, 1.0, 0.0]           2   [0.63012755, 0.5783969, 0.51232946]   \n",
       " 41      4  [0.0, 1.0, 0.0]           2    [0.6701268, 0.6680807, 0.52953374]   \n",
       " 42      4  [0.0, 1.0, 0.0]           2    [0.5528799, 0.5301198, 0.52738965]   \n",
       " 43      4  [0.0, 1.0, 0.0]           2     [0.427184, 0.46378338, 0.6127931]   \n",
       " 44      4  [0.0, 1.0, 0.0]           2  [0.27500665, 0.35241032, 0.44651926]   \n",
       " 45      4  [0.0, 1.0, 0.0]           2   [0.31426013, 0.21076357, 0.6416174]   \n",
       " 46      4  [0.0, 1.0, 0.0]           2   [0.55861866, 0.62601435, 0.5111276]   \n",
       " 47      4  [0.0, 1.0, 0.0]           2     [0.3858961, 0.3886999, 0.5482515]   \n",
       " 48      4  [0.0, 1.0, 0.0]           2    [0.5342432, 0.5840353, 0.50313437]   \n",
       " 49      4  [0.0, 1.0, 0.0]           2  [0.47123253, 0.52087367, 0.47991097]   \n",
       " 50      4  [0.0, 1.0, 0.0]           2   [0.7251538, 0.77654445, 0.40660536]   \n",
       " 51      4  [0.0, 0.0, 1.0]           2  [0.30637872, 0.34344196, 0.64682305]   \n",
       " 52      4  [0.0, 0.0, 1.0]           2    [0.2047118, 0.07218087, 0.8476528]   \n",
       " 53      4  [0.0, 0.0, 1.0]           2   [0.38385522, 0.29958975, 0.8052136]   \n",
       " 54      4  [0.0, 0.0, 1.0]           2                 [0.0, 0.0, 0.6944648]   \n",
       " 55      4  [0.0, 0.0, 1.0]           2    [0.2759403, 0.2741214, 0.65889204]   \n",
       " 56      4  [0.0, 0.0, 1.0]           2  [0.19648325, 0.017487407, 1.0000001]   \n",
       " 57      4  [0.0, 0.0, 1.0]           2    [0.5851909, 0.5503956, 0.70948565]   \n",
       " 58      4  [0.0, 0.0, 1.0]           2   [0.41600072, 0.38619435, 0.6629859]   \n",
       " 59      4  [0.0, 0.0, 1.0]           2   [0.15284884, 0.17373705, 0.6454648]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction                       confidence_score  \\\n",
       " 0       5  [1.0, 0.0, 0.0]           2       [0.61658, 0.6040126, 0.12513053]   \n",
       " 1       5  [0.0, 0.0, 1.0]           2     [0.22795153, 0.0851742, 0.7897891]   \n",
       " 2       5  [1.0, 0.0, 0.0]           1     [0.665874, 0.7680286, 0.056941867]   \n",
       " 3       5  [0.0, 0.0, 1.0]           2    [0.35230207, 0.42069685, 0.5438827]   \n",
       " 4       5  [1.0, 0.0, 0.0]           2    [0.7830608, 0.77921474, 0.17294633]   \n",
       " 5       5  [1.0, 0.0, 0.0]           1   [0.83413124, 0.92963517, 0.07134724]   \n",
       " 6       5  [1.0, 0.0, 0.0]           2   [0.45371532, 0.32357132, 0.14129746]   \n",
       " 7       5  [1.0, 0.0, 0.0]           2     [0.3134091, 0.2824781, 0.05432725]   \n",
       " 8       5  [0.0, 0.0, 1.0]           2   [0.22872066, 0.16467357, 0.70397174]   \n",
       " 9       5  [0.0, 0.0, 1.0]           2    [0.41700673, 0.52705944, 0.5821749]   \n",
       " 10      5  [0.0, 0.0, 1.0]           2    [0.14804149, 0.16527998, 0.6939014]   \n",
       " 11      5  [0.0, 0.0, 1.0]           2   [0.13011539, 0.18037009, 0.65192354]   \n",
       " 12      5  [0.0, 0.0, 1.0]           2    [0.25824833, 0.23280478, 0.6922544]   \n",
       " 13      5  [1.0, 0.0, 0.0]           2   [0.5397582, 0.57031405, 0.090559125]   \n",
       " 14      5  [1.0, 0.0, 0.0]           1          [0.64610434, 0.73528445, 0.0]   \n",
       " 15      5  [0.0, 0.0, 1.0]           2     [0.28526998, 0.28197885, 0.651611]   \n",
       " 16      5  [0.0, 0.0, 1.0]           2    [0.33457875, 0.44484556, 0.5364605]   \n",
       " 17      5  [1.0, 0.0, 0.0]           2    [0.7502172, 0.72010195, 0.21839643]   \n",
       " 18      5  [1.0, 0.0, 0.0]           2     [0.60338044, 0.574206, 0.14691436]   \n",
       " 19      5  [1.0, 0.0, 0.0]           2    [0.6094923, 0.53824055, 0.20767689]   \n",
       " 20      5  [1.0, 0.0, 0.0]           2  [0.43939638, 0.46692598, 0.021080017]   \n",
       " 21      5  [1.0, 0.0, 0.0]           2     [0.7242584, 0.6891092, 0.16220856]   \n",
       " 22      5  [1.0, 0.0, 0.0]           2     [0.60331726, 0.491773, 0.19446516]   \n",
       " 23      5  [1.0, 0.0, 0.0]           1    [0.8065574, 0.89925134, 0.09826243]   \n",
       " 24      5  [1.0, 0.0, 0.0]           2     [0.6373458, 0.6206058, 0.14952779]   \n",
       " 25      5  [1.0, 0.0, 0.0]           2    [0.6238198, 0.6355258, 0.105068564]   \n",
       " 26      5  [1.0, 0.0, 0.0]           2           [1.0, 1.0000001, 0.26429284]   \n",
       " 27      5  [1.0, 0.0, 0.0]           2    [0.5822284, 0.63648546, 0.12084293]   \n",
       " 28      5  [1.0, 0.0, 0.0]           2   [0.5410974, 0.51049125, 0.123497605]   \n",
       " 29      5  [1.0, 0.0, 0.0]           2    [0.67939544, 0.6685575, 0.14855099]   \n",
       " 30      5  [0.0, 1.0, 0.0]           2     [0.47562075, 0.5358232, 0.5117656]   \n",
       " 31      5  [0.0, 1.0, 0.0]           2   [0.27395296, 0.28545332, 0.52912676]   \n",
       " 32      5  [0.0, 1.0, 0.0]           2      [0.7450857, 0.849383, 0.40086734]   \n",
       " 33      5  [0.0, 1.0, 0.0]           2    [0.3934195, 0.26855695, 0.65722287]   \n",
       " 34      5  [0.0, 1.0, 0.0]           2     [0.84006214, 0.8906435, 0.5099338]   \n",
       " 35      5  [0.0, 1.0, 0.0]           2   [0.34250355, 0.21239293, 0.62874496]   \n",
       " 36      5  [0.0, 1.0, 0.0]           2    [0.4131074, 0.53195345, 0.45517194]   \n",
       " 37      5  [0.0, 1.0, 0.0]           2   [0.30246592, 0.41383493, 0.48406065]   \n",
       " 38      5  [0.0, 1.0, 0.0]           2       [0.4267931, 0.32627177, 0.61071]   \n",
       " 39      5  [0.0, 1.0, 0.0]           2   [0.42037773, 0.43711412, 0.54886854]   \n",
       " 40      5  [0.0, 1.0, 0.0]           2         [0.6287613, 0.5791, 0.5124701]   \n",
       " 41      5  [0.0, 1.0, 0.0]           2    [0.66708136, 0.6700114, 0.53012526]   \n",
       " 42      5  [0.0, 1.0, 0.0]           2     [0.55145216, 0.5311011, 0.5275663]   \n",
       " 43      5  [0.0, 1.0, 0.0]           2    [0.42413878, 0.4660958, 0.61324775]   \n",
       " 44      5  [0.0, 1.0, 0.0]           2   [0.27685475, 0.35248458, 0.44637167]   \n",
       " 45      5  [0.0, 1.0, 0.0]           2    [0.31403184, 0.21034789, 0.6410166]   \n",
       " 46      5  [0.0, 1.0, 0.0]           2       [0.5563946, 0.628101, 0.5117172]   \n",
       " 47      5  [0.0, 1.0, 0.0]           2   [0.38552713, 0.38932955, 0.54816353]   \n",
       " 48      5  [0.0, 1.0, 0.0]           2      [0.5325041, 0.5857502, 0.5036112]   \n",
       " 49      5  [0.0, 1.0, 0.0]           2    [0.47079372, 0.5219258, 0.48014557]   \n",
       " 50      5  [0.0, 1.0, 0.0]           2   [0.72335887, 0.77817905, 0.40742385]   \n",
       " 51      5  [0.0, 0.0, 1.0]           2    [0.30387545, 0.3454479, 0.64707553]   \n",
       " 52      5  [0.0, 0.0, 1.0]           2    [0.20158863, 0.07275534, 0.8472103]   \n",
       " 53      5  [0.0, 0.0, 1.0]           2   [0.37842202, 0.30205274, 0.80550015]   \n",
       " 54      5  [0.0, 0.0, 1.0]           2                  [0.0, 0.0, 0.6939961]   \n",
       " 55      5  [0.0, 0.0, 1.0]           2    [0.27407312, 0.2754295, 0.65888035]   \n",
       " 56      5  [0.0, 0.0, 1.0]           2   [0.18987823, 0.019273043, 0.9999999]   \n",
       " 57      5  [0.0, 0.0, 1.0]           2    [0.5790119, 0.55373466, 0.71025217]   \n",
       " 58      5  [0.0, 0.0, 1.0]           2     [0.4124503, 0.38821208, 0.6633123]   \n",
       " 59      5  [0.0, 0.0, 1.0]           2     [0.152112, 0.17456186, 0.64529693]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction                       confidence_score  \\\n",
       " 0       6  [1.0, 0.0, 0.0]           2    [0.62443507, 0.60023534, 0.1245476]   \n",
       " 1       6  [0.0, 0.0, 1.0]           2    [0.22622287, 0.08506191, 0.7891345]   \n",
       " 2       6  [1.0, 0.0, 0.0]           1   [0.67306864, 0.7652985, 0.057088494]   \n",
       " 3       6  [0.0, 0.0, 1.0]           2    [0.35096633, 0.42229545, 0.5442655]   \n",
       " 4       6  [1.0, 0.0, 0.0]           2    [0.78806865, 0.7768413, 0.17282534]   \n",
       " 5       6  [1.0, 0.0, 0.0]           1   [0.83951724, 0.9276408, 0.071808696]   \n",
       " 6       6  [1.0, 0.0, 0.0]           2     [0.46445596, 0.31758475, 0.139678]   \n",
       " 7       6  [1.0, 0.0, 0.0]           2  [0.32587278, 0.27624667, 0.052897334]   \n",
       " 8       6  [0.0, 0.0, 1.0]           2    [0.22646058, 0.1655612, 0.70393944]   \n",
       " 9       6  [0.0, 0.0, 1.0]           2      [0.4134165, 0.5301534, 0.5830641]   \n",
       " 10      6  [0.0, 0.0, 1.0]           2   [0.14614618, 0.16661906, 0.69399214]   \n",
       " 11      6  [0.0, 0.0, 1.0]           2    [0.12853134, 0.18183577, 0.6521611]   \n",
       " 12      6  [0.0, 0.0, 1.0]           2   [0.25536597, 0.23438096, 0.69247746]   \n",
       " 13      6  [1.0, 0.0, 0.0]           2   [0.54831374, 0.56649244, 0.09007764]   \n",
       " 14      6  [1.0, 0.0, 0.0]           1            [0.655053, 0.73157585, 0.0]   \n",
       " 15      6  [0.0, 0.0, 1.0]           2      [0.2831279, 0.2833835, 0.6517625]   \n",
       " 16      6  [0.0, 0.0, 1.0]           2    [0.33314574, 0.44682086, 0.5369754]   \n",
       " 17      6  [1.0, 0.0, 0.0]           2    [0.75478137, 0.7178172, 0.21814108]   \n",
       " 18      6  [1.0, 0.0, 0.0]           2     [0.61113226, 0.5703789, 0.1462177]   \n",
       " 19      6  [1.0, 0.0, 0.0]           2      [0.6160251, 0.534762, 0.20694256]   \n",
       " 20      6  [1.0, 0.0, 0.0]           2   [0.4510845, 0.46148694, 0.020137906]   \n",
       " 21      6  [1.0, 0.0, 0.0]           2      [0.7303475, 0.6860305, 0.1618377]   \n",
       " 22      6  [1.0, 0.0, 0.0]           2   [0.61104953, 0.48741734, 0.19339633]   \n",
       " 23      6  [1.0, 0.0, 0.0]           1    [0.81154287, 0.8974782, 0.09869242]   \n",
       " 24      6  [1.0, 0.0, 0.0]           2    [0.6445087, 0.61714613, 0.14898694]   \n",
       " 25      6  [1.0, 0.0, 0.0]           2    [0.63152945, 0.6319796, 0.10468018]   \n",
       " 26      6  [1.0, 0.0, 0.0]           0     [0.9999999, 0.9999999, 0.26497436]   \n",
       " 27      6  [1.0, 0.0, 0.0]           2   [0.58902323, 0.63371456, 0.12071216]   \n",
       " 28      6  [1.0, 0.0, 0.0]           2  [0.55005634, 0.50605786, 0.122611046]   \n",
       " 29      6  [1.0, 0.0, 0.0]           2     [0.6860188, 0.6653892, 0.14815867]   \n",
       " 30      6  [0.0, 1.0, 0.0]           2      [0.4744047, 0.5372809, 0.5121143]   \n",
       " 31      6  [0.0, 1.0, 0.0]           2   [0.27492917, 0.28537202, 0.52885365]   \n",
       " 32      6  [0.0, 1.0, 0.0]           2      [0.7428516, 0.8515047, 0.4018569]   \n",
       " 33      6  [0.0, 1.0, 0.0]           2     [0.39246666, 0.2684182, 0.6567099]   \n",
       " 34      6  [0.0, 1.0, 0.0]           2     [0.8349899, 0.8937851, 0.51109457]   \n",
       " 35      6  [0.0, 1.0, 0.0]           2    [0.34266078, 0.21161544, 0.6280525]   \n",
       " 36      6  [0.0, 1.0, 0.0]           2       [0.412758, 0.5334133, 0.4556532]   \n",
       " 37      6  [0.0, 1.0, 0.0]           2    [0.30245173, 0.4150771, 0.48440695]   \n",
       " 38      6  [0.0, 1.0, 0.0]           2    [0.42605627, 0.32624543, 0.6103265]   \n",
       " 39      6  [0.0, 1.0, 0.0]           2    [0.41922843, 0.43821728, 0.5490339]   \n",
       " 40      6  [0.0, 1.0, 0.0]           2     [0.6274384, 0.57977545, 0.5125768]   \n",
       " 41      6  [0.0, 1.0, 0.0]           2    [0.66411674, 0.6718868, 0.53067136]   \n",
       " 42      6  [0.0, 1.0, 0.0]           2     [0.5500697, 0.53203905, 0.5277281]   \n",
       " 43      6  [0.0, 1.0, 0.0]           2      [0.4211737, 0.4683298, 0.6137123]   \n",
       " 44      6  [0.0, 1.0, 0.0]           2     [0.27866614, 0.3524865, 0.4462943]   \n",
       " 45      6  [0.0, 1.0, 0.0]           2      [0.31382, 0.20994616, 0.64044785]   \n",
       " 46      6  [0.0, 1.0, 0.0]           2    [0.55422723, 0.63010585, 0.5122912]   \n",
       " 47      6  [0.0, 1.0, 0.0]           2   [0.38517487, 0.38991272, 0.54810095]   \n",
       " 48      6  [0.0, 1.0, 0.0]           2       [0.530815, 0.5873879, 0.5040815]   \n",
       " 49      6  [0.0, 1.0, 0.0]           2   [0.47037446, 0.52290666, 0.48039174]   \n",
       " 50      6  [0.0, 1.0, 0.0]           2     [0.7216195, 0.7797407, 0.40819955]   \n",
       " 51      6  [0.0, 0.0, 1.0]           2   [0.30143774, 0.34738362, 0.64736485]   \n",
       " 52      6  [0.0, 0.0, 1.0]           2   [0.19854438, 0.07337606, 0.84679174]   \n",
       " 53      6  [0.0, 0.0, 1.0]           2    [0.37313116, 0.3044914, 0.80577826]   \n",
       " 54      6  [0.0, 0.0, 1.0]           2                 [0.0, 0.0, 0.69364667]   \n",
       " 55      6  [0.0, 0.0, 1.0]           2       [0.27225673, 0.276693, 0.658911]   \n",
       " 56      6  [0.0, 0.0, 1.0]           2         [0.18345225, 0.021119475, 1.0]   \n",
       " 57      6  [0.0, 0.0, 1.0]           2     [0.5729896, 0.5570122, 0.71097517]   \n",
       " 58      6  [0.0, 0.0, 1.0]           2     [0.40900195, 0.3901738, 0.6636481]   \n",
       " 59      6  [0.0, 0.0, 1.0]           2      [0.15139699, 0.17534459, 0.64521]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction                       confidence_score  \\\n",
       " 0       7  [1.0, 0.0, 0.0]           2    [0.63211083, 0.5963594, 0.12397182]   \n",
       " 1       7  [0.0, 0.0, 1.0]           2   [0.22453022, 0.08493912, 0.78847945]   \n",
       " 2       7  [1.0, 0.0, 0.0]           1     [0.6801009, 0.7625021, 0.05722952]   \n",
       " 3       7  [0.0, 0.0, 1.0]           2    [0.34965754, 0.42393124, 0.5446235]   \n",
       " 4       7  [1.0, 0.0, 0.0]           2     [0.7929616, 0.7744075, 0.17270422]   \n",
       " 5       7  [1.0, 0.0, 0.0]           1     [0.8447819, 0.9256004, 0.07226145]   \n",
       " 6       7  [1.0, 0.0, 0.0]           2    [0.47495222, 0.3114344, 0.13808429]   \n",
       " 7       7  [1.0, 0.0, 0.0]           2  [0.33806038, 0.26984704, 0.051489115]   \n",
       " 8       7  [0.0, 0.0, 1.0]           2   [0.22424674, 0.16646755, 0.70389235]   \n",
       " 9       7  [0.0, 0.0, 1.0]           2    [0.40990877, 0.5333234, 0.58392155]   \n",
       " 10      7  [0.0, 0.0, 1.0]           2   [0.14429438, 0.16799235, 0.69406235]   \n",
       " 11      7  [0.0, 0.0, 1.0]           2    [0.12698281, 0.18334019, 0.6523732]   \n",
       " 12      7  [0.0, 0.0, 1.0]           2     [0.2525456, 0.23599494, 0.6926812]   \n",
       " 13      7  [1.0, 0.0, 0.0]           2     [0.5566752, 0.5625719, 0.08959973]   \n",
       " 14      7  [1.0, 0.0, 0.0]           1            [0.6638005, 0.7277769, 0.0]   \n",
       " 15      7  [0.0, 0.0, 1.0]           2      [0.2810316, 0.2848202, 0.6518949]   \n",
       " 16      7  [0.0, 0.0, 1.0]           2     [0.33174372, 0.4488443, 0.5374609]   \n",
       " 17      7  [1.0, 0.0, 0.0]           2    [0.75923896, 0.7154726, 0.21788692]   \n",
       " 18      7  [1.0, 0.0, 0.0]           2     [0.6187067, 0.5664505, 0.14552939]   \n",
       " 19      7  [1.0, 0.0, 0.0]           2    [0.62240434, 0.5311886, 0.20621622]   \n",
       " 20      7  [1.0, 0.0, 0.0]           2   [0.4625156, 0.45590675, 0.019210935]   \n",
       " 21      7  [1.0, 0.0, 0.0]           2    [0.73629546, 0.6828712, 0.16147137]   \n",
       " 22      7  [1.0, 0.0, 0.0]           2      [0.6186018, 0.48294365, 0.192343]   \n",
       " 23      7  [1.0, 0.0, 0.0]           1    [0.8164153, 0.89566386, 0.09911311]   \n",
       " 24      7  [1.0, 0.0, 0.0]           2     [0.6515064, 0.6135956, 0.14845228]   \n",
       " 25      7  [1.0, 0.0, 0.0]           2    [0.6390624, 0.62834156, 0.10429537]   \n",
       " 26      7  [1.0, 0.0, 0.0]           0            [1.0, 0.9999999, 0.2656436]   \n",
       " 27      7  [1.0, 0.0, 0.0]           2  [0.59566045, 0.63087237, 0.120577335]   \n",
       " 28      7  [1.0, 0.0, 0.0]           2   [0.55881166, 0.50150716, 0.12173653]   \n",
       " 29      7  [1.0, 0.0, 0.0]           2    [0.69248986, 0.6621388, 0.14777005]   \n",
       " 30      7  [0.0, 1.0, 0.0]           2   [0.47321486, 0.53877056, 0.51244175]   \n",
       " 31      7  [0.0, 1.0, 0.0]           2     [0.275877, 0.28528166, 0.52856576]   \n",
       " 32      7  [0.0, 1.0, 0.0]           2   [0.74066997, 0.85367715, 0.40281975]   \n",
       " 33      7  [0.0, 1.0, 0.0]           2     [0.39152813, 0.2682655, 0.6561943]   \n",
       " 34      7  [0.0, 1.0, 0.0]           2      [0.8300369, 0.8970007, 0.5122291]   \n",
       " 35      7  [0.0, 1.0, 0.0]           2    [0.3428054, 0.21080685, 0.62735856]   \n",
       " 36      7  [0.0, 1.0, 0.0]           2   [0.41241407, 0.53490674, 0.45610726]   \n",
       " 37      7  [0.0, 1.0, 0.0]           2   [0.30243397, 0.41634762, 0.48472607]   \n",
       " 38      7  [0.0, 1.0, 0.0]           2      [0.42532873, 0.3262092, 0.609938]   \n",
       " 39      7  [0.0, 1.0, 0.0]           2    [0.41810107, 0.43934262, 0.5491818]   \n",
       " 40      7  [0.0, 1.0, 0.0]           2     [0.6261418, 0.5804604, 0.51267254]   \n",
       " 41      7  [0.0, 1.0, 0.0]           2      [0.66121864, 0.673803, 0.5311998]   \n",
       " 42      7  [0.0, 1.0, 0.0]           2      [0.5487151, 0.5329932, 0.5278753]   \n",
       " 43      7  [0.0, 1.0, 0.0]           2      [0.41827583, 0.4706167, 0.614154]   \n",
       " 44      7  [0.0, 1.0, 0.0]           2   [0.28043032, 0.35248387, 0.44619668]   \n",
       " 45      7  [0.0, 1.0, 0.0]           2   [0.31360507, 0.20952475, 0.63987434]   \n",
       " 46      7  [0.0, 1.0, 0.0]           2      [0.5521095, 0.6321572, 0.5128416]   \n",
       " 47      7  [0.0, 1.0, 0.0]           2    [0.3848257, 0.39050496, 0.54802334]   \n",
       " 48      7  [0.0, 1.0, 0.0]           2     [0.52916217, 0.5890621, 0.5045301]   \n",
       " 49      7  [0.0, 1.0, 0.0]           2   [0.46996093, 0.52390707, 0.48061812]   \n",
       " 50      7  [0.0, 1.0, 0.0]           2      [0.719918, 0.7813386, 0.40895212]   \n",
       " 51      7  [0.0, 0.0, 1.0]           2     [0.2990558, 0.3493656, 0.64763224]   \n",
       " 52      7  [0.0, 0.0, 1.0]           2    [0.19557118, 0.07400894, 0.8463713]   \n",
       " 53      7  [0.0, 0.0, 1.0]           2    [0.36796355, 0.3069886, 0.80604494]   \n",
       " 54      7  [0.0, 0.0, 1.0]           2                  [0.0, 0.0, 0.6932801]   \n",
       " 55      7  [0.0, 0.0, 1.0]           2   [0.27047992, 0.27798474, 0.65892494]   \n",
       " 56      7  [0.0, 0.0, 1.0]           2    [0.17718518, 0.02301681, 0.9999999]   \n",
       " 57      7  [0.0, 0.0, 1.0]           2      [0.5671077, 0.5603665, 0.7116796]   \n",
       " 58      7  [0.0, 0.0, 1.0]           2    [0.40562868, 0.39218032, 0.6639663]   \n",
       " 59      7  [0.0, 0.0, 1.0]           2     [0.1506964, 0.17614508, 0.6451038]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction                       confidence_score  \\\n",
       " 0       8  [1.0, 0.0, 0.0]           2    [0.63963807, 0.5924423, 0.12339699]   \n",
       " 1       8  [0.0, 0.0, 1.0]           2    [0.22288549, 0.08484149, 0.7878306]   \n",
       " 2       8  [1.0, 0.0, 0.0]           1   [0.68698967, 0.75965595, 0.05736661]   \n",
       " 3       8  [0.0, 0.0, 1.0]           2    [0.3483833, 0.42553568, 0.54498696]   \n",
       " 4       8  [1.0, 0.0, 0.0]           2     [0.7977556, 0.7719495, 0.17257023]   \n",
       " 5       8  [1.0, 0.0, 0.0]           1    [0.8499328, 0.92352986, 0.07269764]   \n",
       " 6       8  [1.0, 0.0, 0.0]           2     [0.4852568, 0.3052454, 0.13650727]   \n",
       " 7       8  [1.0, 0.0, 0.0]           2    [0.3500179, 0.26338792, 0.05010891]   \n",
       " 8       8  [0.0, 0.0, 1.0]           2     [0.2220999, 0.16737092, 0.7038574]   \n",
       " 9       8  [0.0, 0.0, 1.0]           2      [0.4064833, 0.536464, 0.58477855]   \n",
       " 10      8  [0.0, 0.0, 1.0]           2    [0.14248943, 0.16935647, 0.6941478]   \n",
       " 11      8  [0.0, 0.0, 1.0]           2    [0.12547469, 0.1848247, 0.65260386]   \n",
       " 12      8  [0.0, 0.0, 1.0]           2   [0.24980438, 0.23759937, 0.69289446]   \n",
       " 13      8  [1.0, 0.0, 0.0]           2     [0.564875, 0.55859685, 0.08912897]   \n",
       " 14      8  [1.0, 0.0, 0.0]           1           [0.67237294, 0.7239225, 0.0]   \n",
       " 15      8  [0.0, 0.0, 1.0]           2    [0.27899182, 0.28624296, 0.6520355]   \n",
       " 16      8  [0.0, 0.0, 1.0]           2    [0.33037364, 0.45083237, 0.5379534]   \n",
       " 17      8  [1.0, 0.0, 0.0]           2     [0.7636105, 0.71310425, 0.2176212]   \n",
       " 18      8  [1.0, 0.0, 0.0]           2    [0.62613595, 0.56248355, 0.1448431]   \n",
       " 19      8  [1.0, 0.0, 0.0]           2     [0.62866795, 0.527581, 0.20549059]   \n",
       " 20      8  [1.0, 0.0, 0.0]           2  [0.47372305, 0.45026731, 0.018301249]   \n",
       " 21      8  [1.0, 0.0, 0.0]           2      [0.7421292, 0.679682, 0.16109765]   \n",
       " 22      8  [1.0, 0.0, 0.0]           2     [0.626016, 0.47843933, 0.19129264]   \n",
       " 23      8  [1.0, 0.0, 0.0]           1    [0.82118404, 0.8938172, 0.09951842]   \n",
       " 24      8  [1.0, 0.0, 0.0]           2      [0.6583692, 0.6100075, 0.1479162]   \n",
       " 25      8  [1.0, 0.0, 0.0]           2     [0.6464504, 0.62465835, 0.1039114]   \n",
       " 26      8  [1.0, 0.0, 0.0]           0            [1.0000001, 1.0, 0.2662815]   \n",
       " 27      8  [1.0, 0.0, 0.0]           2   [0.6021706, 0.62797713, 0.120444536]   \n",
       " 28      8  [1.0, 0.0, 0.0]           2    [0.5673994, 0.49691224, 0.12086928]   \n",
       " 29      8  [1.0, 0.0, 0.0]           2     [0.698835, 0.65885305, 0.14737725]   \n",
       " 30      8  [0.0, 1.0, 0.0]           2     [0.4720534, 0.5402348, 0.51276636]   \n",
       " 31      8  [0.0, 1.0, 0.0]           2     [0.27680933, 0.2851658, 0.5282898]   \n",
       " 32      8  [0.0, 1.0, 0.0]           2        [0.738536, 0.855829, 0.4037633]   \n",
       " 33      8  [0.0, 1.0, 0.0]           2    [0.39062464, 0.2681166, 0.65567946]   \n",
       " 34      8  [0.0, 1.0, 0.0]           2      [0.8251928, 0.9002154, 0.5133374]   \n",
       " 35      8  [0.0, 1.0, 0.0]           2   [0.34296215, 0.20999825, 0.62667036]   \n",
       " 36      8  [0.0, 1.0, 0.0]           2    [0.4120785, 0.53636074, 0.45656395]   \n",
       " 37      8  [0.0, 1.0, 0.0]           2     [0.3024181, 0.41757607, 0.4850552]   \n",
       " 38      8  [0.0, 1.0, 0.0]           2   [0.42463148, 0.32616854, 0.60954905]   \n",
       " 39      8  [0.0, 1.0, 0.0]           2    [0.41700613, 0.44044518, 0.5493307]   \n",
       " 40      8  [0.0, 1.0, 0.0]           2      [0.6248847, 0.5811348, 0.5127573]   \n",
       " 41      8  [0.0, 1.0, 0.0]           2     [0.65839016, 0.67570925, 0.531713]   \n",
       " 42      8  [0.0, 1.0, 0.0]           2       [0.5473999, 0.533932, 0.5280163]   \n",
       " 43      8  [0.0, 1.0, 0.0]           2      [0.41544616, 0.4728856, 0.614594]   \n",
       " 44      8  [0.0, 1.0, 0.0]           2    [0.28215778, 0.3524394, 0.44611263]   \n",
       " 45      8  [0.0, 1.0, 0.0]           2    [0.31340802, 0.20910192, 0.6393082]   \n",
       " 46      8  [0.0, 1.0, 0.0]           2    [0.55003846, 0.6341872, 0.51338387]   \n",
       " 47      8  [0.0, 1.0, 0.0]           2    [0.38449037, 0.39107585, 0.5479493]   \n",
       " 48      8  [0.0, 1.0, 0.0]           2     [0.5275506, 0.5907121, 0.50497293]   \n",
       " 49      8  [0.0, 1.0, 0.0]           2     [0.46956122, 0.524879, 0.48084354]   \n",
       " 50      8  [0.0, 1.0, 0.0]           2     [0.7182604, 0.7829137, 0.40968847]   \n",
       " 51      8  [0.0, 0.0, 1.0]           2     [0.29672968, 0.3513322, 0.6479051]   \n",
       " 52      8  [0.0, 0.0, 1.0]           2    [0.1926744, 0.074673295, 0.8459563]   \n",
       " 53      8  [0.0, 0.0, 1.0]           2    [0.36292422, 0.30949998, 0.8063073]   \n",
       " 54      8  [0.0, 0.0, 1.0]           2                    [0.0, 0.0, 0.69294]   \n",
       " 55      8  [0.0, 0.0, 1.0]           2    [0.26874745, 0.27926636, 0.6589465]   \n",
       " 56      8  [0.0, 0.0, 1.0]           2         [0.17108274, 0.024971485, 1.0]   \n",
       " 57      8  [0.0, 0.0, 1.0]           2     [0.5613643, 0.56372404, 0.7123697]   \n",
       " 58      8  [0.0, 0.0, 1.0]           2    [0.40234363, 0.3941772, 0.66428304]   \n",
       " 59      8  [0.0, 0.0, 1.0]           2    [0.15001404, 0.17693114, 0.6450145]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction                       confidence_score  \\\n",
       " 0       9  [1.0, 0.0, 0.0]           2    [0.64699244, 0.5885091, 0.12282443]   \n",
       " 1       9  [0.0, 0.0, 1.0]           2   [0.22126698, 0.08474791, 0.78721416]   \n",
       " 2       9  [1.0, 0.0, 0.0]           1     [0.6937239, 0.7567942, 0.05749154]   \n",
       " 3       9  [0.0, 0.0, 1.0]           2    [0.34712148, 0.42712808, 0.5453931]   \n",
       " 4       9  [1.0, 0.0, 0.0]           0    [0.80244064, 0.7694833, 0.17238045]   \n",
       " 5       9  [1.0, 0.0, 0.0]           1      [0.8549714, 0.9214511, 0.0730685]   \n",
       " 6       9  [1.0, 0.0, 0.0]           2    [0.4953239, 0.29903507, 0.13498986]   \n",
       " 7       9  [1.0, 0.0, 0.0]           2    [0.36171174, 0.2569015, 0.04883635]   \n",
       " 8       9  [0.0, 0.0, 1.0]           2    [0.2199769, 0.16827059, 0.70388997]   \n",
       " 9       9  [0.0, 0.0, 1.0]           2    [0.40312076, 0.53959465, 0.5856503]   \n",
       " 10      9  [0.0, 0.0, 1.0]           2    [0.14071703, 0.17071831, 0.6943201]   \n",
       " 11      9  [0.0, 0.0, 1.0]           2    [0.12398982, 0.18630338, 0.6529397]   \n",
       " 12      9  [0.0, 0.0, 1.0]           2   [0.24710202, 0.23919952, 0.69316685]   \n",
       " 13      9  [1.0, 0.0, 0.0]           2    [0.57288575, 0.5546024, 0.08868754]   \n",
       " 14      9  [1.0, 0.0, 0.0]           1            [0.6807537, 0.7200515, 0.0]   \n",
       " 15      9  [0.0, 0.0, 1.0]           2    [0.27697992, 0.28765988, 0.6522261]   \n",
       " 16      9  [0.0, 0.0, 1.0]           2   [0.32902312, 0.45280862, 0.53849065]   \n",
       " 17      9  [1.0, 0.0, 0.0]           2    [0.7678788, 0.71072674, 0.21730745]   \n",
       " 18      9  [1.0, 0.0, 0.0]           2      [0.633394, 0.5585003, 0.14416087]   \n",
       " 19      9  [1.0, 0.0, 0.0]           2       [0.6347773, 0.523957, 0.2047683]   \n",
       " 20      9  [1.0, 0.0, 0.0]           2  [0.48468995, 0.44460487, 0.017451763]   \n",
       " 21      9  [1.0, 0.0, 0.0]           2   [0.74782515, 0.67648053, 0.16069245]   \n",
       " 22      9  [1.0, 0.0, 0.0]           2    [0.63325334, 0.47391868, 0.1902479]   \n",
       " 23      9  [1.0, 0.0, 0.0]           1     [0.82584786, 0.8919616, 0.0998652]   \n",
       " 24      9  [1.0, 0.0, 0.0]           2   [0.66507363, 0.60640526, 0.14737308]   \n",
       " 25      9  [1.0, 0.0, 0.0]           2     [0.653667, 0.6209583, 0.103530526]   \n",
       " 26      9  [1.0, 0.0, 0.0]           0                 [1.0, 1.0, 0.26679468]   \n",
       " 27      9  [1.0, 0.0, 0.0]           2    [0.6085248, 0.62506294, 0.12032795]   \n",
       " 28      9  [1.0, 0.0, 0.0]           2   [0.57579136, 0.49229908, 0.12002778]   \n",
       " 29      9  [1.0, 0.0, 0.0]           2     [0.7050333, 0.6555536, 0.14696503]   \n",
       " 30      9  [0.0, 1.0, 0.0]           2    [0.4709077, 0.54168916, 0.51309097]   \n",
       " 31      9  [0.0, 1.0, 0.0]           2    [0.2777059, 0.28503752, 0.52808225]   \n",
       " 32      9  [0.0, 1.0, 0.0]           2    [0.73644376, 0.85797334, 0.4046396]   \n",
       " 33      9  [0.0, 1.0, 0.0]           2    [0.38972163, 0.26796317, 0.6551732]   \n",
       " 34      9  [0.0, 1.0, 0.0]           2    [0.8204539, 0.90342784, 0.51433694]   \n",
       " 35      9  [0.0, 1.0, 0.0]           2    [0.34309363, 0.20918274, 0.6260158]   \n",
       " 36      9  [0.0, 1.0, 0.0]           2    [0.41173887, 0.5378001, 0.45704973]   \n",
       " 37      9  [0.0, 1.0, 0.0]           2   [0.30238962, 0.41879034, 0.48544967]   \n",
       " 38      9  [0.0, 1.0, 0.0]           2    [0.4239292, 0.32612062, 0.60916865]   \n",
       " 39      9  [0.0, 1.0, 0.0]           2   [0.41591907, 0.44153738, 0.54949725]   \n",
       " 40      9  [0.0, 1.0, 0.0]           2    [0.6236396, 0.58180094, 0.51280034]   \n",
       " 41      9  [0.0, 1.0, 0.0]           2     [0.6556132, 0.67760897, 0.5321685]   \n",
       " 42      9  [0.0, 1.0, 0.0]           2    [0.54609704, 0.5348613, 0.52813756]   \n",
       " 43      9  [0.0, 1.0, 0.0]           2     [0.41266894, 0.4751482, 0.6150378]   \n",
       " 44      9  [0.0, 1.0, 0.0]           2     [0.28383422, 0.3523786, 0.4461063]   \n",
       " 45      9  [0.0, 1.0, 0.0]           2    [0.31319523, 0.20867312, 0.6387814]   \n",
       " 46      9  [0.0, 1.0, 0.0]           2     [0.5480063, 0.6362083, 0.51389873]   \n",
       " 47      9  [0.0, 1.0, 0.0]           2    [0.38414836, 0.3916371, 0.54790175]   \n",
       " 48      9  [0.0, 1.0, 0.0]           2     [0.5259619, 0.5923519, 0.50540173]   \n",
       " 49      9  [0.0, 1.0, 0.0]           2     [0.46915722, 0.5258386, 0.4810778]   \n",
       " 50      9  [0.0, 1.0, 0.0]           2        [0.7166259, 0.7844796, 0.41037]   \n",
       " 51      9  [0.0, 0.0, 1.0]           2      [0.29444695, 0.3532927, 0.648214]   \n",
       " 52      9  [0.0, 0.0, 1.0]           2    [0.1898371, 0.07534671, 0.84556854]   \n",
       " 53      9  [0.0, 0.0, 1.0]           2    [0.35798526, 0.31201506, 0.8065547]   \n",
       " 54      9  [0.0, 0.0, 1.0]           2                  [0.0, 0.0, 0.6927384]   \n",
       " 55      9  [0.0, 0.0, 1.0]           2   [0.26704335, 0.28054237, 0.65901315]   \n",
       " 56      9  [0.0, 0.0, 1.0]           2   [0.16511536, 0.026948571, 0.9999999]   \n",
       " 57      9  [0.0, 0.0, 1.0]           2     [0.55573726, 0.5670798, 0.7130014]   \n",
       " 58      9  [0.0, 0.0, 1.0]           2      [0.3991115, 0.396168, 0.66460764]   \n",
       " 59      9  [0.0, 0.0, 1.0]           2    [0.14933872, 0.17771184, 0.6450163]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_output = extractor.get_testing_results()\n",
    "epoch_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-constitutional",
   "metadata": {},
   "source": [
    "Let's make a plot of the accuracy and loss per epoch from the model training. What trends do you see here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "continental-anchor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-feature",
   "metadata": {
    "id": "9EEGuDVuzb5r"
   },
   "source": [
    "## Evaluate the model\n",
    "\n",
    "Let's see how the model performs on brand new, unseen before data. Two values will be returned: loss (a number which represents our error, lower values are better), and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "incredible-maine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.42762122 0.41295823 0.44243446]\n",
      " [0.35250974 0.32392985 0.60524696]\n",
      " [0.43586612 0.44269884 0.42642426]\n",
      " [0.37471446 0.3844378  0.5459873 ]\n",
      " [0.45504722 0.44494137 0.45457846]\n",
      " [0.4643153  0.4717982  0.4302415 ]\n",
      " [0.40086207 0.36180025 0.44541568]\n",
      " [0.37728864 0.3543541  0.42430326]\n",
      " [0.35228214 0.3386906  0.5848279 ]\n",
      " [0.38459453 0.4043137  0.5558526 ]\n",
      " [0.33829817 0.3391232  0.5824827 ]\n",
      " [0.33534697 0.3418775  0.5723422 ]\n",
      " [0.35706785 0.35122567 0.5822001 ]\n",
      " [0.41454646 0.406966   0.43406904]\n",
      " [0.43357778 0.4362054  0.41233563]\n",
      " [0.3623393  0.35978997 0.57216734]\n",
      " [0.37152132 0.38897625 0.54429585]\n",
      " [0.4489494  0.43455744 0.46558806]\n",
      " [0.425222   0.40765488 0.44766307]\n",
      " [0.42546606 0.40155014 0.46251526]\n",
      " [0.3989859  0.38752642 0.4166123 ]\n",
      " [0.4454113  0.4285052  0.45171425]\n",
      " [0.42519718 0.392707   0.45895696]\n",
      " [0.459177   0.46658662 0.43680817]\n",
      " [0.43081132 0.41612098 0.44845024]\n",
      " [0.4287988  0.41869292 0.43770638]\n",
      " [0.48990294 0.48567995 0.4777152 ]\n",
      " [0.4208343  0.41941833 0.44182268]\n",
      " [0.4150591  0.3959553  0.44174913]\n",
      " [0.43786144 0.42480683 0.44835025]\n",
      " [0.3965543  0.4046839  0.5380715 ]\n",
      " [0.36246735 0.3593265  0.5417452 ]\n",
      " [0.44340327 0.46057996 0.5114949 ]\n",
      " [0.3822305  0.356309   0.57288957]\n",
      " [0.4582253  0.46861303 0.5383768 ]\n",
      " [0.37400383 0.3459209  0.56574434]\n",
      " [0.386115   0.4039966  0.52433825]\n",
      " [0.36682236 0.38296428 0.53129786]\n",
      " [0.3882658  0.366587   0.5616159 ]\n",
      " [0.38685253 0.38698432 0.5469931 ]\n",
      " [0.42350104 0.41177276 0.5380003 ]\n",
      " [0.42914218 0.42870465 0.54274654]\n",
      " [0.40982005 0.40347722 0.5417588 ]\n",
      " [0.3862791  0.39292428 0.56305414]\n",
      " [0.36354858 0.37122753 0.5216565 ]\n",
      " [0.36872882 0.34583083 0.56887263]\n",
      " [0.4101569  0.421388   0.53826946]\n",
      " [0.3812472  0.37816557 0.5466021 ]\n",
      " [0.40626758 0.4136374  0.53618723]\n",
      " [0.39624542 0.40188268 0.53022647]\n",
      " [0.43990678 0.4475916  0.51289916]\n",
      " [0.365421   0.37138906 0.57118416]\n",
      " [0.3469645  0.3222684  0.619547  ]\n",
      " [0.3766312  0.3640942  0.6099864 ]\n",
      " [0.31347123 0.30895257 0.58209515]\n",
      " [0.36058614 0.3585321  0.57383054]\n",
      " [0.34260282 0.31371513 0.6573913 ]\n",
      " [0.4115209  0.4091711  0.5870607 ]\n",
      " [0.38388714 0.37896633 0.5752015 ]\n",
      " [0.3398193  0.34035912 0.57040054]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "   Iris-setosa       1.00      0.10      0.17        21\n",
      "Iris-versicolo       0.00      0.00      0.00        21\n",
      "Iris-virginica       0.33      1.00      0.50        18\n",
      "\n",
      "      accuracy                           0.33        60\n",
      "     macro avg       0.44      0.37      0.22        60\n",
      "  weighted avg       0.45      0.33      0.21        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# make a prediction\n",
    "testingPredictions = model.predict(x_test)\n",
    "testingPredictions = list(testingPredictions.argmax(axis=-1))\n",
    "\n",
    "confidence_scores = model.predict(x_test, batch_size=32)\n",
    "print(confidence_scores)\n",
    "\n",
    "target_names = ['Iris-setosa', 'Iris-versicolo', 'Iris-virginica']\n",
    "print(classification_report(y_test.argmax(axis=-1), testingPredictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "civilian-savage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step - loss: 1.0300 - accuracy: 0.3333\n",
      "loss: 1.030\n",
      "accuracy: 0.333\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "  print(\"%s: %.3f\" % (name, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-macro",
   "metadata": {},
   "source": [
    "Now, we need to export the data in order to support some interactive visualizations that we've created. Feel free to skip over this code block and move to the interactive visualizations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "homeless-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "# output_directory = \"libraries/\"\n",
    "# output_filename = \"predict_LR.json\"\n",
    "# full_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "# true_label = []\n",
    "# for i in range(len(y_test)):\n",
    "#     for j in range(len(y_test[i])):\n",
    "#         if y_test[i][j] == 1:\n",
    "#             if j == 0:\n",
    "#                 true_label.append(target_names[0])\n",
    "#             if j == 1:\n",
    "#                 true_label.append(target_names[1])\n",
    "#             if j == 2:\n",
    "#                 true_label.append(target_names[2])\n",
    "\n",
    "\n",
    "# for i in range(len(testingPredictions)):\n",
    "#     if testingPredictions[i] == 0:\n",
    "#         testingPredictions[i] = target_names[0]\n",
    "#     if testingPredictions[i] == 1:\n",
    "#         testingPredictions[i] = target_names[1]\n",
    "#     if testingPredictions[i] == 2:\n",
    "#         testingPredictions[i] = target_names[2]\n",
    "\n",
    "# data = []\n",
    "# data.extend([{\n",
    "#       'index': i,\n",
    "#       'true_label': true_label[i],\n",
    "#       'predicted_label': testingPredictions[i],\n",
    "#       'confidence_score': confidence_scores.tolist()[i],\n",
    "#       'features': x_test.tolist()[i]\n",
    "#   } for i in range(len(testingPredictions))])\n",
    "\n",
    "\n",
    "\n",
    "# with open(full_path, 'w') as outfile:\n",
    "#     json.dump(data, outfile, indent=4, sort_keys=False)\n",
    "    \n",
    "# collected_epoch_filename = 'predict_LR_extended.json'\n",
    "# full_path_extended = os.path.join(output_directory, collected_epoch_filename)\n",
    "\n",
    "# extended_data = {}\n",
    "# for i, table in enumerate(epoch_history_extracted_data):\n",
    "#     jsonified = table.to_json(orient='index')\n",
    "#     parsed = json.loads(jsonified)\n",
    "# #     extended_data.append(parsed)\n",
    "#     extended_data[f'{i}'] = parsed\n",
    "    \n",
    "# # print(extended_data)\n",
    "    \n",
    "# with open(full_path_extended, 'w') as outfile:\n",
    "#     json.dump(extended_data, outfile, indent=4, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "recreational-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "output_directory = \"libraries/\"\n",
    "output_filename = \"predict_LR_extended.json\"\n",
    "full_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "data = {}\n",
    "for i in range(len(epoch_output[0])):\n",
    "    data[i] = {}\n",
    "    data[i]['Num Epochs'] = len(epoch_output)\n",
    "    data[i]['Index'] = i\n",
    "    data[i]['Test Label'] = int(epoch_output[0]['actual'][i].argmax())\n",
    "    data[i]['Test Prediction'] = {}\n",
    "    data[i]['Test Confidence Score'] = {}\n",
    "#     data[i]['Intermediate Values'] = {}\n",
    "    for j in range(len(epoch_output)):\n",
    "        data[i]['Test Prediction'][j] = int(epoch_output[j]['prediction'][i])\n",
    "        data[i]['Test Confidence Score'][j] = epoch_output[j]['confidence_score'][i].tolist()\n",
    "#         data[i]['Intermediate Values'][j] = epoch_output[j]['intermediate_values'][i]\n",
    "    data[i]['Test Sentence'] = epoch_output[0]['input'][i].tolist()\n",
    "    \n",
    "with open(full_path, 'w') as outfile:\n",
    "    json.dump(data, outfile, indent=4, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "quarterly-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "output_directory = \"libraries/\"\n",
    "collected_epoch_filename = 'predict_LR_extended.json'\n",
    "full_path_extended = os.path.join(output_directory, collected_epoch_filename)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import libraries.mlvislib as mlvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "enclosed-publicity",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"https://d3js.org/d3.v3.min.js\" charset=\"utf-8\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\t\t<style> \n",
       "\t\tbody {\n",
       "\t\t\tfont-family: Arial, sans-serif;\n",
       "\t\t\tfont-size: larger;\n",
       "\t\t}\n",
       "\t\t.box_highlighted { \n",
       "\t\t\tbackground-color: #ffb; \n",
       "\t\t\tborder: 1px solid #b53;\n",
       "\t\t}\n",
       "\t\t.highlight{\n",
       "\t\t\tbackground-color: yellow;\n",
       "\t\t}\n",
       "\t\t.lighthigh{\n",
       "\t\t\tbackground-color: green;\n",
       "\t\t}\n",
       "\t\tli{\n",
       "\t\t\tfont-size: smaller;\n",
       "\t\t}\n",
       "\t\ttd{\n",
       "\t\t\tmin-width: 100px;\n",
       "\t\t}\n",
       "\t\t#review{\n",
       "\t\t\tborder:1px solid pink; \n",
       "\t\t\tpadding: 5px; \n",
       "\t\t\tfloat: left; \n",
       "\t\t\twidth: 750px; \n",
       "\t\t\theight: 500px; \n",
       "\t\t\tbackground-color: white;\n",
       "\t\t\tmargin: 20px;\n",
       "\t\t\toverflow: scroll;\n",
       "\t\t\t}\n",
       "\t\t#matrix{\n",
       "\t\t\tborder:1px solid pink; \n",
       "\t\t\tpadding: 5px; \n",
       "\t\t\tfloat: left;\n",
       "\t\t\tmargin: 20px;\n",
       "\t\t}\n",
       "\t\t#slider {\n",
       "\t\t  -webkit-appearance: none;\n",
       "\t\t  width: 100%;\n",
       "\t\t  height: 15px;\n",
       "\t\t  border-radius: 5px;  \n",
       "\t\t  background: #d3d3d3;\n",
       "\t\t  outline: none;\n",
       "\t\t  opacity: 0.7;\n",
       "\t\t  -webkit-transition: .2s;\n",
       "\t\t  transition: opacity .2s;\n",
       "\t\t}\n",
       "\t\t#slider::-webkit-slider-thumb {\n",
       "\t\t  -webkit-appearance: none;\n",
       "\t\t  appearance: none;\n",
       "\t\t  width: 25px;\n",
       "\t\t  height: 25px;\n",
       "\t\t  border-radius: 50%; \n",
       "\t\t  background: #4ca2af;\n",
       "\t\t  cursor: pointer;\n",
       "\t\t}\n",
       "\t\t#slider::-moz-range-thumb {\n",
       "\t\t  width: 25px;\n",
       "\t\t  height: 25px;\n",
       "\t\t  border-radius: 50%;\n",
       "\t\t  background: #4ca2af;\n",
       "\t\t  cursor: pointer;\n",
       "\t\t}\n",
       "\t\t </style>\n",
       "\t\t<h1> Interactive Confusion Matrix </h1>\n",
       "\t\t<h3 id=\"confidence_setting\"> Confidence: 0.5 </h3>\n",
       "\t\t<input class=\"slider\" id=\"confidence_slider\" type=\"range\" min=\"0\" max=\"1\" step=\".1\" value=\".5\"/>\n",
       "\t\t<h3 id=\"epoch_setting\"> Epoch: 1 </h3>\n",
       "\t\t<input class=\"slider\" id=\"epoch_slider\" type=\"range\" min=\"1\" max=\"20\" step=\"1\" value=\"1\"/>\n",
       "\t\t<div>\n",
       "\t\t\t<div>\n",
       "\t\t\t\t<div id=\"matrix\"></div>\n",
       "\t\t\t\t<div id=\"review\">\n",
       "\t\t\t\t\tData \n",
       "\t\t\t\t\t<ul id = \"testList\"></ul>\n",
       "\t\t\t\t</div>\n",
       "\t\t\t</div>\n",
       "\t\t</div>\n",
       "\t\t<script> \n",
       "\t\tconsole.log(\"Visualization: Running JavaScript...\");\n",
       "\t\tvar dname = \"libraries/predict_LR_extended.json\";\n",
       "\t\tvar currentConfSetting = .5;\n",
       "\t\tvar currentEpochSetting = 1;\n",
       "\t\tvar lastEpochIndex = 0;\n",
       "        // Delete this line below just in case it slows down the program\n",
       "\t\tconsole.log(\"Visualization: Reading JSON file(\", dname, \")...\");\n",
       "\n",
       "\t\td3.json( \"libraries/predict_LR_extended.json\", function(d) {\n",
       "\t\t\tconsole.log(\"Visualization: Logging complete JSON...\");\n",
       "\t\t\tconsole.log(d);\n",
       "\t\t\tlastEpochIndex = d[0]['Num Epochs']\n",
       "\t\t\td3.select(\"#epoch_slider\").attr(\"max\", lastEpochIndex);\n",
       "\t\t});\n",
       "\n",
       "\n",
       "\t\t/*--------------------------------------------------------------------------------\n",
       "\t\tI've temporarily left out the 'getType' function, since the names of these\n",
       "\t\ttypes are not included in the JSON file that is given to the JavaScript. More\n",
       "\t\tfunctionality can be incldued later to bring the names in as well as the raw data.\n",
       "\t\tType will be represented by the given numeric identifier for now.\n",
       "\t\t--------------------------------------------------------------------------------*/\n",
       "\n",
       "\t\t/*--------------------------------------------------------------------------------\n",
       "\t\tFunction: extractTypes\n",
       "\t\tBehavior: Identifies what different types each data point can be identified as based \n",
       "\t\t\t\t  off of the 'true_label' attribute in JSON file.\n",
       "\t\tInput: JSON file\n",
       "\t\tOutput: Returns array of possible values for 'Test Label'.\n",
       "\t\t--------------------------------------------------------------------------------*/\n",
       "\n",
       "\t\tfunction extractTypes(data){\n",
       "\t\t\tvar lookup = {};\n",
       "\t\t\tvar items = data;\n",
       "\t\t\tvar result = [];\n",
       "\n",
       "\t\t\tfor (var item, i=0; item = items[i++];){\n",
       "\t\t\t\tvar name = item['Test Label'];\n",
       "\t\t\t\tif(!(name in lookup)){\n",
       "\t\t\t\t\tlookup[name] = 1;\n",
       "\t\t\t\t\tresult.push(name);\n",
       "\t\t\t\t}\n",
       "\t\t\t}\n",
       "\t\t\treturn result.sort();\n",
       "\t\t}\n",
       "\n",
       "\t\t/*--------------------------------------------------------------------------------\n",
       "\t\tFunction: fetchDataWindowResults\n",
       "\t\tBehavior: Fetches subset of 'd' variable to be displayed in 'Data' window\n",
       "\t\tInput: 'd' variable, testLabel, predLabel, epoch, conf\n",
       "\t\tOutput: Subset of 'd' variable formatted the same as 'd'\n",
       "\t\t--------------------------------------------------------------------------------*/\n",
       "        function fetchDataWindowResults(d, testLabel, predLabel, epoch, conf){\n",
       "            console.log(\"Visualization: Calling fetchDataWindowResults...\");\n",
       "            var fullDataSet = d;\n",
       "            var selectedTestLabel = testLabel;\n",
       "            var selectedPredictionLabel = predLabel;\n",
       "            var selectedEpoch = epoch;\n",
       "            var selectedConfMin = conf;\n",
       "\n",
       "            var selectedEntries = []\n",
       "            console.log(d);\n",
       "            console.log(\"Visualization: fetchDataWindowResults: for testLabel, predLabel, epoch, conf of: \", \n",
       "                testLabel, predLabel, epoch, conf);\n",
       "            for (const dataPoint of Object.entries(d)){\n",
       "                var currentPrediction = dataPoint[1]['Test Prediction'][epoch];\n",
       "                var currentTestLabel = dataPoint[1]['Test Label'];\n",
       "                var currentSentence = dataPoint[1]['Test Sentence'];\n",
       "                var currentConfScore = dataPoint[1]['Test Confidence Score'][epoch];\n",
       "                if (currentPrediction == selectedPredictionLabel &&\n",
       "                currentTestLabel == selectedTestLabel &&\n",
       "                currentConfScore > selectedConfMin){\n",
       "                    selectedEntries.push(dataPoint);\n",
       "                    // console.log(\"Prediction:\", currentPrediction,\n",
       "                    //     \"Test Sentence:\", currentSentence,\n",
       "                    //     \"Confidence Score:\", currentConfScore);\n",
       "                }\n",
       "            }\n",
       "            return selectedEntries;\n",
       "        }\n",
       "\n",
       "\t\t/*--------------------------------------------------------------------------------\n",
       "\t\tFunction: Slider Re-Draw\n",
       "\t\tBehavior: This d3 code will redraw each time there is a change in the slider.\n",
       "\t\tInput: None\n",
       "\t\tOutput: Visualization should be redrawn\n",
       "\t\t--------------------------------------------------------------------------------*/\n",
       "\t\t/*--------------------------------------------------------------------------------\n",
       "\t\tBUG: This should be adjusted to be called both on load and on change. \n",
       "\t\t--------------------------------------------------------------------------------*/\n",
       "\n",
       "\t\td3.selectAll(\".slider\").on(\"change\", function() {\n",
       "\t\t\td3.select(\"svg\").remove();\n",
       "\n",
       "\t\t\tif(this.id == \"confidence_slider\"){\n",
       "\t\t\t\tcurrentConfSetting = this.value;\n",
       "\t\t\t}\n",
       "\t\t\tif(this.id == \"epoch_slider\"){\n",
       "\t\t\t\tcurrentEpochSetting = this.value;\n",
       "\t\t\t}\n",
       "\n",
       "\t\t\td3.select(\"#confidence_setting\").text(\"Confidence: \" + currentConfSetting);\n",
       "\t\t\td3.select(\"#epoch_setting\").text(\"Epoch: \" + currentEpochSetting);\n",
       "\t\t\tconsole.log(\"Visualization: Confidence set to (\", currentConfSetting,\n",
       "\t\t\t\t\t\t\") Epoch set to (\", currentEpochSetting, \")\");\n",
       "\n",
       "\t\t\t/*--------------------------------------------------------------------------------\n",
       "\t\t\tFunction: Re-Draw\n",
       "\t\t\tBehavior: Adjusting the slider will call this function to redraw the\n",
       "\t\t\t\t\t  visualization. First a table is build keep track of the number of\n",
       "\t\t\t\t\t  elements to be in each cell. Next, data is read into an array based on\n",
       "\t\t\t\t\t  the parameters set by the sliders.\n",
       "\t\t\tInput: filepath to JSON\n",
       "\t\t\tOutput: visualization should be redrawn\n",
       "\t\t\t--------------------------------------------------------------------------------*/\n",
       "\n",
       "\t\t\td3.json(\"libraries/predict_LR_extended.json\", function(d) {\n",
       "                console.log(\"Debugging: displaying variable d: \", d);\n",
       "                \n",
       "\n",
       "\t\t\t\tvar totalItems = Object.keys(d).length\n",
       "\t\t\t\tconsole.log(\"Visualization: \", totalItems, \"pieces of test data included\")\n",
       "\n",
       "\t\t\t\tvar possibleOutputValues = extractTypes(d);\n",
       "\t\t\t\tconsole.log(\"Visualization: Possible outcomes inlcudes; \", possibleOutputValues)\n",
       "\n",
       "\t\t\t\tvar tableDimension = extractTypes(d).length;\n",
       "\t\t\t\tconsole.log(\"Visualization: Constructing\", tableDimension, \"x\",\n",
       "\t\t\t\t\t\t\ttableDimension, \"chart...\");\n",
       "\t\t\t\tvar dataset = [];\n",
       "\n",
       "\t\t\t\tvar table = new Array(tableDimension);\n",
       "\t\t\t\tfor(var i=0; i<tableDimension; i++){\n",
       "\t\t\t\t\ttable[i] = new Array(tableDimension);\n",
       "\t\t\t\t\tfor(var j=0; j<tableDimension; j++){\n",
       "\t\t\t\t\t\ttable[i][j] = 0;\n",
       "\t\t\t\t\t}\n",
       "\t\t\t\t}\n",
       "\t\t\t\tconsole.log(\"Visualization: Table initialized as: \", table)\n",
       "\n",
       "\n",
       "\t\t\t\t/*\n",
       "\t\t\t\t# NOTE: To be removed, we only need to know the integer representation \n",
       "\t\t\t\t#\t\tof which epoch we are using.\n",
       "\t\t\t\tvar selectedEpoch = {};\n",
       "\n",
       "\t\t\t\tfor(var singleEpoch, i=0; singleEpoch = d[i++];){\n",
       "\t\t\t\t\tif((singleEpoch[0][\"Epoch\"] + 1) == parseInt(currentEpochSetting)){\n",
       "\t\t\t\t\t\tselectedEpoch = singleEpoch;\n",
       "\t\t\t\t\t}\n",
       "\t\t\t\t}\n",
       "\t\t\t\t*/\n",
       "\n",
       "\t\t\t\tconsole.log(\"Visualization: Preparing to display epoch (\", currentEpochSetting, \")...\");\n",
       "\n",
       "\t\t\t\t/*--------------------------------------------------------------------------------\n",
       "\t\t\t\tNOTE: Will we need to display just integer representations of classifications,\n",
       "\t\t\t\t\t  or will we need to display titles of classicications along the axis\n",
       "\t\t\t\t--------------------------------------------------------------------------------*/\n",
       "\n",
       "\t\t\t\t// NOTE EDITING HERE\n",
       "\n",
       "\t\t\t\tfor(var jsonEntry, i=0; jsonEntry = d[i++];){\n",
       "                    // console.log(\"Debugging: jsonEntry \", jsonEntry);\n",
       "\t\t\t\t\tvar index = i;\n",
       "\t\t\t\t\t// var epoch = jsonEntry[\"Epoch\"];\n",
       "\t\t\t\t\tvar entryText = jsonEntry[\"Test Sentence\"];\n",
       "\t\t\t\t\tvar confidenceScore = jsonEntry[\"Test Confidence Score\"][currentEpochSetting-1];\n",
       "                    // console.log(\"Debugging: confidenceScore \", confidenceScore);\n",
       "\t\t\t\t\tvar trueLabel = jsonEntry[\"Test Label\"];\n",
       "                    // console.log(\"Debugging: trueLabel \", trueLabel);\n",
       "                    // console.log(typeof trueLabel);\n",
       "\t\t\t\t\tvar predictedLabel = jsonEntry[\"Test Prediction\"][currentEpochSetting-1];\n",
       "                    // console.log(\"Debugging: predictedLabel \", predictedLabel);\n",
       "                    // console.log(typeof predictedLabel);\n",
       "\t\t\t\t\tvar tableXCoordinate = possibleOutputValues.indexOf(predictedLabel); //Predicted\n",
       "                    // console.log(\"Debugging: tableXCoordinate \", tableXCoordinate);\n",
       "                    // console.log(typeof tableXCoordinate);\n",
       "\t\t\t\t\tvar tableYCoordinate = possibleOutputValues.indexOf(trueLabel); // Actual\n",
       "                    // console.log(\"Debugging: tableYCoordinate \", tableYCoordinate);\n",
       "                    // console.log(typeof tableYCoordinate);\n",
       "\n",
       "\t\t\t\t\tif(confidenceScore > currentConfSetting){\n",
       "\t\t\t\t\t\ttable[tableXCoordinate][tableYCoordinate]+=1;\n",
       "\t\t\t\t\t\tdataset.push([trueLabel, predictedLabel, entryText, index]);\n",
       "\t\t\t\t\t}\n",
       "\t\t\t\t}\n",
       "\n",
       "\t\t\t\tconsole.log(\"Visualization: Table for confidence \", currentConfSetting, \" at epoch \", currentEpochSetting, table);\n",
       "\t\t\t\tconsole.log(\"Visualization: Creating SVG...\");\n",
       "\n",
       "\t\t\t\t/*--------------------------------------------------------------------------------\n",
       "\t\t\t\tNOTE: Will leave this as the default viz size for now\n",
       "\t\t\t\t--------------------------------------------------------------------------------*/\n",
       "\n",
       "\t\t\t\tvar w = 750;\n",
       "\t\t\t\tvar h = 750;\n",
       "\n",
       "\t\t\t\tvar svg = d3.select(\"body\")\n",
       "\t\t\t\t\t\t\t.select(\"#matrix\")\n",
       "\t\t\t\t\t\t\t.append(\"svg\")\n",
       "\t\t\t\t\t\t\t.attr(\"width\", w)\n",
       "\t\t\t\t\t\t\t.attr(\"height\", h);\n",
       "\n",
       "\t\t\t\tvar rect = svg.selectAll(\"rect\")\n",
       "\t\t\t\t\t\t\t  .data(dataset)\n",
       "\t\t\t\t\t\t\t  .enter()\n",
       "\t\t\t\t\t\t\t  .append(\"rect\");\n",
       "\n",
       "\t\t\t\tvar counters = new Array(tableDimension * tableDimension).fill(0);\n",
       "\t\t\t\tvar ycounters = new Array(tableDimension * tableDimension).fill(0);\n",
       "\t\t\t\tvar cellDimension = h / tableDimension;\n",
       "\t\t\t\tvar blockStackDimension = Math.round(Math.sqrt(totalItems)) + 1;\n",
       "\t\t\t\tvar marginBuffer = 5;\n",
       "\t\t\t\tvar cubeDimension = ((cellDimension - marginBuffer) / blockStackDimension);\n",
       "\n",
       "\t\t\t\t/*--------------------------------------------------------------------------------\n",
       "\t\t\t\tFormat: d[trueLabel, predictedLabel, entryText, index, epoch]\n",
       "\t\t\t\t--------------------------------------------------------------------------------*/\n",
       "\n",
       "\t\t\t\t/*--------------------------------------------------------------------------------\n",
       "\t\t\t\tFilling each cell of the matrix with the proper number of squares\n",
       "                NOTE: boxes have the attributes: x, y, id, width, height, opacity, fill, and class\n",
       "\t\t\t\t--------------------------------------------------------------------------------*/\n",
       "\t\t\t\trect.attr(\"x\", function (d, i){\n",
       "\t\t\t\t\tvar matrixnum = (parseInt(d[1]) * tableDimension) + parseInt(d[0]);\n",
       "\t\t\t\t\tvar inmatrixcol = counters[matrixnum] % blockStackDimension;\n",
       "\t\t\t\t\tcounters[matrixnum]++;\n",
       "\t\t\t\t\treturn (d[1] * (cellDimension + marginBuffer)) + (inmatrixcol * (cubeDimension));\n",
       "\t\t\t\t\t})\n",
       "\t\t\t\t\t.attr(\"y\", function(d, i){\n",
       "\t\t\t\t\t\tvar matrixnum = (parseInt(d[1] * tableDimension) + parseInt(d[0]));\n",
       "\t\t\t\t\t\tvar hm = Math.floor(ycounters[matrixnum]/blockStackDimension);\n",
       "\t\t\t\t\t\tycounters[matrixnum]++;\n",
       "\t\t\t\t\t\treturn (d[0] * (cellDimension + marginBuffer)) + (hm * (cubeDimension));\n",
       "\t\t\t\t\t})\n",
       "\t\t\t\t\t.attr(\"id\", function(d){\n",
       "\t\t\t\t\t\treturn \"rect\" + d[3];\n",
       "\t\t\t\t\t})\n",
       "\t\t\t\t\t.attr(\"width\", function(d){\n",
       "\t\t\t\t\t\treturn cubeDimension;\n",
       "\t\t\t\t\t})\n",
       "\t\t\t\t\t.attr(\"height\", function(d){\n",
       "\t\t\t\t\t\treturn cubeDimension;\n",
       "\t\t\t\t\t})\n",
       "\t\t\t\t\t.attr(\"opacity\", function(d){\n",
       "\t\t\t\t\t\treturn 1;\n",
       "\t\t\t\t\t})\n",
       "\t\t\t\t\t.attr(\"fill\", function(d){\n",
       "\t\t\t\t\t\treturn (\"black\");\n",
       "\t\t\t\t\t})\n",
       "\t\t\t\t\t.attr(\"class\", function(d){\n",
       "\t\t\t\t\t\tpredicted_label = \"predicted_label_\" + d[1];\n",
       "\t\t\t\t\t\ttrue_label = \"true_label_\" + d[0];\n",
       "\t\t\t\t\t\treturn true_label + \" \" + predicted_label;\n",
       "\t\t\t\t});\n",
       "\n",
       "                // fetchDataWindowResults(d, 1, 1, (currentEpochSetting - 1), currentConfSetting);\n",
       "\n",
       "                rect.on(\"click\", function(d_on){\n",
       "                    console.log(\"Actual: \", d_on[0], \"Predicted: \", d_on[1]);\n",
       "                    var actual = d_on[0];\n",
       "                    var prediction = d_on[1];\n",
       "                    var selectedDataSet = fetchDataWindowResults(d, actual, prediction,\n",
       "                        (currentEpochSetting - 1), currentConfSetting);\n",
       "                    console.log(selectedDataSet);\n",
       "\n",
       "                    d3.select(\"#testList\").selectAll(\"li\").remove();\n",
       "                    for (var i = 0; i < selectedDataSet.length; i++){\n",
       "                        var tableRowData = selectedDataSet[i][1];\n",
       "                        var dataPointString = \"Label: \" + actual +\n",
       "                        \" Prediction: \" + prediction + \n",
       "                        \" Confidence Score: \" +  tableRowData['Test Confidence Score'][currentEpochSetting - 1] +\n",
       "                        \" Input Data: \" + tableRowData['Test Sentence'];\n",
       "                        d3.select(\"#testList\").append(\"li\").text(dataPointString);\n",
       "                        \n",
       "                    }\n",
       "\n",
       "                    d3.select(\"#testList\").append(\"li\").text(\"Appended\");\n",
       "                }\n",
       "                );\n",
       "\n",
       "/*\n",
       "\t\t\t\td3.select(\"#review\")\n",
       "\t\t\t\t\t.select(\"testList\")\n",
       "\t\t\t\t\t.selectAll(\"rect\")\n",
       "\t\t\t\t\t.data(\n",
       "\t\t\t\t\t\tdataset.filter(d => d[0] != d[1]),\n",
       "\t\t\t\t\t\tfunction(d){\n",
       "\t\t\t\t\t\t\treturn d[3];\n",
       "\t\t\t\t\t\t}\n",
       "\t\t\t\t\t)\n",
       "\t\t\t\t\t.enter()\n",
       "\t\t\t\t\t.append(\"li\")\n",
       "\t\t\t\t\t.attr(\"id\", function(d){\n",
       "\t\t\t\t\t\treturn \"text\" + d[3];\n",
       "\t\t\t\t\t})\n",
       "\t\t\t\t\t.html(function(d){\n",
       "\t\t\t\t\t\ttable = \"<table><tr>\"\n",
       "\t\t\t\t\t\ttable += \"<td> True: \";\n",
       "\t\t\t\t\t\ttable += parseInt(d[0]); //getType(d[0]);\n",
       "\t\t\t\t\t\ttable += \"</td>\"\n",
       "\t\t\t\t\t\ttable += \"<td> Predict: \";\n",
       "\t\t\t\t\t\ttable += parseInt(d[1]); //getType(d[1]);\n",
       "\t\t\t\t\t\ttable += \"</td>\"\n",
       "\t\t\t\t\t\ttable += \"<td>\" + d[2].substr(0,200); + \"</td>\"\n",
       "\t\t\t\t\t\ttable += \"</tr> </table>\"\n",
       "\t\t\t\t\t\treturn  table;\n",
       "\t\t\t\t});\n",
       "\n",
       "\t\t\t\trect.on(\"click\", function(d_on){\n",
       "\t\t\t\t\td3.select(\"#review\")\n",
       "\t\t\t\t\t\t.select(\"#testList\")\n",
       "\t\t\t\t\t\t.html(\"\");\n",
       "\t\t\t\t\tif(!this.classList.contains(\"past\")){\n",
       "\t\t\t\t\t\td3.selectAll(\".past\")\n",
       "\t\t\t\t\t\t\t.attr(\"fill\", \"pink\")\n",
       "\t\t\t\t\t\t\t.classed(\"past\", false);\n",
       "\t\t\t\t\t\td3.selectAll(\".reclick\")\n",
       "\t\t\t\t\t\t\t.attr(\"fill\", \"pink\")\n",
       "\t\t\t\t\t\t\t.classed(\"reclick\", false)\n",
       "\t\t\t\t\t}\n",
       "\t\t\t\t\tif(!this.classList.contains(\"reclick\")){\n",
       "\t\t\t\t\t\td3.selectAll(\".reclick\")\n",
       "\t\t\t\t\t\t\t.attr(\"fill\", \"pink\")\n",
       "\t\t\t\t\t\t\t.classed(\"reclick\", false);\n",
       "\t\t\t\t\t}\n",
       "\t\t\t\t\td3.select(this);\n",
       "\t\t\t\t\ttextId = \"\";\n",
       "\t\t\t\t\tx = \".\" + this.classList[0];\n",
       "\t\t\t\t\ty = \".\" + this.classList[1];\n",
       "\t\t\t\t\ttest = x + y;\n",
       "\t\t\t\t\tx1 = x.charAt(x.length - 1);\n",
       "\t\t\t\t\ty1 = y.charAt(y.length - 1);\n",
       "\t\t\t\t\tif(this.classList.contains(\"past\")){\n",
       "\t\t\t\t\t\td3.select(this)\n",
       "\t\t\t\t\t\t\t.classed(\"reclick\", true)\n",
       "\t\t\t\t\t\tId = this.id;\n",
       "\t\t\t\t\t\ttextId = \"#text\" + Id.substring(4);\n",
       "\t\t\t\t\t}\n",
       "\t\t\t\t\td3.selectAll(test)\n",
       "\t\t\t\t\t\t.attr(\"fill\", \"purple\")\n",
       "\t\t\t\t\t\t.classed(\"past\", \"true\");\n",
       "\t\t\t\t\td3.select(\"#review\")\n",
       "\t\t\t\t\t\t.select(\"#testList\")\n",
       "\t\t\t\t\t\t.selectAll(\"rect\")\n",
       "\t\t\t\t\t\t.data(\n",
       "\t\t\t\t\t\t\tdataset\n",
       "\t\t\t\t\t\t\t\t.filter(d => d[0] == x1)\n",
       "\t\t\t\t\t\t\t\t.filter(d => d[1] == y1),\n",
       "\t\t\t\t\t\t\t\tfunction(d){\n",
       "\t\t\t\t\t\t\t\t\treturn d[3];\n",
       "\t\t\t\t\t\t\t\t}\n",
       "\t\t\t\t\t\t)\n",
       "\t\t\t\t\t\t.enter()\n",
       "\t\t\t\t\t\t.append(\"li\")\n",
       "\t\t\t\t\t\t.attr(\"id\", function(d){\n",
       "\t\t\t\t\t\t\treturn \"text\" + d[3];\n",
       "\t\t\t\t\t\t})\n",
       "\t\t\t\t\t\t.html(function(d){\n",
       "\t\t\t\t\t\t\ttable = \"<table><tr>\"\n",
       "\t\t\t\t\t\t\ttable += \"<td> True: \";\n",
       "\t\t\t\t\t\t\ttable += parseInt(d[0]); //getType(d[0]);\n",
       "\t\t\t\t\t\t\ttable += \"</td>\"\n",
       "\t\t\t\t\t\t\ttable += \"<td> Predict: \";\n",
       "\t\t\t\t\t\t\ttable += parseInt(d[1]); //getType(d[1]);\n",
       "\t\t\t\t\t\t\ttable += \"</td>\"\n",
       "\t\t\t\t\t\t\ttable += \"<td>\" + d[2].substr(0,200); + \"</td>\"\n",
       "\t\t\t\t\t\t\ttable += \"</tr> </table>\"\n",
       "\t\t\t\t\t\t\treturn table;\n",
       "\t\t\t\t\t});\n",
       "\n",
       "\t\t\t\t\td3.select(\"#review\")\n",
       "\t\t\t\t\t\t.select(\"testList\")\n",
       "\t\t\t\t\t\t.selectAll(\"li\")\n",
       "\t\t\t\t\t\t.on(\"mouseover\", function(d_on){\n",
       "\t\t\t\t\t\t\td3.select(this)\n",
       "\t\t\t\t\t\t\t\t.classed(\"lighthigh\", true)\n",
       "\t\t\t\t\t\t\t\tid = this.id;\n",
       "\t\t\t\t\t\t\t\trectId = \"#rect\" + id.substring(4);\n",
       "\t\t\t\t\t\t\t\td3.selectAll(rectId)\n",
       "\t\t\t\t\t\t\t\t\t.attr(\"fill\", \"green\");\n",
       "\t\t\t\t\t\t})\n",
       "\t\t\t\t\t\t.on(\"mouseout\", function(d_on){\n",
       "\t\t\t\t\t\t\td3.select(this)\n",
       "\t\t\t\t\t\t\t\t.classed(\"lighthigh\", false)\n",
       "\t\t\t\t\t\t\t\tid = this.id;\n",
       "\t\t\t\t\t\t\t\trectId = \"#rect\" + id.substring(4);\n",
       "\t\t\t\t\t\t\t\td3.selectAll(rectId)\n",
       "\t\t\t\t\t\t\t\t\t.attr(\"fill\", \"purple\");\n",
       "\t\t\t\t\t});\n",
       "\t\t\t\t});\n",
       "*/\n",
       "\n",
       "\t\t\t\t/*--------------------------------------------------------------------------------\n",
       "\t\t\t\tChanging the color of rows in the Data section on mouseover/mouseout (repeat of above?)\n",
       "\t\t\t\t--------------------------------------------------------------------------------*/\n",
       "/*\n",
       "\t\t\t\td3.select(\"#review\")\n",
       "\t\t\t\t\t.select(\"#testList\")\n",
       "\t\t\t\t\t.selectAll(\"li\")\n",
       "\t\t\t\t\t.on(\"mouseover\", function(d_on){\n",
       "\t\t\t\t\t\td3.select(this)\n",
       "\t\t\t\t\t\t\t.classed(\"lighthigh\", true)\n",
       "\t\t\t\t\t\t\tid = this.id;\n",
       "\t\t\t\t\t\t\trectId = \"#rect\" + id.substring(4);\n",
       "\t\t\t\t\t\t\td3.selectAll(rectId)\n",
       "\t\t\t\t\t\t\t\t.attr(\"fill\", \"green\");\n",
       "\t\t\t\t\t})\n",
       "\t\t\t\t  .on(\"mouseout\", function(d_on){\n",
       "\t\t\t\t\t\td3.select(this)\n",
       "\t\t\t\t\t\t\t.classed(\"lighthigh\", false)\n",
       "\t\t\t\t\t\t\tid = this.id;\n",
       "\t\t\t\t\t\t\trectId = \"#rect\" + id.substring(4);\n",
       "\t\t\t\t\t\t\td3.selectAll(rectId)\n",
       "\t\t\t\t\t\t\t\t.attr(\"fill\", \"pink\");\n",
       "\t\t\t\t});\n",
       "*/\n",
       "\n",
       "\t\t\t});\n",
       "\t\t})\n",
       "\t\t </script>\n",
       "\t\t"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = mlvs.ConfusionMatrix(full_path_extended)\n",
    "cm.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-canal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
