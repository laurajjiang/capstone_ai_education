{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "burning-clinic",
   "metadata": {
    "id": "Eg62Pmz3o83v"
   },
   "source": [
    "## Creating a Logistic Regression Model\n",
    "\n",
    "Logistic Regression is classification algorithm commonly used in machine learning. It allows us to categorize data into discrete classes (binary categories) by learning the relationship from a given set of labeled data. A logistic regression model learns a linear relationship from the given dataset and then introduces a non-linearity in the form of the Sigmoid function, described below, to return a probability value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bulgarian-collection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script data-require=\"d3@3.5.3\" data-semver=\"3.5.3\" src=\"//cdnjs.cloudflare.com/ajax/libs/d3/3.5.3/d3.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from IPython.core.display import HTML, display\n",
    "# Initializing D3 for the entire notebook (this fixes error requiring kernel to load twice)\n",
    "# (feel free to move this line to anywhere in notebook before visualizations)\n",
    "display(HTML('<script data-require=\"d3@3.5.3\" data-semver=\"3.5.3\" src=\"//cdnjs.cloudflare.com/ajax/libs/d3/3.5.3/d3.js\"></script>'))\n",
    "  \n",
    "def sigmoid(z): \n",
    "    return 1 / (1 + np.exp( - z)) \n",
    "  \n",
    "plt.plot(np.arange(-5, 5, 0.1), sigmoid(np.arange(-5, 5, 0.1))) \n",
    "plt.title('Visualization of the Sigmoid Function') \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-transaction",
   "metadata": {
    "id": "iAsKG535pHep"
   },
   "source": [
    "## Load the dataset\n",
    "\n",
    "The Iris flower dataset is available on [Keras Dataset API](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html). \n",
    "\n",
    "The following code loads the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "extreme-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# import iris data from sklearn datasets library\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "introductory-dining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                  5.1               3.5                1.4               0.2   \n",
      "1                  4.9               3.0                1.4               0.2   \n",
      "2                  4.7               3.2                1.3               0.2   \n",
      "3                  4.6               3.1                1.5               0.2   \n",
      "4                  5.0               3.6                1.4               0.2   \n",
      "..                 ...               ...                ...               ...   \n",
      "145                6.7               3.0                5.2               2.3   \n",
      "146                6.3               2.5                5.0               1.9   \n",
      "147                6.5               3.0                5.2               2.0   \n",
      "148                6.2               3.4                5.4               2.3   \n",
      "149                5.9               3.0                5.1               1.8   \n",
      "\n",
      "     target  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "..      ...  \n",
      "145       2  \n",
      "146       2  \n",
      "147       2  \n",
      "148       2  \n",
      "149       2  \n",
      "\n",
      "[150 rows x 5 columns]\n",
      "(150, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# To use tenforflow 1.x functions, import compact v1\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.enable_eager_execution()\n",
    "# # # make unable to use tensorflow v2.x functions to avoid crash\n",
    "# tf.disable_v2_behavior()\n",
    "\n",
    "# change to pandas dataframe\n",
    "iris = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "iris = iris.astype({\"target\": int })\n",
    "\n",
    "print(iris)\n",
    "print(iris.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-premiere",
   "metadata": {
    "id": "l50X3GfjpU4r"
   },
   "source": [
    "## Explore the data \n",
    "\n",
    "Let's take a moment to understand the format of the data. Each data contains sepal length, sepal width, petal length, petal width and a corresponding species label. The label is an integer value of either 0 or 1, where 0 is a `Iris-setosa`, and 1 is a `Iris-versicolo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "coated-excerpt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the head of dataframe\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "representative-regard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7039b14ca0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEJCAYAAAB2T0usAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAq3UlEQVR4nO3de5xVdb3/8dfHYYrxkqNCJwERKrVyhpsIGnlBO6KJip6U8pJkSeZJ8cfRo5bHfHgsPdlJM3vIMc00vBEp3rOE8JJJMYCgIeU1uZwjSSAIGA6f3x9r7WFmz55ZazNrr7323u/n47EfM3uttb/rsxbj/rrW+n6+H3N3RESktu1Q7gBERKT81BmIiIg6AxERUWcgIiKoMxAREdQZiIgIKXQGZlZnZgvN7OEC6w43s3Vmtih8XV7qeEREpLNeKexjCrAU+FAX65929/EpxCEiIl0oaWdgZgOAY4HvAFOTaLNPnz4+aNCgJJoSEakZLS0tf3P3vl2tL/WVwfXAvwO7dLPNwWb2PLASuNDdX+yuwUGDBjF//vzkIhQRqQFm9kZ360v2zMDMxgNvuXtLN5stAPZ296HAj4BZXbQ12czmm9n81atXJx+siEiNK+UD5DHA8Wb2OnAPcISZTW+/gbu/4+4bwt8fBerNrE9+Q+5+s7uPdPeRfft2eZUjIiLbqWSdgbtf6u4D3H0Q8AVgjruf3n4bM/uImVn4+6gwnrdLFZOIiBSWxmiiDszsHAB3nwZ8Hvi6mb0PbAK+4JpGVaQibdmyheXLl7N58+Zyh1LTevfuzYABA6ivry/qc1Zp370jR450PUAWyZ7XXnuNXXbZhT322IPwgl9S5u68/fbbrF+/nsGDB3dYZ2Yt7j6yq8+mfmUgkqRZC1dw7ePLWLl2E/0aG7ho3H5MGN6/3GHVpM2bNzNo0CB1BGVkZuyxxx5sz0AbdQZSsWYtXMGl9y1h05ZWAFas3cSl9y0BUIdQJuoIym97/w00N5FUrGsfX9bWEeRs2tLKtY8vK1NEIpVLnYFUrJVrNxW1XKrfzjvv3OW6T3/60yXb73e/+92StZ0WdQZSsfo1NhS1XGpTa2tw9fjss8+WbB/qDETK6KJx+9FQX9dhWUN9HReN269MEUkxZi1cwZhr5jD4kkcYc80cZi1ckVjbc+fOZezYsZx66qk0NzcD264aVq1axaGHHsqwYcNoamri6aef7vT5F198kVGjRjFs2DCGDBnCX/7yFwCmT5/etvxrX/sara2tXHLJJWzatIlhw4Zx2mmnAfCDH/yApqYmmpqauP766wF49913OfbYYxk6dChNTU3ce++9AFx55ZUceOCBNDU1MXnyZMo1wlMPkKVi5R4SazRR5Unj4f8f/vAHXnjhhU5DLO+66y7GjRvHt771LVpbW9m4cWOnz06bNo0pU6Zw2mmn8Y9//IPW1laWLl3Kvffey+9+9zvq6+s599xzufPOO7nmmmu48cYbWbRoEQAtLS3cdtttzJs3D3dn9OjRHHbYYbz66qv069ePRx55BIB169YB8I1vfIPLLw9m7z/jjDN4+OGHOe644xI5B8VQZyAVbcLw/vryr0DdPfxP6t9z1KhRnToCgAMPPJCzzjqLLVu2MGHCBIYNG9Zpm4MPPpjvfOc7LF++nJNOOol99tmH2bNn09LSwoEHHhjEu2kTH/7whzt99plnnuHEE09kp512AuCkk07i6aef5uijj+bCCy/k4osvZvz48RxyyCEA/Pa3v+V73/seGzduZM2aNey///5l6Qx0m0hEUpfGw//cl3G+Qw89lKeeeor+/ftzxhlncMcdd3D//fczbNgwhg0bxvz58zn11FN58MEHaWhoYNy4ccyZMwd358wzz2TRokUsWrSIZcuWccUVV3Rqv6vbPPvuuy8tLS00Nzdz6aWXcuWVV7J582bOPfdcZs6cyZIlSzj77LPLlsGtzkBEUlfOh/9vvPEGH/7whzn77LP5yle+woIFCzjxxBPbvuRHjhzJq6++ykc/+lHOP/98jj/+eBYvXsyRRx7JzJkzeeuttwBYs2YNb7wRzApdX1/Pli1bgKCzmTVrFhs3buTdd9/l/vvv55BDDmHlypXsuOOOnH766Vx44YUsWLCg7Yu/T58+bNiwgZkzZ5b8+Lui20QikrqLxu3X4ZkBpPfwf+7cuVx77bXU19ez8847c8cdd3Ta5t5772X69OnU19fzkY98hMsvv5zdd9+dq666iqOOOoqtW7dSX1/Pj3/8Y/bee28mT57MkCFDGDFiBHfeeSeTJk1i1KhRAHz1q19l+PDhPP7441x00UXssMMO1NfXc9NNN9HY2MjZZ59Nc3MzgwYNarsFVQ6am0hEErF06VI++clPxt5eU4mUTqF/C81NJCKZpIf/2aJnBiIios5ARETUGYiICOoMREQEPUCWMtJoEpHs0JWBlEVubpoVazfhbJubJsnJyqT2lGsK6zhWrlzJ5z//+e367OGHH06ph9SrM5CyUGEaSUsaU1i39/777xdc3q9fv7JmGEdRZyBlocI0wuIZcF0TXNEY/Fw8I7GmezKF9bp16xg0aBBbt24FYOPGjey1115s2bKFV155haOPPpoDDjiAQw45hJdeegmASZMmMXXqVMaOHcvFF1/Mk08+2TbX0fDhw1m/fj2vv/46TU1NQNBBXXjhhTQ3NzNkyBB+9KMfATB79myGDx9Oc3MzZ511Fu+9916nY7v77rtpbm6mqamJiy++OLFzpmcGUhb9GhtYUeCLX4VpasTiGfDQ+bAl/BtY92bwHmDIKYnsYnunsN51110ZOnQoTz75JGPHjuWhhx5i3Lhx1NfXM3nyZKZNm8Y+++zDvHnzOPfcc5kzZw4Af/7zn3niiSeoq6vjuOOO48c//jFjxoxhw4YN9O7du8M+br75Zl577TUWLlxIr169WLNmDZs3b2bSpEnMnj2bfffdly996UvcdNNNXHDBBW2fW7lyJRdffDEtLS3stttuHHXUUcyaNYsJEyb0+HzpykDKQoVpatzsK7d1BDlbNgXLE9LdFNa33XYbV1xxBUuWLGGXXXbptM3EiRPbis/cc889TJw4kQ0bNvDss89y8skntxW3WbVqVdtnTj75ZOrqgr/pMWPGMHXqVG644QbWrl1Lr14d/7/7iSee4Jxzzmlbvvvuu7Ns2TIGDx7MvvvuC8CZZ57JU0891eFzf/zjHzn88MPp27cvvXr14rTTTuu0zfZSZyBlMWF4f64+qZn+jQ0Y0L+xgatPatZoolqxbnlxy7dDT6awPv7443nsscdYs2YNLS0tHHHEEWzdupXGxsa22U0XLVrE0qVLC+7vkksu4ZZbbmHTpk0cdNBBbbeTctwdM+u0LEop55LTbSIpG81NU8N2HRDcGiq0vMTeeOMN+vfvz9lnn827777LggULuP766znxxBM7bDdq1CimTJnC+PHjqaur40Mf+hCDBw/mF7/4BSeffDLuzuLFixk6dGinfbzyyis0NzfT3NzM73//e1566aUORXSOOuoopk2bxuGHH952m+gTn/gEr7/+Oi+//DIf//jH+fnPf85hhx3Wod3Ro0czZcoU/va3v7Hbbrtx9913c9555yVyXnRlIAWVsj6tCEdeDvV5z4fqG4LlJTZ37ty2B7u//OUvmTJlSsHtJk6cyPTp05k4cWLbsjvvvJNbb72VoUOHsv/++/PAAw8U/Oz1119PU1MTQ4cOpaGhgWOOOabD+q9+9asMHDiQIUOGMHToUO666y569+7Nbbfdxsknn0xzczM77LAD55xzTofP7bnnnlx99dWMHTuWoUOHMmLECE444YQenpGAprCWTvLr00JwP1+3caQ7xU5hzeIZwTOCdcuDK4IjL0/s4XGt0xTWkog06tOKMOQUfflniG4TSSfKARCpPeoMpJNy1qeVylZpt52r0fb+G6gzkE6UAyDbo3fv3rz99tvqEMrI3Xn77bc7JbnFoWcG0knuuYBmFJViDBgwgOXLl7N69epyh1LTevfuzYABxQ/RLfloIjOrA+YDK9x9fN46A34IfA7YCExy9wXdtafRRCIixcvCaKIpwFLgQwXWHQPsE75GAzeFP0USoZoJIvGU9JmBmQ0AjgVu6WKTE4A7PPAc0Ghme5YyJqkdqpkgEl+pHyBfD/w7sLWL9f2B9jnpy8NlIj2mmgki8ZWsMzCz8cBb7t7S3WYFlnV6iGFmk81svpnN18MpiUv5EiLxlfLKYAxwvJm9DtwDHGFm0/O2WQ7s1e79AGBlfkPufrO7j3T3kX379i1VvFJllC8hEl/JOgN3v9TdB7j7IOALwBx3Pz1vsweBL1ngIGCdu6/Kb0tkeyhfQiS+1PMMzOwcAHefBjxKMKz0ZYKhpV9OOx6pXsqXEIlPs5aKiNSALOQZSA26bNYS7p73Jq3u1JnxxdF7cdWE5nKHJSJdUGcgibts1hKmP/fXtvet7m3v1SGIZJMmqpPE3T2vQDnDbpaLSPmpM5DEtXbxHKqr5SJSfuoMJHF1ViiXsOvlIlJ+6gwkcV8cvVdRy0Wk/PQAWRKXe0is0UQilUN5BiIiNSAqz0C3iURERLeJatFpP/k9v3tlTdv7MR/bnTvPPriMEW0/Fa+RzFs8A2ZfCeuWw64D4MjLYcgp6bcRQVcGNSa/IwD43StrOO0nvy9TRNtPxWsk8xbPgIfOh3VvAh78fOj8YHmabcSgzqDG5HcEUcuzTMVrJPNmXwlb8upnbNkULE+zjRjUGUjFUvEaybx1y4tbXqo2YlBnIBVLxWsk83YdUNzyUrURgzqDGjPmY7sXtTzLVLxGMu/Iy6E+739O6huC5Wm2EYM6gxpz59kHd/rir9TRRBOG9+fqk5rp39iAAf0bG7j6pGaNJpLsGHIKHHcD7LoXYMHP424obiRQEm3EoKQzEZEaoOI20kkSY/Oj2tD4f5HKos6gxuTG5ueGZObG5gOxv6yj2khiHyKSLj0zqDFJjM2PakPj/0UqjzqDGpPE2PyoNjT+X6TyRN4mMrORwCFAP2AT8ALwhLtXXsqq0K+xgRUFvpSLGZsf1UYS+xCRdHV5ZWBmk8xsAXAp0AAsA94CPgP8xsxuN7OB6YQpSUlibH5UGxr/L1J5ursy2AkY4+4Fr+3NbBiwD/DXEsQlJZJ7gNuTkT5RbSSxDxFJl/IMRERqQI/zDMxsMHAeMKj99u5+fBIBVpM0xtbH2YfG+EtNSGGO/1oSJ89gFnAr8BCwtaTRVLA0xtbH2YfG+EtNyM3xn5vaOTfHP6hD2E5xhpZudvcb3P237v5k7lXyyCpMGmPr4+xDY/ylJqQ0x38tiXNl8EMz+zbwa+C93EJ3X1CyqCpQGmPr4+xDY/ylJqQ0x38tidMZNANnAEew7TaRh+8llMbY+jj70Bh/qQm7DgjLQBZYLtslzm2iE4GPuvth7j42fKkjyJPG2Po4+9AYf6kJKc3xX0viXBk8DzQSJJxJF9IYWx9nHxrjLzUh95BYo4kSE5lnYGZzgSHAH+n4zKAsQ0uVZyAiUrwk6hl8ezt33Bt4CvhguJ+Z7v7tvG0OBx4AXgsX3efuGg7QQ5fNWsLd896k1Z06M744ei+umtAcez1kJ2dCRNIRpzP4K7DK3TcDmFkD8E8xPvcecIS7bzCzeuAZM3vM3Z/L2+5pdx9fVNTSpctmLWH6c9tmCGl1b3t/1YTmyPWQnZwJEUlPnAfIv6BjsllruKxbHtgQvq0PX5U190UFuntegREW7ZZHrYfs5EyISHridAa93P0fuTfh7x+I07iZ1ZnZIoKHz79x93kFNjvYzJ43s8fMbP8u2plsZvPNbP7q1avj7LpmtXbxDCi3PGo9ZCdnQkTSE6czWG1mbQ+LzewE4G9xGnf3VncfBgwARplZU94mC4C93X0o8COCqS8KtXOzu49095F9+/aNs+uaVWfW7fKo9dB1TkLSOROl3oeIxBenMzgH+KaZ/dXM/gpcDEwuZifuvhaYCxydt/yd3K0kd38UqDezPsW0LR19cfRe3S6PWg/ZyZkQkfREPkB291eAg8xsZ4KhqOvjNGxmfYEt7r42fOj8WeC/8rb5CPB/7u5mNoqgc3q72IOQbXIPgbsaLRS1HrKTMyEi6ekyz8DMTgfucveCM5Wa2ceAPd39mS7WDwFuB+oIvuRnuPuVZnYOgLtPM7NvAF8H3icoqTnV3Z/tLmDlGYiIFK8neQZ7AAvNrAVoAVYDvYGPA4cRPDe4pKsPu/tiYHiB5dPa/X4jcGPEMYiISIl12Rm4+w/N7EaCCenGEGQhbwKWAme4u8pd5kkiiSpOQlhP20ijQE4Sx5EZSRRRidOGirVIGXX7zMDdW4HfhC/pRhJJVHESwnraRhoFcpI4jsxIoohKnDZUrEXKLM5oIokhiSSqOAlhPW0jjQI5SRxHZiRRRCVOGyrWImWmziAhSSRRxUkI62kbaRTISeI4MiOJIipx2lCxFikzdQYJSSKJKk5CWE/biBNnT48liePIjK6KpRRTRCVOG0nsR6QHIjsDM/ugmZ1qZt80s8tzrzSCqyRJJFHFSQjraRtpFMhJ4jgyI4kiKnHaULEWKbM4s5Y+AKwjGF76XsS2NSuJJKo4CWE9bSONAjlJHEdmJFFEJU4bKtYiZRanuM0L7p4/p1DZKOlMRKR4SRS3edbMmt19SYJxSQlF5QioqExGPTwVWn4G3gpWBwdMgvE/qL0YpCy67AzMbAlB/YFewJfN7FWC20RGUK5gSDohSjGicgRUVCajHp4K82/d9t5bt71P68s4CzFI2XT3AHk8cBxwDMEUFEeF73PLJYOicgRUVCajWn5W3PJqjUHKprvpKN4AMLOfu/sZ7deZ2c+BMwp+UMoqKkdARWUyyluLW16tMUjZxMkz6FB9zMzqgANKE470VFSOgIrKZJTVFbe8WmOQsumyMzCzS81sPTDEzN4JX+sJSlg+kFqEUpSoHAEVlcmoAyYVt7xaY5Cy6e420dXA1WZ2tbtfmmJM0gNROQIqKpNRuQe05RzJk4UYpGy6K24zorsPuvuCkkQUQXkGIiLF60mewX+HP3sDI4HnCYaVDgHmAZ9JKsgsSGLsfVQbac3xrzyCIlVKHYGoHIC0jiNqP1mJQ4rS3W2isQBmdg8wOZd0ZmZNwIXphJeOJMbeR7WR1hz/yiMoUqXUEYjKAUjrOKL2k5U4pGhxRhN9on32sbu/AAwrWURlkMTY+6g20prjX3kERaqUOgJROQBpHUfUfrIShxQtznQUS83sFmA6QUby6QSlL6tGEmPvo9pIa45/5REUqVLqCETlAKR1HFH7yUocUrQ4VwZfBl4EpgAXAH8Kl1WNJMbeR7WR1hz/yiMoUqXUEYjKAUjrOKL2k5U4pGiRnYG7b3b369z9xPB1nbtvTiO4tCQx9j6qjbTm+FceQZEqpY5AVA5AWscRtZ+sxCFF626iuhnufkq7Ces6qKaJ6pIYex/VRlpz/CuPoEiVUkcgKgcgreOI2k9W4pCidZdnsKe7rzKzvQutz81dlDblGYiIFG+78wzcfVX465HA0+7+l6SDqzbVlKsgGZTGuPrbj4fXntz2fvBhcOaD6bchqYvzAHkQ8D9m9oqZzTCz88xsWGnDqjy58f0r1m7C2Ta+f9bCFYm1kctVyI1AyuUqXDZLdYeqXm5c/bo3Ad82rn7xjOT2kf8lDsH7249Ptw0pizgPkC939yOAJuAZ4CKCesjSTjXlKkgGpTGuPv9LPGp5qdqQsojMMzCzy4AxwM7AQoLs46dLHFfFqaZcBckgjauXEotzm+gkYA/gCeA+4MF2zxMkVE25CpJBGlcvJRbnNtEIgofIfwD+GVhiZs+UOrBKU025CpJBaYyrH3xYcctL1YaURWRnEE5MdzpwJjARWA7MKXFcFWfC8P5cfVIz/RsbMKB/YwNXn9RcdK5Cd21cNaGZ0w8a2HYlUGfG6QcN1GiiWjDkFDjuBth1L8CCn8fdkOxoojMf7PylXexIoCTakLLoMs+gbQOzR4AnCR4e/9Hdt6QRWFeUZyAiUrye1DMAwN2P3c4d9waeAj4Y7memu387bxsDfgh8DtgITCpX0RwRkVoWZ9bS7fUecIS7bzCzeuAZM3vM3Z9rt80xwD7hazRwU/gzUXGSwbJSECYqqaxijiWJBKmoYi5p7SfOPuLEWmpxkr2ijiWtcx4lzj6yUNymUuKMoWSdgQf3nzaEb+vDV/49qROAO8JtnzOzxtw0GEnFEafYS1YKwkQVwKmYY0mi8EhUMZe09hNnH3FiLbXukr1yHULUsaR1zqPE2UcWittUSpwxxRlaut3MrM7MFgFvAb9x93l5m/QH2mdMLQ+XJSZOMlhWCsJEJZVVzLEkkSAVVcwlrf3E2UecWEstTrJX1LGkdc6jxNlHForbVEqcMXU3a+lDFJitNMfdI/PL3b0VGGZmjcD9ZtYUVkpr202hjxWIZTIwGWDgwIFRu+0gTjJYVgrCRCWVVcyxJJEgFVXMJa39xNlHnFizIOpY0jrnUeLsIwtJeJUSZ0zd3Sb6flI7cfe1ZjYXOBpo3xksB9oPkh8ArCzw+ZuBmyEYTVTMvvs1NrCiwBdh+wSvONukoc6sYIeQG0paMcey64BwDp0Cy+OyusJfTu2LvKSxnzj7iBNrFkQdS1rnPEqcfaQRR5RKiTOmLm8TufuT3b2iGjazvuEVAWbWAHwWeClvsweBL1ngIGBd0tnNcZLBslIQJiqprGKOJYkEqahiLmntJ84+4sRaanGSvaKOJa1zHiXOPrJQ3KZS4owpztxE+wBXA58CeueWu/tHIz66J3C7mdURdDoz3P1hMzsn/Pw04FGCYaUvEwwtTbycZpxiL1kpCBNVAKdijiWJwiNRxVzS2k+cfcSJtdTOfDB6NFHUsaR1zqPE2UcWittUSpwxxUk6ewb4NnAdcBzBF7bl5wykRUlnIiLFi0o6izOaqMHdZxN0AG+4+xXAEUkFmBWzFq5gzDVzGHzJI4y5Zk5RdQikRBbPgOua4IrG4GehufvjbJOFONJqI4ljqRa1dKwJiJNnsNnMdgD+YmbfAFYAHy5tWOnKxNh86SgrY7iTiCOtNpI4lmpRS8eakDhXBhcAOwLnAwcAZxBMWlc1MjE2XzrKyhjuJOJIq40kjqVa1NKxJiTO3ER/BAivDs539/UljyplmRibLx1lZQx3EnGk1UaUChrz3mO1dKwJiTOF9UgzWwIsJqhl8LyZHVD60NKTRGEaSVicYi5pFHxJIo602ohSSwVyaulYExLnNtFPgXPdfZC7DwL+FbitpFGlLBNj86WjrIzhTiKOtNpI4liqRS0da0LidAbr3b2t5rG7PwNU1a2iJArTSMLiFHNJo+BLEnGk1UYSx1ItaulYExInz+A6ggfIdxPMGzQR+DvwS4C06w8oz0BEpHg9Lm4DDAt/5ieZfZqgc6i6nAPJiCTqCKQ1l3wScfS01kBax1oh8/PHkpXaDBkQZzTR2DQCEekgiToCaY01TyKOntYaSOtYq2n8flbyVDIizmiifzKzW83ssfD9p8zsK6UPTWpaEnUE0hprnkQcPa01kNaxVtP4/azkqWREnAfIPwMeB/qF7/9MkIgmUjpJ1BFIa6x5EnH0tNZAWsdaTeP3s5KnkhFxOoM+7j4D2Arg7u8DGavaIVUnzjjxruoFtK9FUEzb2yuJOKK2ycqxVtP4/azkqWREnM7gXTPbg7ACWa7uQEmjEkmijkBaY82TiKOntQbSOtZqGr+flTyVjIgzmmgqQRGaj5nZ74C+wOdLGpVIEnUE0ppLPok4elprIK1jraD5+SNlpTZDRkTmGQCYWS9gP4KaxcvcfUupA+uK8gxERIrX4zwDMzsZ+JW7v2hmlwEjzOyqtJPNJGVZGBudRAw3joa/tau22ucT8I156ceRxH6y8G8iVSvOM4P/cPf1ZvYZYBxwO3BTacOSssqNjV73JuDbxkanWRwkiRjyOwII3t84Ot04kthPFv5NpKrF6QxyI4eOBW5y9weAD5QuJCm7LIyNTiKG/I4ganmp4khiP1n4N5GqFqczWGFm/wOcAjxqZh+M+TmpVFkYG52FGNKMI416BiLdiPOlfgpB0tnR7r4W2B24qJRBSZllYWx0FmJIM4406hmIdCOyM3D3je5+n7v/JXy/yt1/XfrQpGyyMDY6iRj6fKK45aWKI4n9ZOHfRKqabvdIZ1mYCz6JGL4xr/MXf7GjidI6F2nUMxDpRqw8gyxRnoGISPGi8gx0ZSDls3gGXNcEVzQGP7dnmGRUG0nsI4k4pHZVyN9GnOkoRJKXxDzvUW1ojn8ptwr629CVgZRHEuPmszI2XzkA0pUK+ttQZyDlkcS4+ayMzVcOgHSlgv421BlIeSQxbj4rY/OVAyBdqaC/DXUGUh5JjJvPyth85QBIVyrob0OdgZRHEuPmszI2XzkA0pUK+ttQnoGISA0oW56Bme1lZr81s6Vm9qKZTSmwzeFmts7MFoWv7F07iYjUgFLmGbwP/Ju7LzCzXYAWM/uNu/8pb7un3X18CeOoLkkUOMlKkZQkirlk5ViS8PDUrstapqWazqcUpWSdgbuvAlaFv683s6VAfyC/M5C40kjUSksSCWNZOZYkPDwV5t+67b23bnufVodQTedTipbKA2QzGwQMBwrNEHawmT1vZo+Z2f5pxFOx0kjUSksSCWNZOZYktPysuOWlUE3nU4pW8ukozGxn4JfABe7+Tt7qBcDe7r7BzD4HzAL2KdDGZGAywMCBA0sbcJalkaiVliQSxrJyLEnw1uKWl0I1nU8pWkmvDMysnqAjuNPd78tf7+7vuPuG8PdHgXoz61Ngu5vdfaS7j+zbt28pQ862NBK10pJEwlhWjiUJVlfc8lKopvMpRSvlaCIDbgWWunvBm55m9pFwO8xsVBjP26WKqeKlkaiVliQSxrJyLEk4YFJxy0uhms6nFK2Ut4nGAGcAS8xsUbjsm8BAAHefBnwe+LqZvQ9sAr7glZb4kKbcQ7yejPZIoo0kRMURJ86sHEsScg+JyzmaqJrOpxRNSWciIjUgKulM9QwqTTWNA8/CuHoRAdQZVJZqGgeehXH1ItJGE9VVkmoaB56FcfUi0kadQSWppnHgWRhXLyJt1BlUkmoaB56FcfUi0kadQSWppnHgWRhXLyJt1BlUkgoqlBFp/A9g5Fe2XQlYXfBeD49FykJ5BiIiNUB5BgmZtXAF1z6+jJVrN9GvsYGLxu3HhOH9yx1WYZWSi1ApcaZF50PKSJ1BDLMWruDS+5awaUsw0mXF2k1cet8SgOx1CJWSi1ApcaZF50PKTM8MYrj28WVtHUHOpi2tXPv4sjJF1I1KyUWolDjTovMhZabOIIaVazcVtbysKiUXoVLiTIvOh5SZOoMY+jU2FLW8rColF6FS4kyLzoeUmTqDGC4atx8N9R2ToRrq67ho3H5liqgblZKLUClxpkXnQ8pMD5BjyD0krojRRJUyJ32lxJkWnQ8pM+UZiIjUAOUZiPRUEnUXlEMgGafOQKQ7SdRdUA6BVAA9QBbpThJ1F5RDIBVAnYFId5Kou6AcAqkA6gxEupNE3QXlEEgFUGcg0p0k6i4oh0AqgDoDke4kUXehmupQSNVSnoGISA2IyjPQlYGIiKgzEBERdQYiIoI6AxERQZ2BiIigzkBERFBnICIiqDMQERFK2BmY2V5m9lszW2pmL5rZlALbmJndYGYvm9liMxtRqnhERKRrpbwyeB/4N3f/JHAQ8K9m9qm8bY4B9glfk4GbShhP7Vg8A65rgisag5+LZ5Q7IhHJuJJ1Bu6+yt0XhL+vB5YC+UWDTwDu8MBzQKOZ7VmqmGpCrpDKujcB31ZIRR2CiHQjlWcGZjYIGA7My1vVH3iz3fvldO4wpBgqpCIi26HknYGZ7Qz8ErjA3d/JX13gI51mzjOzyWY238zmr169uhRhVg8VUhGR7VDSzsDM6gk6gjvd/b4CmywH9mr3fgCwMn8jd7/Z3Ue6+8i+ffuWJthqoUIqIrIdSjmayIBbgaXu3tXk7w8CXwpHFR0ErHP3VaWKqSaokIqIbIdeJWx7DHAGsMTMFoXLvgkMBHD3acCjwOeAl4GNwJdLGE9tyBVMmX1lcGto1wFBR6BCKiLSDRW3ERGpASpuIyIikdQZiIiIOgMREVFnICIiqDMQEREqcDSRma0G3ihjCH2Av5Vx/8WolFgVZ7IqJU6onFirIc693b3LrN2K6wzKzczmdzc8K0sqJVbFmaxKiRMqJ9ZaiFO3iURERJ2BiIioM9geN5c7gCJUSqyKM1mVEidUTqxVH6eeGYiIiK4MREREnUG3zKzOzBaa2cMF1h1uZuvMbFH4Kssc0Wb2upktCWPoNINfOD34DWb2spktNrMR5YgzjCUq1qyc00Yzm2lmL5nZUjM7OG99Js5pjDizcj73axfDIjN7x8wuyNum7Oc0ZpxZOaf/z8xeNLMXzOxuM+udt7748+nuenXxAqYCdwEPF1h3eKHlZYjxdaBPN+s/BzxGUFXuIGBehmPNyjm9Hfhq+PsHgMYsntMYcWbifObFVAf8L8GY98yd0xhxlv2cEpQGfg1oCN/PACb19HzqyqALZjYAOBa4pdyx9NAJwB0eeA5oNLM9yx1UVpnZh4BDCQoz4e7/cPe1eZuV/ZzGjDOLjgRecff8xNGyn9M8XcWZFb2ABjPrBexI5wqRRZ9PdQZdux74d2BrN9scbGbPm9ljZrZ/OmF14sCvzazFzCYXWN8feLPd++XhsnKIihXKf04/CqwGbgtvEd5iZjvlbZOFcxonTij/+cz3BeDuAsuzcE7b6ypOKPM5dfcVwPeBvwKrCCpE/jpvs6LPpzqDAsxsPPCWu7d0s9kCgkvIocCPgFlpxFbAGHcfARwD/KuZHZq33gp8plxDyKJizcI57QWMAG5y9+HAu8Aledtk4ZzGiTML57ONmX0AOB74RaHVBZaV5e80Is6yn1Mz243g//wHA/2Anczs9PzNCny02/OpzqCwMcDxZvY6cA9whJlNb7+Bu7/j7hvC3x8F6s2sT9qBuvvK8OdbwP3AqLxNlgN7tXs/gM6XlKmIijUj53Q5sNzd54XvZxJ86eZvU+5zGhlnRs5ne8cAC9z9/wqsy8I5zekyzoyc088Cr7n7anffAtwHfDpvm6LPpzqDAtz9Uncf4O6DCC4X57h7h57XzD5iZhb+PorgXL6dZpxmtpOZ7ZL7HTgKeCFvsweBL4WjCw4iuKRclWacufiiYs3COXX3/wXeNLP9wkVHAn/K26zs5zROnFk4n3m+SNe3Xsp+TtvpMs6MnNO/AgeZ2Y5hLEcCS/O2Kfp89ipNrNXJzM4BcPdpwOeBr5vZ+8Am4AsePsZP0T8B94d/m72Au9z9V3lxPkowsuBlYCPw5ZRjLCbWLJxTgPOAO8PbBa8CX87oOY2KMyvnEzPbEfhn4GvtlmXunMaIs+zn1N3nmdlMgltW7wMLgZt7ej6VgSwiIrpNJCIi6gxERAR1BiIigjoDERFBnYGIiKDOQGqcBbNQdjUrbaflCexvgpl9qt37uWYWWbPWzPZMIh4z62tmv+ppO1J91BmIpGsC8KmojQqYCvykpzt399XAKjMb09O2pLqoM5BMCzOXHwknBnvBzCaGyw8wsyfDSe8et3BGxvD/tK83s2fD7UeFy0eFyxaGP/frbr8FYvipmf0x/PwJ4fJJZnafmf3KzP5iZt9r95mvmNmfw3h+YmY3mtmnCea8udaCufA/Fm5+spn9Idz+kC7C+BfgV2HbdWb2fQtqQyw2s/PC5a+b2XfN7PdmNt/MRoTn5pVcQlJoFnBa3OOX2qAMZMm6o4GV7n4sgJntamb1BJOEneDuq8MO4jvAWeFndnL3T1swEd5PgSbgJeBQd3/fzD4LfJfgCzaObxFMSXKWmTUCfzCzJ8J1w4DhwHvAMjP7EdAK/AfBXEHrgTnA8+7+rJk9SDAf/szweAB6ufsoM/sc8G2CuWfamNlg4O/u/l64aDLBJGXDw+PZvd3mb7r7wWZ2HfAzgnm2egMvAtPCbeYDV8U8dqkR6gwk65YA3zez/yL4En3azJoIvuB/E36Z1hFM5ZtzN4C7P2VmHwq/wHcBbjezfQhmb6wvIoajCCYuvDB83xsYGP4+293XAZjZn4C9gT7Ak+6+Jlz+C2Dfbtq/L/zZAgwqsH5Pgumqcz4LTHP398PjXNNu3YPhzyXAzu6+HlhvZpvNrDGsefAWwWyXIm3UGUimufufzewAgnlWrjazXxPMePqiux/c1ccKvP9P4LfufqKZDQLmFhGGAf/i7ss6LDQbTXBFkNNK8N9UoemDu5NrI/f5fJsIOqD28XQ1j0yura15sW1t13bvsE2RNnpmIJlmZv2Aje4+naCgxwhgGdDXwpq/ZlZvHYuM5J4rfIZgtsZ1wK7AinD9pCLDeBw4z6xttsrhEdv/ATjMzHazoBJV+9tR6wmuUorxZzpeMfwaOCdsm7zbRHHsS+fZbaXGqTOQrGsmuEe/iODe/VXu/g+C2SP/y8yeBxbRcT73v5vZswT3yL8SLvsewZXF7whuKxXjPwluKy02sxfC910KK1F9F5gHPEEwtfS6cPU9wEXhg+iPddFEfnvvAq+Y2cfDRbcQTGO8ODz+U4s8nrHAI0V+RqqcZi2VqmJmc4EL3X1+mePY2d03hP/3fj/wU3e/vwftnQgc4O6XJRDbUwQP3//e07akeujKQKQ0rgivZl4AXqOH5RHDjuT1ngZlZn2BH6gjkHy6MhAREV0ZiIiIOgMREUGdgYiIoM5ARERQZyAiIqgzEBER4P8DhtRZxDHXDrEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a scatter plot with sepal length and width between two iris species in the dataframe\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(iris['sepal length (cm)'][:50], iris['sepal width (cm)'][:50], label='Iris-setosa')\n",
    "plt.scatter(iris['sepal length (cm)'][51:], iris['sepal width (cm)'][51:], label='Iris-versicolo')\n",
    "# plt.scatter(iris['sepal length (cm)'][101:], iris['sepal width (cm)'][101:], label='Iris-virginica')\n",
    "plt.xlabel('sepal length (cm)')\n",
    "plt.ylabel('sepal width (cm)')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-replication",
   "metadata": {
    "id": "h5bmxzE0Fobd"
   },
   "source": [
    "## Processing Data\n",
    "\n",
    "We need to process our data to split the overall dataset into a training set and a test set.\n",
    "\n",
    "The x-value from our dataset will become the features (sepal length, sepal width, petal length, petal width) and y value as species (labels -- `Iris-setosa` or `Iris-versicolo`) from the Iris dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ordinary-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = iris.drop(labels=['target'], axis=1).values\n",
    "y = iris['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-algorithm",
   "metadata": {},
   "source": [
    "Set a seed to get reproducibility for numpy and tensorflow so for the next step, our training and test dataset are split the same way each time you run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "protected-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 23\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-passion",
   "metadata": {},
   "source": [
    "Next, we split the dataset into a training set (60%, used to train our model and familiarize the model with the kind of data it will be classifying) and a test set (40%, evaluate the model and how effective it is as classifying new, unseen before data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "changed-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use random choice from numpy library to set dataset randomly\n",
    "train_data = np.random.choice(len(x), round(len(x) * 0.6), replace=False)\n",
    "test_data = np.array(list(set(range(len(x))) - set(train_data)))\n",
    "\n",
    "# separate the dataset into features and labels\n",
    "x_train = x[train_data]\n",
    "y_train = y[train_data]\n",
    "x_test = x[test_data]\n",
    "y_test = y[test_data]\n",
    "\n",
    "# the number of labels\n",
    "num_labels = 3 \n",
    "\n",
    "# the number of features: sepal length & width, petal length & width\n",
    "num_features = 4\n",
    "\n",
    "# Convert to float32.\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "\n",
    "x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-submission",
   "metadata": {},
   "source": [
    "Now, we normalize the feature values in the dataset. Normalization is optional for logistic regression, however, the main goal of normalizing features is to help the convergence of the technique used for optimization. \n",
    "\n",
    "(TODO: explain a bit more about normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "confidential-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the normalization function\n",
    "def min_max_normalization(data):\n",
    "    col_max = np.max(data, axis=0)\n",
    "    col_min = np.min(data, axis=0)\n",
    "    return np.divide(data - col_min, col_max - col_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "original-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized processing, must be placed after the data set segmentation, \n",
    "# otherwise the test set will be affected by the training set\n",
    "x_train = min_max_normalization(x_train)\n",
    "x_test = min_max_normalization(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "considered-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make y dataset shape to fit on model dimensions as multi-class classification\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, 3)\n",
    "y_test = to_categorical(y_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adopted-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-helicopter",
   "metadata": {
    "id": "LLC02j2g-llC"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "With the processed dataset, we can now start to build the model with Tensorflow and Keras. You'll notice that our model has an activation function, which defines the output range of our model. As mentioned, we will be using the Sigmoid function as our activation function to normalize our output between 0 and 1. \n",
    "\n",
    "There are a few other activation functions that you can try out and see how they affect the model. Learn about more activation functions through the [Keras documentation](https://keras.io/api/layers/activations/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sitting-april",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 15\n",
      "Trainable params: 15\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import L1L2\n",
    "\n",
    "# Set up the logistic regression model\n",
    "model = Sequential()\n",
    "# the number of class, Iris dataset has 3 classes\n",
    "output_dim = num_labels\n",
    "# input dimension = number of features your data has\n",
    "input_dim = num_features\n",
    "\n",
    "model.add(Dense(output_dim,\n",
    "                input_dim = input_dim,\n",
    "                activation='sigmoid'\n",
    "                )) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-spanish",
   "metadata": {},
   "source": [
    "Before training our model, we also need to set a few parameters: learning rate, batch size, and the number of epoch interations.\n",
    "* <b>Learning rate</b> is a hyper-parameter that controls how much we are adjusting the weights of our network with respect the loss gradient.\n",
    "* <b>Batch size</b> defines the number of samples that will be propagated through the network.\n",
    "* An <b>epoch</b> is a full iteration over our samples during model training. \n",
    "\n",
    "Feel free to adjust these parameters, especially the learning rate and number of epochs. How do they change the model's accuracy and loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "furnished-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.03\n",
    "batch_size = 32\n",
    "epochs_num = 500"
   ]
  },
  {
   "attachments": {
    "log_loss.PNG": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAABXCAYAAADf7eU1AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAEnQAABJ0Ad5mH3gAACHCSURBVHhe7Z2HuxRFtsDfn/I2mdaEgpFgAkFBJSkmQCQYSIogIkgSRIkiGRFBQDArKBkk5wUkSHYRVknu6i7IU3Hrza+mz6Wmb/dMz9w7M32H8/u++u6d6uru6qpTVaeqTlX9j1EURVEURYkRqpwoiqIoihIrVDlRFEVRFCVWqHKiKIqiKEqsUOVEURRFUZRYocqJoiiKoiixQpUTRVEURVFihSoniqIoiqLEClVOFEVRFEWJFaqcKIqiKIoSK1Q5URRFURQlVqhyoiiKoihKrFDlRFEURVGUWKHKiRIr/vvf/5rVq9fk5FatWh3oH9UpiqIo8UCVEyVWrFmz1vzvH/5ccFezVh3z+++/e7FQFEVRiokqJ0qsQEF4umPnFMXh1tvqmjVr15pt27abvXv3msOHvzUnTpww//73v82vv/5qfvvtN3sfzv2fa+fOnTM//PCD+dvWreazz+aaMWPGmTaPtzMXX/LXlHfgZs9534uFoiiKUkxUOVFix+HDh80tt96eojh07NTFKhyVAVNHP//8s1m4cJHp1Llr2TsefPCRSnuHoiiKkjuqnCixZO68eSnKCW7ChIlWsahMGFl57bVhZe/YsGGjd0WpajCi9txzPcyIkaM8H0VR8sWBAwdMiwcfNosWLfZ8KhdVTpRYghLyyiuvpignf/zTRXkzXP322yP2HYMGvZJ4t+epFIw9e/eaPXv2eL9y4+1p020eUmG6IEtMC1alUTHivHbtOu9XabMm8Z0HDhz0flVdyDNGYyu7AxVXXh40OLC8rVu/vlLKmyonSmxBuB96+NEy5QTXqNF91tYkHyxYsNDcdns9O5pS6vCNpG8cGuwzZ84k0r2u+eSTTz2f7KFBePbZ56yMLFmy1PNNwjUq0NFvjKkSCgpxnJZQtIYNH3FBNHQ7d+4yV151jbUXywaSRuS42BCHTz/9zDS/v0WVkLGKglze3fCe0PLWrNkD5o0xYyuUFnlXTujpUsiGjRhp3XDvr7ihw4bHMjOrarxLDQxZr7yqWoqC0rNnr7xU2jyzwxNPmQ8++NDzKT2Q6wdaPGQba9zy5V/mJS2jgtFytWtqmLFjx3s+uUFZvPGmmtZWKahcbty4yVx08WXm/fc/KOr3ZoK4U9fcdXejhBL+H883nFIYXSE/Xn/9DdO4SbNICgrZR90sMoxDwS0mxKduvQbmp59+8nzCKYU8++WXX8xfLro0tLwtTigs1NWzZ8/xfLKnIMqJbdgTmec2MOIXZ+WkKsa7FJk27Z2UPMDNnDnLu1q5UFHGufGqKMj1gIGD7NJp0nHR4iVF+95z53433bs/b3tZFS1LR44kp+X69u0f+j3jJ0w01apVNzt27PB84seGjRvtd8yZ857nkwrfRlp9/fXXti4ibCmM9KGk3te4qek/YGBGWSB3qYNlpAz3448/Ji8WgY2bNtk4vPnmFM8nFcmz/fsPmjenvFUSebZ37z77HZS3MHr16l2h8lawaR0yhx4bH8TfqtIAVNV4lxLkwXOJRow8EHdt9evNrl27vRBKtojSPW/e50WT6WXLlts4zJ33heeTO7NmvWuftXTZMs+nPMhRy1aP2UaN/+MGcXryqY42jkHxI8+e79nLNGx4b1k5wJXKNOT8+Qvs96xfv8HzSQ/fzagJ9xw/fsLzLSzk0xNPPm1uvKmW+ec//+n5nofOAApX48bNSirP3p09x37HkqXh5Y0OwzXX1jB9XuqbUx2jykkGqmq8S42TJ0+a+g3uLivcuMfbtg+sxJXMSK/7w48+LopMk2/tOzxpGtzVsMJ5SPxp1HlWpm+ZNGmy/e4vV6zwfOLD5s1bbNxmh4yakGcY/bIT8ooVK8vKQakoJ8jBzTXrmKef7hRJJvhuqZuPHD3q+RYWybN+/Qd4PqmgnEycOMnm2cqVq0oizyhjbMHAflGZyluP53uay6+42nzzzd89n+iocpKBqhrvUmTJ0uQ8puteH/2G5kkOiHLy7ruzi5J+Mn2Rblg4KsSf4WO+KRMsN+a9bdq0jZXcEBfJk2PHjnu+4dDoERZXKsoJdO/R037Tl19mVh6pm+9/4EEb/uDBwq/2cfMsymjP6tXnd7+uynlGul9z7XWRlhDLjt9Dh43wfKJT5ZQTnkPG8jcKvMd9F/9mc38x4k0YcengOvGJ8sxSgG8dNWp0WQEXR6/kQsQv29kg0zpvvz0t9Bny/KjvEXl0wS9I7mUZIpVXOqK8+9SpU/ZZUZYi86ymze63q0POnj3r+RYf0gebi6h1TFVQTqLknZ8VK5MjQlEMpEkzqZt37/7a8y1PUgajr0wLineQDPO7SdPmdtVKtDyLv3ISJc9OnDhp7r2vSaT0JAwj3oyyZLsaq8ooJ9z/yaefmVeGvGq15cGvDLGaW1gCkflLly4z/foNsCswGCplXpLKGKtqDAKjaucVjTdLzIgv8Sb+YfEmzkNefc2Ge+CBh+yGUtzbrv0TKeF37dplOnXqYsM82rK1ndNbsHCRGTGi9DefIh2YzpFCjqOCYMfXCwXSYNu2baZr12etfITJEg1YGKKcTJgwqZxM8/vnn89a4z3egRs/fqI5c+bnQPnH7/jx42bKlKmmbSJvhgx5zW5mR2+WqRuGdZ95plvZvcTtkUdb2dUzYRVW8pknbHkZ9frowMqc7+YbCbtmzbrAuAXBkmK+fdOmzZ5P8WFDK+JE3KIQZ+WEfGEYv1u37lY+g+KHX1CngnuRi3btOgTKtQvXpW7maAs/XKcNQNYfa9PWvNi7j3n/gw9D04vwWxPlir2OaDPenZ1sM1jWTZtx3fU3pYzOUecQ15GjXvd80hNn5YRvJ89oc6Ym2siwPDtf3tJ3KgTCUn7pDGD0nA1VQjmhAntr6tv2XipThJrdQhvc1chMTfj7P5pEJJFRQJjvI7HZvwIBGz58ZNncX5SdJCsSb+LlxpuMDYs334hhFcZ6VOrfff+9mfzmlDKDL+IBbPJDIUHZOnbsmO0x0Bj89fKrIisnfENFXTGhh3z9DTfZdBH38suDy9KolCHtyWd6ImvXrbPfPjDx7W5lIhvKXVv9utA0EeXEPy3G/1RSTZo0t/JI47J6zRrT84Veid5SU3vNDc/zv5i/wJavni+8aFauWmWNNnn2Pfc0NnPnzisnw8QVhaVmrVtCZYkyig3JBx9+ZO/nfCVXkeFZYmR48OAhzzca7733vr1v0uQ3PZ/iQx1FnIhbFOKqnJAvLJVt1bqN3YiL+CGfvzlxPHLkqPUPkk/kAbmgMfNf88N1qZv90yq//vqbmfJWsu5FsWYvDtK4QYOGpteLfcq1GTxL5JYdo+kIU8ZoM1onvmXxkiX2WovE+wQ6CPhRT0chrsoJac6qPcrbh155ox1yyxvxbZyoE4h7pnzxQ/pw31dfZbdqp2jKSdQPJIHojXEfFZ0LAkZDzXVJSJ7LUknCz3EKuhRmhA2r6okTJ0eKQ2XEmwPnXLgm8ZZCInPwhw59Y38LoxONB/68FwFheRZKl79SZ7QlqnKC4PEtubqZs971nlQ82K+CdHHd559/EdrYlQpU7CLDLCeVb3crO+QNvw4dngxND1FO6AlKGP6yUyv+GJi69yJ/KNZcc6dPduzcaarXuMHKpFs2ZJkn7yE+bhn417/+Za8xvRIE38L1VatXm3OJZ9auc5v9TRkWpIG7ulp1GyYbZOqgf/+Bnk/xIS7EKaqhrquc+BvaYsKS3jvr32UP24win/76FJlr1vwBex05SQf3Uh8R1pUNt+59c8pUzzcJ1yg/KEaSbrxzxoyZNryraFDG2G9GyhvxdvcxmT9/ob2HlWJRcJWTOOWZlEc6IeSTlDc3z6S8he1rkg7s2rh3/oKFnk80Yj9yMmNmcolg23YdAu9pk9Buuf5OQriABJU9HFyBlQrP75+JXONNA849meI9K5Fx4DYWrlBQmPAnHm5caJzdcNyPi8KqxPeTBmGOkakgf3EYFRYb0pQeGWkhjkJ16FB2veiqhHyzDC1Pm57c/8WvSLB0Ef/Bg4d4PuURQz5XOUHWZDSCVQh+ZGUCYQgLyC9+/rIh8kwvzJVTYKtyrj31dCfPJxWWGPM85J3eFmH9vWxp4Jj7dt8bBU625t4uXZ/N+t58wHcRF+JE3KJAOSQ8zp++xYR8p3MI09+ZYeOHfLp5l0k+kQuu+ztqfvhuqQ9JD0Hq3lq1b015ryCy2bvPS/Y69ijyHHeqiWviz+igC9dE9lmOH4XVMR054ZgQySNZBebPMylvL73UL+syw6AC90pbF5VYKicID3NaZGDXZ7rZe1544UXvaipSyRKOZ/Ke88rJ+XkxnoUfDo0+KtnFe2RO8eYd27d/VRa/Vq3a2DRAIDD2k3fyVxokHMPujABt2LjJvjNd3EoRvlms9cVh4+AWqrhAnMjzXBxyIDBvvmfPXpvXHTt2tt/MnLjAe0TBYHVTGDyXMPwVuflqR1IRwDFl6MctQ4yYgOx34C8b8nxOk/bL5ZYtf7PXBgx42fNJhe+lQuM+GRJmnxvJV/yjKGBh0LvnXqYe/HELgpEiyYtsndtohkEciAtxirqZWEWUE+4NimsUl6lsMW0udRFLgokftksC/iKfYas9kAuuIyfpIC5S/iWd8aMBxa9dyMihTKHJMnactBluWeNeqfv9ygnXxo6bYK8tW77c801PRZSTiuRZJkNxRty3b99uv6nbcz1s/CZPTs0zKW90irOF9OHeseOy2wU6lsoJvS0ywxUaEjkIGnGuy7AyTuYOXS1YRiDoXZ8+fdrzzUwh4838Pltw4y+O63yHvBeh7ucNAbuOd0tv9kKC9PanxbZEQYsb5I1bYWTj3ApT4Hwh5uWRZ3eo+ejRf5Slw+nT4Vt6o0gThueLbGHHJPcij36kDOFkLw4MCPlN2XArXJFv/vqRERjpYYdBHMT42d+jlQYO25RsIb24N6pysnv37nJ5EtVJo5kOvocOCXEqhHJCmgXFNYoLkosgRD6J3/79BzzfVPkMWy0l0/JRlBOpmyWd8cOYFj/iG5S/knbETxQpFk7gN2bsOC9UUt6pf5mCp5PownvGjB1v7+EIiChURDmpSJ5FWcUGxEnaLXfUim9lJRn+0inJBtKHe8eNnxCpvAmxU04QasLs37/f3sMcF7+DKjnAqJXr0sgD83mc11G3Xn1rUIeB1hNPdrR+7CAp4aJQyHjzbIwZGbp/+JGWdi6fMBQOd/dBhIjGgZET5ncvufRyG+6xNu0iCT1heGdFXFwgLuMSGjnfj/v73w97V+IH+Zur80NPju+l8nHzA0UG/5Ytg3cZFbhP7pfnU1YkHYPuxU+ui3KCLA0bNsIaZKPwoESgZBOGnnOQwizTOoyqpIMVQITzTw3J/DdyfzqHM1Uoo9wfdVrHzYdcXCYI07lzVxsn4haFiign/vhl46Ii8kk9FiSfjHiEdaaQC8JkMnTmuUEjJ9iy4Bc2ai17b4hyArQZtevcav1ZxID9hZSRKW9NDSwPM2bOstexdYtCRZSToLyI6qLClDhxI03d+MlW9biwPEsH017cy3RbNvGJnXKCIBCG3g33yCgIghJEt+e62+syrQMUgE6du9hTTp99trvVpGnIWa6XRdpYcok3YbKJN+9AWXEVGZ6BvxQQCjt+Tz3V0Xz99XlNGD8aZJZmEu7777/3roTjnwrJ1oUpXMWANKK3Q7w+/vgTz7e0Ic+Z3uGbp09/x/NNInKXaQhV5Iq/PA+k0cYFVZ6ygRlOelCEw2CQlQs8C0WC+Wrml8mbIGRfEspTOqQho+GWOMJHH31s/du3fyLFPyrSsMfJIFamIqSRzURFlJN848onHS0X8cfQPwjulfo2k0Es3y1hJd24X6aFUNCDENl3dyf+/vvjpsfzL9iGlDgykjVw4CCzdevWUDmel1BKeI4o6pmoiHJSCKS8denyjOeTREwJsJPMpbzJiKx/aiwTRVNOgjKczV0QCpYZynWWznIPhm/+e0goGQ4VYxvCoIiIYBJGXC4UKt6cdHzPvU3KCS2KB+HeeWdGWVz8BR7YaZNwUexpeEdFXFAaFAPSUSzB30ooh7nmcVWD9BcF0z1Lhryhp4r/hg3pd6xEwSScq5xwvxhqs629n3Xr1ttrrkEsUx740RuV52TKB97DPbfcekfasOwqSTi/XYk0Lhzklwui3LD6KC7INxG3KMRdORH5dBsk4il1KaeNB8G9yAUjcZlWtPA8lGGe5yp1UvfedHOtRJjydS+2KFwXg1iYOnWaadjoXvs/YcSlQ/KAJctRiLNywrdKncCeXC6Sl9g35oJ03DdnmKbzk3flhI8mIxA0t5HHD4dw8JdGusZ1yWmMO+rWLxMarkmFieGfCAx/Z3v7FbR1lhLjL6s4KPAkuDgaeA4jitK4Vka8ZaiWeP/uxJslzvizgZrEG6Mk/NgkyEXCYkVNWASFBsidA+WZ+DNFdKFsRsY3UwmxCRJ5y+8LBWRM5siRaYEpFfyQA+QvCNKJ+6UxZJRD5BmkYmeO+T//OX9kP9elHLgrShipw49Gwi1vGEYuWrzYPtufN/IsGqCweIIY2zL9IlAG8MPluomaLImOsgljoRCDePZDCkPyDucqJ9RRpGO6tCwkxJMRPeLmKpbSM2c6LiyufBtygXzwfxiSDqKcIPsia/xNqXud50h9yuovqXtB9jEJajOYag+KC7aL3PPGmLGeT3lS8+y8cuLmmb98FAvJM7e8SZ7hci1vsulhtjsy5105wZCHhhNhkI8Ux+maYoDjOgTTzTBWDoiw8RcBkooSS3BXyIAdVN3nue6qq681L/Xtn3FutzLijeCJ1uiPN7/dKRgqc4bHObPghV697WqFQYNfMbVq3WKnfETAuZ8CWb9BQzNy5Ot25KBvv/72XnaJvVDYuHGTuf6Gm+1ZHEEVR6mDzDdqdJ+VBxpZhqRFDtmZM6zCoxIXGRSHLCN/QFpikc/+IUwVMt3AniUcyIbtEyMlbnoj4x2eeDrlea7jXTNnziqXR6++OtReTzfSx7PZ2K1m4t10Amgs5Lm331HPXs8W0kVsEmh04gL5SRlGWUyXdy1aPFzWIIsj/0jn5ve3yClN8gHxYISNuFFHybQVDju5sDK7c2dyJK53n/CTbPHne8XWDnfpZVeYRvc0TqRRcgSF90vdi+0Ty2VltJr61d9m/PLLL2XP8jvajL6JzoDfsJRvoI3IlGfENTzPwm1vCo0/z2Q0Hscmi2F5lg7ShXS//Y47s74/78oJw71kEJp+ZOcMEQv8Zhc7NFkqKYxrGE3wfzAaactEYlDQGVKUZxIHDPWYTyOxEZZ0BTmneCc046B4E0/iK5o43+GPN4oLu70Sf/YhQeAJT49KwvKXERbC4NBq2eHz408+taf2+t9dqmA7xHkNHC1POlyoYCTNtI7YK8noG7IWBnKN4i0yi4yLnLtgU8IUA6MMOHaO9CsmIEa0vP/885J/GQZGyeG6f15ejOSQ+3RQRlFEJ06abL+PuHNfd2dpcTacOXPGjraheOVyf76g7GID4xpp+knmXXidRNrE6ZtYeWTlM5Fv5LPIJ8pBWF3FSAth0hmZcq9bN4sM40gjQepeZI96EkNXRgb9acRuspgC8F67I7LzTMJLx9jfZvD8gQMHR8iz8+XN77iWrh0qNNhMsvR3+PBkng0dNrxC5Q2jZu5npDfb+wtmc1IoECgSY33InDsJxFk1hNm3b5/nW3UJK+SlCg3yQw8/antf335b/M3gigFK6bhxE1IqYpRTFIErrqxWsHShx8dSX8pTGJRDyhqjL66sUg7ZIZbeY5AM00tF6eJbBe5Baed5UTe+8jPHM87LZb+GfLNx0yYbty/mZ7eTZtwgz2jYkE/JWzvSd8999vvYZC8IwmKkytEJhRxNkDZj/PgJnk8qyJ3seOzfJO98nmVn7Bk3GMGkvPnzDJvJdHmWCY5W4X6UxGwpOeWE8zJIjE2byu9wKaAFEibKyhYlPlBJsPHYtdWvz7gHQq5QMIMay7ggNh44RiuA+PI/fhxvUKiKnR4fvUa/db+LGHSzd4W/h8j5JVzbsiW1rFrFpen99hrKi7BgQXK7cBq5XL6RdGKZKs8sZOMXFVG+mKqMswymgzwmj3Ain7BgQdKWibo3bKRANufD1qiQyGoSlJQwxP7pu+++83ySlEqeSXlLzbNkeUuXZ+kgbZgOyrW8lZxywvz7pZddbucCSVBXYJhXFAMfKqlcElwpDgg689DkHYd45QNkBdsL9k2JK8dPnLBp0CNRGWIzQZxlXxGGpgvZ6FJ+nu6Y3JPi00S5Io8E4sXmW8SJ8uiuKhIIz6gLCqdbTvFPnmlS3545BbIqiOnabHZ4dpGpJIbT4wq7phLHXHqacQCZwGaBHjf5RL5K3iELYXUu4WjksZcqdL3MqCPyRiPKQgO/HG/evNkudiB+v/1WPm6lkmeULUwLouZZJsaMSW7xkE7pS0fJKSckLHPUjZs0s0OEHNXO0FLbth1sL+/mmrXt70IXACV3yFOGiRH0fA7Hy5krbBIWV6g4WYFQ784GNk06d3nGTnGxYsCtVAsF76Q8YTDL0QEokBjSUfb+/JdLbDmkPJKHQRw/ftLUuO7GlGkawm7dus0ah2O8zneyIo6dkaPudumHEafbbq9re4bFSKeo8O0vDxps2rRpF+t4hkGcmToj35kmQNnnFPZM8slpzDSQnHpdDE6ePGUbYmSOs336D3jZMwSvbQ1iMylWVT3PZs9+z9YpfDd5xmIDyl2u34NywxRzRcpbySknAgmCcQ+9JAz6MO5jhQ4CFlZRKvGDvGIPApQGjCLzlXcnTpyw9hMY2ca9gkGGqcSpPJDvYss07yYOlC8xoiVespFiJjD6vuHGmmbHjvNbY/NMjFenTZ9h5/OxNcr1G7nvue49vJ5v/KZz/JCWHNMfd0UqDOJ87Nhx83ZCiabXnEk+2SeK8j1r1uxEOM+zCIgccyQDCxdw/B+lfEmeYUBa1fOsMspbp05drEJXkfJWssqJUhrIlEWfHE7DjApH899R9077Hg4FUwoLFePixUtSjF8rk3379lvFJF/ykw9o7Nj5ePfu3EaKqhKMsOBkL6iqCg3x6NFjDAdzXsjs3LnLvPra0AoraaqcKLFl5apVdkOmx9q0rfTeCA0VmwqxRfXFl/zVKiYsMY168JqiKIqSP1Q5UWIJ2net2rfYEQ16JCgn4vgtfvQw3WuukzD8ZdUIy+TYIZLhcgzcUEhch81EVepdK4qilCqqnCixA+NFdntkRINVV1jRuy7IL8ixky+7mvqVkDCX61p+RVEUpXJR5USJFSz3btU6uc10IR0ruRhlURRFUYqPKidKrMB4cdiw5MFxletY1ug5u52236XfRl1RFEUpHKqcKIqiKIoSK1Q5URRFURQlVqhyoiiKoihKrFDlRCkIJ0+dMmvXrfN+5ZclS8uf46IoiqJUHVQ5UfLOoW++sec2uEf85xNW31Tm4W5r166zcRfHs93f7LWiKIqiVB6qnCh5hYb7qac65u0k4SA4DZcDvNjIrTJYvXqt3TelxYMPW8VH/hfHniwc964buCmKolQOqpwoeWX2nPdMy5atC95wc7ZDzxdejPxewokL4ty5361i0qjRfeXCcNgd15YsLZwCpiiKUsqocqLkDRrxVq3amEmT3/R8Csd3331nLrn0cnv0fhCnT582Q4cON+3bP2GP9mcn2Tvr32WPSQ/i4MFDVgGZMuUtz+c8bN7WuvXj5pZbbzdnz571fBVFUZRcUeVEyRtHj/7DNugHDhz0fFKRM3BcouzSmm6EQ+B63XoNzHvvfeD5nGfDxo12BGTEiFFm85Yt9p1yRk+Y/cgXX8y337Jv3z7PJxU2cUte3+/5KIqiKLmiyomSN+bOnWdHL4IUjhUrVpq77m5kG/SOnbqY1WvXmsZNmlmlIZ2BKbYd3EM4niGsXLnKHovv0ubxdna3WVeRIS6MkmRzrDn393qxj/nLRZeGKk+8i3ht3RY8UqMoiqJER5UTJW8wmnDDjTcnGvTUUY5Tp06Z6jVusFMlHPJHo96kaXPzww8/2P/DtpL/6aefzPUJxYJThus3uLssHMoD00cYqrqKSL9+A6zC4io7jJbMn7/A+xUN7E1q17kt0N4EROG56OJLdVpHURSlElDlRMkbI0aOsnYc/gYdBYRRFUDh4DeGs4Rj9cuePXvsNT+TJ0+xysWRI0dTnnH27P/ZEZohQ16zv4Xp02fYqR0Z7eD5KDD8/j3xf5jzw3Jh3tfnpX6eTypLly231xn5CVJeFEVRlOxQ5UTJG4xsJJWB8AabZbo07IcOHfJ8MvP551/Ye06cOGl/i/KwfPmX9rcg/jJyguLA7z/+6SLzhz/+JcXhj0M58iP2JJ988qnnk0rfvv3TXlcURVGyQ5UTJW/Mnj3H1KvXIO1oAg1/7Tq3Rh5xYNSjW7fuZSMgMGnSZHPFldXMjz/+aH8Lb0+bbm68qWaZckL4atfUsL95X5hz4TfvQvlgxMbPjh07zZVXXWPtZvz3KoqiKLmhyomSN778coWpcd2NZUqEgFHr423bWyXh3nubpBiyspR33fr13i9jtm3fbg4f/tb7ldzUrdo11c2LvfvY3ygE7dp1sAapfuWgd5++pnHjZinvZ1po5MjXvV+ZYZqnZq061qbF//wzZ87Yd2OLsndv8CoeRVEUJXtUOVHyxq5du+0eIq5ywP+MQjS65z4zfvxEaxMihq3vv/+BnVYRJQDbE8I2adK87Bn8RRno0uUZ+3v79q/sMxgl8fNoy9Z2lMVVKlBueOa8eZ97PuEQltU3hEehkTgcO3bMbNy4yTRr/oBVik6eTE4vKYqiKJWDKidK3jh1Krn6ZvPmLZ5PEgxlGY1A6di1e7ddqYNSgh8KgYBywiZpXBN/FI1ly5ebhx5+1Po3bXq/fcf+/an7ixCuzi23lyk+LqKgPPjgI2b1mjVmzdq1ZX9xwCGFl19xtZ3SwdVv0NCuyEHZYuSGpcUsX/aPpiiKoigVR5UTJW8w0sDIwtBhwz2fJDToXJOGnf/d335QJGTUYt269WbRosX2N0rGqFGjzSOPtC67LqAQXXTxZaEbwPEuVvv06z/AdOrc1XR9ppvp0aOn6d3nJS9EajzlrzhFURQlf6hyouSVRYuX2IPx/MpDVFAuUE5QCOR8G1lRwx4pjGYEKSAcNjho0Cs5v1dRFEUpHqqcKHkFpYJpkSCbkCigfCxdtsz+j6LxfM9eZtKkKdZmBEVlcUL58Ssgn302t0IKkaIoilJcVDlR8g5KAgaw7DuSLexp4rJ3714zfMQou8InTPnomVBgFi5c5P1SFEVRqhqqnCgFgRGUQh2KxxJmRVEUpeqiyomiKIqiKLFClRNFURRFUWKFKieKoiiKosQKVU4URVEURYkVqpwoiqIoihIrVDlRFEVRFCVWqHKiKIqiKEqsUOVEURRFUZRYocqJoiiKoiixQpUTRVEURVFihSoniqIoiqLEClVOFEVRFEWJFaqcKIqiKIoSK1Q5URRFURQlVqhyoiiKoihKrFDlRFEURVGUGGHM/wNgkglqoZrjEgAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "id": "corresponding-invite",
   "metadata": {},
   "source": [
    "### Loss function and optimizer\n",
    "\n",
    "Here, we declare a loss function for our linear regression. Why does we need one?\n",
    "\n",
    "The loss function is critical for machine & deep learning models.\n",
    "The loss function for linear regression is squared loss, but the loss function for logistic regression is Log Loss, which is defined as follows:\n",
    "\n",
    "![log_loss.PNG](attachment:log_loss.PNG)\n",
    "\n",
    "where:\n",
    "\n",
    "* (x,y) ∈ D is the data set containing many labeled examples, which are  (x,y) pairs.\n",
    "* y is the label in a labeled example. Since this is logistic regression, every value of y must either be 0 or 1.\n",
    "* y' is the predicted value (somewhere between 0 and 1), given the set of features in x.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "thirty-possible",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Setup Callback function\n",
    "# Requires: model and validation_data (X and Y values of test data)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=[0, 1])\n",
    "\n",
    "# confusion_matrix_updates = ConfMatrixCallbackPlotter(\n",
    "#                                 model = model,\n",
    "#                                 scaler = scaler,\n",
    "#                                 validation_data = (x_test, y_test),\n",
    "#                                 original_input = x_test)\n",
    "\n",
    "import libraries.extractioncallback as excb\n",
    "\n",
    "extractor = excb.CallbackDataExtractor(\n",
    "    model = model,\n",
    "    layer = 0,\n",
    "    validation_data = (x_test, y_test),\n",
    "    sample_every = 10,\n",
    "    rec_int_values = False,\n",
    "    is_bin = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-desert",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "We can start to train our model. We use the parameters that we set above. The `validation_split` parameter selects what fraction of our training data will be held as validation data, used to evaluate model metrics per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "living-movement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 1s 508ms/step - loss: 1.0547 - accuracy: 0.3196 - val_loss: 1.0380 - val_accuracy: 0.3929\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.0531 - accuracy: 0.3300 - val_loss: 1.0345 - val_accuracy: 0.3929\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.0569 - accuracy: 0.3196 - val_loss: 1.0311 - val_accuracy: 0.3929\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.0383 - accuracy: 0.3404 - val_loss: 1.0277 - val_accuracy: 0.3929\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.0448 - accuracy: 0.3091 - val_loss: 1.0244 - val_accuracy: 0.3929\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.0182 - accuracy: 0.3300 - val_loss: 1.0212 - val_accuracy: 0.3929\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1.0296 - accuracy: 0.3300 - val_loss: 1.0180 - val_accuracy: 0.3929\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 1.0078 - accuracy: 0.3508 - val_loss: 1.0149 - val_accuracy: 0.4286\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.9936 - accuracy: 0.3824 - val_loss: 1.0118 - val_accuracy: 0.4643\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.9995 - accuracy: 0.4257 - val_loss: 1.0088 - val_accuracy: 0.4643\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.9857 - accuracy: 0.4573 - val_loss: 1.0058 - val_accuracy: 0.5000\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.0005 - accuracy: 0.4788 - val_loss: 1.0029 - val_accuracy: 0.5357\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.9924 - accuracy: 0.5215 - val_loss: 1.0000 - val_accuracy: 0.5357\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.9804 - accuracy: 0.5743 - val_loss: 0.9972 - val_accuracy: 0.5357\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.9750 - accuracy: 0.6277 - val_loss: 0.9944 - val_accuracy: 0.5714\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.9730 - accuracy: 0.6489 - val_loss: 0.9916 - val_accuracy: 0.6071\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.9699 - accuracy: 0.6489 - val_loss: 0.9889 - val_accuracy: 0.6429\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.9730 - accuracy: 0.7120 - val_loss: 0.9861 - val_accuracy: 0.6429\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.9738 - accuracy: 0.6808 - val_loss: 0.9835 - val_accuracy: 0.6429\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.9672 - accuracy: 0.6704 - val_loss: 0.9808 - val_accuracy: 0.6429\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.9586 - accuracy: 0.7019 - val_loss: 0.9782 - val_accuracy: 0.6429\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.9895 - accuracy: 0.6915 - val_loss: 0.9756 - val_accuracy: 0.6429\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.9709 - accuracy: 0.6915 - val_loss: 0.9731 - val_accuracy: 0.6429\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.9669 - accuracy: 0.6915 - val_loss: 0.9705 - val_accuracy: 0.6429\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.9486 - accuracy: 0.6915 - val_loss: 0.9680 - val_accuracy: 0.6429\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.9574 - accuracy: 0.7019 - val_loss: 0.9655 - val_accuracy: 0.6429\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.9580 - accuracy: 0.6915 - val_loss: 0.9630 - val_accuracy: 0.6429\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.9371 - accuracy: 0.7019 - val_loss: 0.9606 - val_accuracy: 0.6429\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.9175 - accuracy: 0.7332 - val_loss: 0.9582 - val_accuracy: 0.6429\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.9537 - accuracy: 0.6499 - val_loss: 0.9558 - val_accuracy: 0.6429\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.9264 - accuracy: 0.7124 - val_loss: 0.9534 - val_accuracy: 0.6429\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.9299 - accuracy: 0.6915 - val_loss: 0.9511 - val_accuracy: 0.6429\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.9450 - accuracy: 0.6811 - val_loss: 0.9487 - val_accuracy: 0.6429\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.9177 - accuracy: 0.7332 - val_loss: 0.9464 - val_accuracy: 0.6429\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.9181 - accuracy: 0.7124 - val_loss: 0.9441 - val_accuracy: 0.6429\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.9107 - accuracy: 0.7124 - val_loss: 0.9418 - val_accuracy: 0.6429\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.9166 - accuracy: 0.7019 - val_loss: 0.9395 - val_accuracy: 0.6429\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.9245 - accuracy: 0.6915 - val_loss: 0.9373 - val_accuracy: 0.6429\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.9224 - accuracy: 0.6811 - val_loss: 0.9350 - val_accuracy: 0.6429\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.8867 - accuracy: 0.7540 - val_loss: 0.9328 - val_accuracy: 0.6429\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.9165 - accuracy: 0.6811 - val_loss: 0.9306 - val_accuracy: 0.6429\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.9136 - accuracy: 0.6919 - val_loss: 0.9284 - val_accuracy: 0.6429\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.8979 - accuracy: 0.7019 - val_loss: 0.9263 - val_accuracy: 0.6429\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.8886 - accuracy: 0.7228 - val_loss: 0.9241 - val_accuracy: 0.6429\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.8944 - accuracy: 0.7019 - val_loss: 0.9220 - val_accuracy: 0.6429\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.8726 - accuracy: 0.7332 - val_loss: 0.9199 - val_accuracy: 0.6429\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.9035 - accuracy: 0.6915 - val_loss: 0.9178 - val_accuracy: 0.6429\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.8833 - accuracy: 0.7124 - val_loss: 0.9157 - val_accuracy: 0.6429\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.8928 - accuracy: 0.6915 - val_loss: 0.9136 - val_accuracy: 0.6429\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.8780 - accuracy: 0.7019 - val_loss: 0.9115 - val_accuracy: 0.6429\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.8777 - accuracy: 0.7124 - val_loss: 0.9095 - val_accuracy: 0.6429\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.8896 - accuracy: 0.6707 - val_loss: 0.9074 - val_accuracy: 0.6429\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.8786 - accuracy: 0.6915 - val_loss: 0.9054 - val_accuracy: 0.6429\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.8827 - accuracy: 0.6707 - val_loss: 0.9034 - val_accuracy: 0.6429\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.8687 - accuracy: 0.6915 - val_loss: 0.9014 - val_accuracy: 0.6429\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.8805 - accuracy: 0.6811 - val_loss: 0.8994 - val_accuracy: 0.6429\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.8702 - accuracy: 0.6915 - val_loss: 0.8975 - val_accuracy: 0.6429\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.8765 - accuracy: 0.6603 - val_loss: 0.8955 - val_accuracy: 0.6429\n",
      "Epoch 59/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 78ms/step - loss: 0.8600 - accuracy: 0.7019 - val_loss: 0.8936 - val_accuracy: 0.6429\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.8687 - accuracy: 0.6915 - val_loss: 0.8917 - val_accuracy: 0.6429\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.8606 - accuracy: 0.6915 - val_loss: 0.8897 - val_accuracy: 0.6429\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.8430 - accuracy: 0.7332 - val_loss: 0.8878 - val_accuracy: 0.6429\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.8504 - accuracy: 0.7124 - val_loss: 0.8859 - val_accuracy: 0.6429\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.8662 - accuracy: 0.6707 - val_loss: 0.8840 - val_accuracy: 0.6429\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.8501 - accuracy: 0.6811 - val_loss: 0.8822 - val_accuracy: 0.6429\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.8441 - accuracy: 0.6915 - val_loss: 0.8803 - val_accuracy: 0.6429\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.8476 - accuracy: 0.7127 - val_loss: 0.8785 - val_accuracy: 0.6429\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.8589 - accuracy: 0.6815 - val_loss: 0.8767 - val_accuracy: 0.6429\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.8510 - accuracy: 0.6919 - val_loss: 0.8748 - val_accuracy: 0.6429\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.8435 - accuracy: 0.7231 - val_loss: 0.8730 - val_accuracy: 0.6429\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.8406 - accuracy: 0.7023 - val_loss: 0.8712 - val_accuracy: 0.6429\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.8330 - accuracy: 0.7231 - val_loss: 0.8695 - val_accuracy: 0.6429\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.8305 - accuracy: 0.7231 - val_loss: 0.8677 - val_accuracy: 0.6429\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.8416 - accuracy: 0.6919 - val_loss: 0.8659 - val_accuracy: 0.6429\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.8324 - accuracy: 0.7023 - val_loss: 0.8641 - val_accuracy: 0.6429\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.8221 - accuracy: 0.7231 - val_loss: 0.8624 - val_accuracy: 0.6429\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.8253 - accuracy: 0.7231 - val_loss: 0.8607 - val_accuracy: 0.6429\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.8114 - accuracy: 0.7335 - val_loss: 0.8589 - val_accuracy: 0.6429\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.8346 - accuracy: 0.6919 - val_loss: 0.8572 - val_accuracy: 0.6429\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.8183 - accuracy: 0.7231 - val_loss: 0.8555 - val_accuracy: 0.6429\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.8225 - accuracy: 0.7023 - val_loss: 0.8538 - val_accuracy: 0.6429\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.8199 - accuracy: 0.7023 - val_loss: 0.8522 - val_accuracy: 0.6429\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.8184 - accuracy: 0.7023 - val_loss: 0.8505 - val_accuracy: 0.6429\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.8167 - accuracy: 0.7023 - val_loss: 0.8488 - val_accuracy: 0.6429\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.7979 - accuracy: 0.7231 - val_loss: 0.8472 - val_accuracy: 0.6429\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.8174 - accuracy: 0.6919 - val_loss: 0.8455 - val_accuracy: 0.6429\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.8199 - accuracy: 0.6815 - val_loss: 0.8439 - val_accuracy: 0.6429\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.8084 - accuracy: 0.7127 - val_loss: 0.8423 - val_accuracy: 0.6429\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.8076 - accuracy: 0.7023 - val_loss: 0.8407 - val_accuracy: 0.6429\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.7979 - accuracy: 0.7231 - val_loss: 0.8391 - val_accuracy: 0.6429\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.7862 - accuracy: 0.7127 - val_loss: 0.8375 - val_accuracy: 0.6429\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.7788 - accuracy: 0.7440 - val_loss: 0.8359 - val_accuracy: 0.6429\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.8064 - accuracy: 0.6919 - val_loss: 0.8343 - val_accuracy: 0.6429\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.8050 - accuracy: 0.6815 - val_loss: 0.8328 - val_accuracy: 0.6429\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.7937 - accuracy: 0.7023 - val_loss: 0.8312 - val_accuracy: 0.6429\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.8042 - accuracy: 0.6815 - val_loss: 0.8297 - val_accuracy: 0.6429\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.8023 - accuracy: 0.6919 - val_loss: 0.8281 - val_accuracy: 0.6429\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.7842 - accuracy: 0.7231 - val_loss: 0.8266 - val_accuracy: 0.6429\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.7763 - accuracy: 0.7231 - val_loss: 0.8251 - val_accuracy: 0.6429\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.7804 - accuracy: 0.7127 - val_loss: 0.8236 - val_accuracy: 0.6429\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.7576 - accuracy: 0.7440 - val_loss: 0.8221 - val_accuracy: 0.6429\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.7963 - accuracy: 0.6815 - val_loss: 0.8206 - val_accuracy: 0.6429\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.7793 - accuracy: 0.7023 - val_loss: 0.8192 - val_accuracy: 0.6429\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.7749 - accuracy: 0.7231 - val_loss: 0.8177 - val_accuracy: 0.6429\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.7569 - accuracy: 0.7440 - val_loss: 0.8162 - val_accuracy: 0.6429\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.7862 - accuracy: 0.7023 - val_loss: 0.8148 - val_accuracy: 0.6429\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.7655 - accuracy: 0.7023 - val_loss: 0.8133 - val_accuracy: 0.6429\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.7763 - accuracy: 0.6919 - val_loss: 0.8119 - val_accuracy: 0.6429\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.7830 - accuracy: 0.7023 - val_loss: 0.8104 - val_accuracy: 0.6429\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.7534 - accuracy: 0.7231 - val_loss: 0.8090 - val_accuracy: 0.6429\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.7834 - accuracy: 0.6815 - val_loss: 0.8076 - val_accuracy: 0.6429\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.7547 - accuracy: 0.7127 - val_loss: 0.8062 - val_accuracy: 0.6429\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.7650 - accuracy: 0.7023 - val_loss: 0.8048 - val_accuracy: 0.6429\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.7744 - accuracy: 0.6815 - val_loss: 0.8034 - val_accuracy: 0.6429\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.7647 - accuracy: 0.7023 - val_loss: 0.8020 - val_accuracy: 0.6429\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.7592 - accuracy: 0.7127 - val_loss: 0.8007 - val_accuracy: 0.6429\n",
      "Epoch 117/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 72ms/step - loss: 0.7539 - accuracy: 0.7127 - val_loss: 0.7993 - val_accuracy: 0.6429\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.7593 - accuracy: 0.7023 - val_loss: 0.7979 - val_accuracy: 0.6429\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.7484 - accuracy: 0.7231 - val_loss: 0.7966 - val_accuracy: 0.6429\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.7490 - accuracy: 0.7231 - val_loss: 0.7952 - val_accuracy: 0.6429\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.7664 - accuracy: 0.6815 - val_loss: 0.7939 - val_accuracy: 0.6429\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.7699 - accuracy: 0.6815 - val_loss: 0.7926 - val_accuracy: 0.6429\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.7515 - accuracy: 0.7023 - val_loss: 0.7913 - val_accuracy: 0.6429\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.7468 - accuracy: 0.7023 - val_loss: 0.7900 - val_accuracy: 0.6429\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.7325 - accuracy: 0.7440 - val_loss: 0.7887 - val_accuracy: 0.6429\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.7362 - accuracy: 0.7127 - val_loss: 0.7874 - val_accuracy: 0.6429\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.7499 - accuracy: 0.6919 - val_loss: 0.7861 - val_accuracy: 0.6429\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.7359 - accuracy: 0.7231 - val_loss: 0.7848 - val_accuracy: 0.6429\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.7491 - accuracy: 0.7026 - val_loss: 0.7835 - val_accuracy: 0.6429\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.7656 - accuracy: 0.6710 - val_loss: 0.7823 - val_accuracy: 0.6429\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.7315 - accuracy: 0.7231 - val_loss: 0.7810 - val_accuracy: 0.6429\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.7381 - accuracy: 0.7023 - val_loss: 0.7798 - val_accuracy: 0.6429\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.7200 - accuracy: 0.7547 - val_loss: 0.7785 - val_accuracy: 0.6429\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.7287 - accuracy: 0.7127 - val_loss: 0.7773 - val_accuracy: 0.6429\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.7324 - accuracy: 0.7339 - val_loss: 0.7761 - val_accuracy: 0.6429\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.7336 - accuracy: 0.7235 - val_loss: 0.7748 - val_accuracy: 0.6429\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.7107 - accuracy: 0.7547 - val_loss: 0.7736 - val_accuracy: 0.6429\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.7360 - accuracy: 0.7130 - val_loss: 0.7724 - val_accuracy: 0.6429\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.7147 - accuracy: 0.7339 - val_loss: 0.7712 - val_accuracy: 0.6429\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.7277 - accuracy: 0.7235 - val_loss: 0.7700 - val_accuracy: 0.6429\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.7313 - accuracy: 0.7130 - val_loss: 0.7688 - val_accuracy: 0.6429\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.7198 - accuracy: 0.7235 - val_loss: 0.7676 - val_accuracy: 0.6429\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.7276 - accuracy: 0.7235 - val_loss: 0.7665 - val_accuracy: 0.6429\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.7220 - accuracy: 0.7339 - val_loss: 0.7653 - val_accuracy: 0.6429\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.7246 - accuracy: 0.7026 - val_loss: 0.7641 - val_accuracy: 0.6429\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.7231 - accuracy: 0.7342 - val_loss: 0.7630 - val_accuracy: 0.6429\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.7038 - accuracy: 0.7550 - val_loss: 0.7618 - val_accuracy: 0.6429\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.7035 - accuracy: 0.7759 - val_loss: 0.7607 - val_accuracy: 0.6429\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.7257 - accuracy: 0.7241 - val_loss: 0.7595 - val_accuracy: 0.6429\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.7339 - accuracy: 0.7134 - val_loss: 0.7584 - val_accuracy: 0.6429\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.7015 - accuracy: 0.7443 - val_loss: 0.7572 - val_accuracy: 0.6429\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.6978 - accuracy: 0.7342 - val_loss: 0.7561 - val_accuracy: 0.6429\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.7143 - accuracy: 0.7550 - val_loss: 0.7550 - val_accuracy: 0.6429\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.7278 - accuracy: 0.7030 - val_loss: 0.7539 - val_accuracy: 0.6429\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6880 - accuracy: 0.7759 - val_loss: 0.7528 - val_accuracy: 0.6429\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.6900 - accuracy: 0.7655 - val_loss: 0.7517 - val_accuracy: 0.6429\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.6951 - accuracy: 0.7554 - val_loss: 0.7506 - val_accuracy: 0.6429\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.7076 - accuracy: 0.7342 - val_loss: 0.7495 - val_accuracy: 0.6429\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.7167 - accuracy: 0.7450 - val_loss: 0.7484 - val_accuracy: 0.6429\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6673 - accuracy: 0.8075 - val_loss: 0.7474 - val_accuracy: 0.6429\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.7089 - accuracy: 0.7450 - val_loss: 0.7463 - val_accuracy: 0.6429\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.6870 - accuracy: 0.7554 - val_loss: 0.7452 - val_accuracy: 0.6786\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.7130 - accuracy: 0.7345 - val_loss: 0.7442 - val_accuracy: 0.6786\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.6896 - accuracy: 0.7554 - val_loss: 0.7431 - val_accuracy: 0.6786\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.6902 - accuracy: 0.7450 - val_loss: 0.7421 - val_accuracy: 0.6786\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.7060 - accuracy: 0.7345 - val_loss: 0.7410 - val_accuracy: 0.6786\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.6852 - accuracy: 0.7658 - val_loss: 0.7400 - val_accuracy: 0.6786\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.6881 - accuracy: 0.7450 - val_loss: 0.7390 - val_accuracy: 0.6786\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6796 - accuracy: 0.7658 - val_loss: 0.7379 - val_accuracy: 0.6786\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.6830 - accuracy: 0.7762 - val_loss: 0.7369 - val_accuracy: 0.6786\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6748 - accuracy: 0.7554 - val_loss: 0.7359 - val_accuracy: 0.7143\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6796 - accuracy: 0.7554 - val_loss: 0.7349 - val_accuracy: 0.7143\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.6681 - accuracy: 0.7762 - val_loss: 0.7339 - val_accuracy: 0.7143\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.6747 - accuracy: 0.7870 - val_loss: 0.7329 - val_accuracy: 0.7143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6935 - accuracy: 0.7557 - val_loss: 0.7319 - val_accuracy: 0.7143\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6806 - accuracy: 0.7870 - val_loss: 0.7309 - val_accuracy: 0.7143\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6922 - accuracy: 0.7557 - val_loss: 0.7299 - val_accuracy: 0.7143\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.6984 - accuracy: 0.7557 - val_loss: 0.7289 - val_accuracy: 0.7143\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6762 - accuracy: 0.7557 - val_loss: 0.7280 - val_accuracy: 0.7143\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6788 - accuracy: 0.7765 - val_loss: 0.7270 - val_accuracy: 0.7143\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6655 - accuracy: 0.7765 - val_loss: 0.7260 - val_accuracy: 0.7143\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6730 - accuracy: 0.7661 - val_loss: 0.7251 - val_accuracy: 0.7143\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6788 - accuracy: 0.7661 - val_loss: 0.7241 - val_accuracy: 0.7143\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.6699 - accuracy: 0.7974 - val_loss: 0.7232 - val_accuracy: 0.7143\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6786 - accuracy: 0.7557 - val_loss: 0.7222 - val_accuracy: 0.7143\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6578 - accuracy: 0.7974 - val_loss: 0.7213 - val_accuracy: 0.7143\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6630 - accuracy: 0.7661 - val_loss: 0.7203 - val_accuracy: 0.7143\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6716 - accuracy: 0.7765 - val_loss: 0.7194 - val_accuracy: 0.7500\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6483 - accuracy: 0.8078 - val_loss: 0.7185 - val_accuracy: 0.7500\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6705 - accuracy: 0.7661 - val_loss: 0.7176 - val_accuracy: 0.7500\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6696 - accuracy: 0.7661 - val_loss: 0.7166 - val_accuracy: 0.7500\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6671 - accuracy: 0.7661 - val_loss: 0.7157 - val_accuracy: 0.7500\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.6918 - accuracy: 0.7453 - val_loss: 0.7148 - val_accuracy: 0.7500\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.6739 - accuracy: 0.7661 - val_loss: 0.7139 - val_accuracy: 0.7500\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6563 - accuracy: 0.7765 - val_loss: 0.7130 - val_accuracy: 0.7500\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6463 - accuracy: 0.7870 - val_loss: 0.7121 - val_accuracy: 0.7500\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6546 - accuracy: 0.7661 - val_loss: 0.7112 - val_accuracy: 0.7500\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6707 - accuracy: 0.7453 - val_loss: 0.7103 - val_accuracy: 0.7500\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6520 - accuracy: 0.7870 - val_loss: 0.7095 - val_accuracy: 0.7500\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.6713 - accuracy: 0.7557 - val_loss: 0.7086 - val_accuracy: 0.7500\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6427 - accuracy: 0.7870 - val_loss: 0.7077 - val_accuracy: 0.7500\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6706 - accuracy: 0.7557 - val_loss: 0.7068 - val_accuracy: 0.7500\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6478 - accuracy: 0.7870 - val_loss: 0.7060 - val_accuracy: 0.7500\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6252 - accuracy: 0.7974 - val_loss: 0.7051 - val_accuracy: 0.7500\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6454 - accuracy: 0.7765 - val_loss: 0.7042 - val_accuracy: 0.7500\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6616 - accuracy: 0.7661 - val_loss: 0.7034 - val_accuracy: 0.7500\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6345 - accuracy: 0.7974 - val_loss: 0.7025 - val_accuracy: 0.7500\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.6236 - accuracy: 0.7974 - val_loss: 0.7017 - val_accuracy: 0.7500\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6458 - accuracy: 0.7870 - val_loss: 0.7008 - val_accuracy: 0.7500\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.6396 - accuracy: 0.7765 - val_loss: 0.7000 - val_accuracy: 0.7500\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.6448 - accuracy: 0.7870 - val_loss: 0.6992 - val_accuracy: 0.7500\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.6519 - accuracy: 0.7557 - val_loss: 0.6983 - val_accuracy: 0.7500\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6422 - accuracy: 0.7765 - val_loss: 0.6975 - val_accuracy: 0.7500\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.6468 - accuracy: 0.7765 - val_loss: 0.6967 - val_accuracy: 0.7500\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.6434 - accuracy: 0.7870 - val_loss: 0.6959 - val_accuracy: 0.7500\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6565 - accuracy: 0.7557 - val_loss: 0.6951 - val_accuracy: 0.7500\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6558 - accuracy: 0.7557 - val_loss: 0.6942 - val_accuracy: 0.7500\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6549 - accuracy: 0.7661 - val_loss: 0.6934 - val_accuracy: 0.7500\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6361 - accuracy: 0.7661 - val_loss: 0.6926 - val_accuracy: 0.7500\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6372 - accuracy: 0.7765 - val_loss: 0.6918 - val_accuracy: 0.7500\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6343 - accuracy: 0.7870 - val_loss: 0.6910 - val_accuracy: 0.7500\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.6201 - accuracy: 0.8078 - val_loss: 0.6903 - val_accuracy: 0.7500\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.6351 - accuracy: 0.7765 - val_loss: 0.6895 - val_accuracy: 0.7500\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.6161 - accuracy: 0.7974 - val_loss: 0.6887 - val_accuracy: 0.7500\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6452 - accuracy: 0.7769 - val_loss: 0.6879 - val_accuracy: 0.7500\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6319 - accuracy: 0.7870 - val_loss: 0.6871 - val_accuracy: 0.7500\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6265 - accuracy: 0.7870 - val_loss: 0.6863 - val_accuracy: 0.7500\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6240 - accuracy: 0.7765 - val_loss: 0.6856 - val_accuracy: 0.7500\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6602 - accuracy: 0.7668 - val_loss: 0.6848 - val_accuracy: 0.7500\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6383 - accuracy: 0.7977 - val_loss: 0.6840 - val_accuracy: 0.7500\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.6190 - accuracy: 0.8081 - val_loss: 0.6833 - val_accuracy: 0.7500\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6405 - accuracy: 0.7772 - val_loss: 0.6825 - val_accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.6331 - accuracy: 0.7981 - val_loss: 0.6818 - val_accuracy: 0.7500\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.6431 - accuracy: 0.7876 - val_loss: 0.6810 - val_accuracy: 0.7500\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6373 - accuracy: 0.7873 - val_loss: 0.6803 - val_accuracy: 0.7500\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6201 - accuracy: 0.8293 - val_loss: 0.6795 - val_accuracy: 0.7500\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6254 - accuracy: 0.8189 - val_loss: 0.6788 - val_accuracy: 0.7500\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.6148 - accuracy: 0.8189 - val_loss: 0.6780 - val_accuracy: 0.7500\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6077 - accuracy: 0.8397 - val_loss: 0.6773 - val_accuracy: 0.7500\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.6574 - accuracy: 0.8088 - val_loss: 0.6766 - val_accuracy: 0.7500\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6299 - accuracy: 0.7984 - val_loss: 0.6759 - val_accuracy: 0.7500\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6057 - accuracy: 0.8401 - val_loss: 0.6751 - val_accuracy: 0.7500\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.6227 - accuracy: 0.8192 - val_loss: 0.6744 - val_accuracy: 0.7500\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.6213 - accuracy: 0.8296 - val_loss: 0.6737 - val_accuracy: 0.7500\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.6167 - accuracy: 0.8296 - val_loss: 0.6730 - val_accuracy: 0.7500\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.6214 - accuracy: 0.8296 - val_loss: 0.6722 - val_accuracy: 0.7500\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6210 - accuracy: 0.8192 - val_loss: 0.6715 - val_accuracy: 0.7500\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6055 - accuracy: 0.8401 - val_loss: 0.6708 - val_accuracy: 0.7500\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6277 - accuracy: 0.8296 - val_loss: 0.6701 - val_accuracy: 0.7500\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6400 - accuracy: 0.7880 - val_loss: 0.6694 - val_accuracy: 0.7500\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.6232 - accuracy: 0.8088 - val_loss: 0.6687 - val_accuracy: 0.7500\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5959 - accuracy: 0.8296 - val_loss: 0.6680 - val_accuracy: 0.7500\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6356 - accuracy: 0.8088 - val_loss: 0.6673 - val_accuracy: 0.7500\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5926 - accuracy: 0.8505 - val_loss: 0.6666 - val_accuracy: 0.7500\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.6325 - accuracy: 0.7984 - val_loss: 0.6659 - val_accuracy: 0.7500\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6063 - accuracy: 0.8296 - val_loss: 0.6652 - val_accuracy: 0.7500\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.6238 - accuracy: 0.8088 - val_loss: 0.6645 - val_accuracy: 0.7500\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6213 - accuracy: 0.8088 - val_loss: 0.6638 - val_accuracy: 0.7500\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5914 - accuracy: 0.8401 - val_loss: 0.6632 - val_accuracy: 0.7500\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6123 - accuracy: 0.8296 - val_loss: 0.6625 - val_accuracy: 0.7857\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6110 - accuracy: 0.8088 - val_loss: 0.6618 - val_accuracy: 0.7857\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6093 - accuracy: 0.8296 - val_loss: 0.6612 - val_accuracy: 0.7857\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.6023 - accuracy: 0.8401 - val_loss: 0.6605 - val_accuracy: 0.7857\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5989 - accuracy: 0.8401 - val_loss: 0.6598 - val_accuracy: 0.7857\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5973 - accuracy: 0.8401 - val_loss: 0.6592 - val_accuracy: 0.7857\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6068 - accuracy: 0.8088 - val_loss: 0.6585 - val_accuracy: 0.7857\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5831 - accuracy: 0.8401 - val_loss: 0.6579 - val_accuracy: 0.7857\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6026 - accuracy: 0.8401 - val_loss: 0.6572 - val_accuracy: 0.7857\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6115 - accuracy: 0.8192 - val_loss: 0.6565 - val_accuracy: 0.7857\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.6140 - accuracy: 0.8192 - val_loss: 0.6559 - val_accuracy: 0.7857\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5960 - accuracy: 0.8401 - val_loss: 0.6552 - val_accuracy: 0.7857\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6079 - accuracy: 0.8401 - val_loss: 0.6546 - val_accuracy: 0.7857\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5941 - accuracy: 0.8296 - val_loss: 0.6540 - val_accuracy: 0.7857\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6077 - accuracy: 0.8296 - val_loss: 0.6533 - val_accuracy: 0.7857\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6017 - accuracy: 0.8296 - val_loss: 0.6527 - val_accuracy: 0.7857\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5860 - accuracy: 0.8401 - val_loss: 0.6521 - val_accuracy: 0.7857\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5930 - accuracy: 0.8296 - val_loss: 0.6514 - val_accuracy: 0.7857\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6235 - accuracy: 0.7880 - val_loss: 0.6508 - val_accuracy: 0.7857\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6009 - accuracy: 0.7984 - val_loss: 0.6502 - val_accuracy: 0.7857\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5970 - accuracy: 0.8296 - val_loss: 0.6495 - val_accuracy: 0.7857\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5787 - accuracy: 0.8401 - val_loss: 0.6489 - val_accuracy: 0.7857\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5794 - accuracy: 0.8296 - val_loss: 0.6483 - val_accuracy: 0.7857\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.6023 - accuracy: 0.8088 - val_loss: 0.6477 - val_accuracy: 0.7857\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5851 - accuracy: 0.8401 - val_loss: 0.6471 - val_accuracy: 0.7857\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5668 - accuracy: 0.8505 - val_loss: 0.6465 - val_accuracy: 0.7857\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5928 - accuracy: 0.8296 - val_loss: 0.6459 - val_accuracy: 0.7857\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5882 - accuracy: 0.8192 - val_loss: 0.6453 - val_accuracy: 0.7857\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5850 - accuracy: 0.8192 - val_loss: 0.6447 - val_accuracy: 0.7857\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5956 - accuracy: 0.8192 - val_loss: 0.6441 - val_accuracy: 0.7857\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5938 - accuracy: 0.8192 - val_loss: 0.6434 - val_accuracy: 0.7857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5999 - accuracy: 0.8192 - val_loss: 0.6428 - val_accuracy: 0.7857\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5883 - accuracy: 0.8192 - val_loss: 0.6422 - val_accuracy: 0.7857\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5895 - accuracy: 0.8192 - val_loss: 0.6417 - val_accuracy: 0.7857\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5925 - accuracy: 0.8088 - val_loss: 0.6411 - val_accuracy: 0.8214\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5735 - accuracy: 0.8296 - val_loss: 0.6405 - val_accuracy: 0.8214\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5917 - accuracy: 0.8296 - val_loss: 0.6399 - val_accuracy: 0.8214\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5725 - accuracy: 0.8296 - val_loss: 0.6393 - val_accuracy: 0.8214\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5823 - accuracy: 0.8192 - val_loss: 0.6387 - val_accuracy: 0.8214\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5725 - accuracy: 0.8401 - val_loss: 0.6381 - val_accuracy: 0.8214\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5739 - accuracy: 0.8192 - val_loss: 0.6376 - val_accuracy: 0.8214\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5893 - accuracy: 0.8088 - val_loss: 0.6370 - val_accuracy: 0.8214\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.5886 - accuracy: 0.8296 - val_loss: 0.6364 - val_accuracy: 0.8214\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.5820 - accuracy: 0.8088 - val_loss: 0.6358 - val_accuracy: 0.8214\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5894 - accuracy: 0.8192 - val_loss: 0.6353 - val_accuracy: 0.8214\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5825 - accuracy: 0.8296 - val_loss: 0.6347 - val_accuracy: 0.8214\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.5878 - accuracy: 0.7984 - val_loss: 0.6341 - val_accuracy: 0.8214\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5697 - accuracy: 0.8296 - val_loss: 0.6335 - val_accuracy: 0.8214\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5864 - accuracy: 0.8296 - val_loss: 0.6330 - val_accuracy: 0.8214\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5874 - accuracy: 0.7984 - val_loss: 0.6324 - val_accuracy: 0.8214\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5659 - accuracy: 0.8192 - val_loss: 0.6318 - val_accuracy: 0.8214\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5524 - accuracy: 0.8401 - val_loss: 0.6313 - val_accuracy: 0.8214\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.5740 - accuracy: 0.8088 - val_loss: 0.6307 - val_accuracy: 0.8214\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5661 - accuracy: 0.8401 - val_loss: 0.6302 - val_accuracy: 0.8214\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5786 - accuracy: 0.8296 - val_loss: 0.6296 - val_accuracy: 0.8214\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5583 - accuracy: 0.8505 - val_loss: 0.6291 - val_accuracy: 0.8214\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5628 - accuracy: 0.8505 - val_loss: 0.6285 - val_accuracy: 0.8214\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5756 - accuracy: 0.8088 - val_loss: 0.6280 - val_accuracy: 0.8214\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5563 - accuracy: 0.8296 - val_loss: 0.6274 - val_accuracy: 0.8214\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5715 - accuracy: 0.8192 - val_loss: 0.6269 - val_accuracy: 0.8214\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5772 - accuracy: 0.8192 - val_loss: 0.6264 - val_accuracy: 0.8214\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5573 - accuracy: 0.8505 - val_loss: 0.6258 - val_accuracy: 0.8214\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5816 - accuracy: 0.7984 - val_loss: 0.6253 - val_accuracy: 0.8214\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5688 - accuracy: 0.8296 - val_loss: 0.6248 - val_accuracy: 0.8214\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5663 - accuracy: 0.8505 - val_loss: 0.6242 - val_accuracy: 0.8214\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5705 - accuracy: 0.8088 - val_loss: 0.6237 - val_accuracy: 0.8214\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5848 - accuracy: 0.8401 - val_loss: 0.6232 - val_accuracy: 0.8214\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5832 - accuracy: 0.8088 - val_loss: 0.6226 - val_accuracy: 0.8214\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5673 - accuracy: 0.8401 - val_loss: 0.6221 - val_accuracy: 0.8214\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5727 - accuracy: 0.7880 - val_loss: 0.6216 - val_accuracy: 0.8214\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5846 - accuracy: 0.8192 - val_loss: 0.6211 - val_accuracy: 0.8214\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5697 - accuracy: 0.7880 - val_loss: 0.6205 - val_accuracy: 0.8214\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5849 - accuracy: 0.7984 - val_loss: 0.6200 - val_accuracy: 0.8214\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5766 - accuracy: 0.8192 - val_loss: 0.6195 - val_accuracy: 0.8214\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5643 - accuracy: 0.8088 - val_loss: 0.6190 - val_accuracy: 0.8214\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5828 - accuracy: 0.8088 - val_loss: 0.6185 - val_accuracy: 0.8214\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5530 - accuracy: 0.8296 - val_loss: 0.6180 - val_accuracy: 0.8214\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5787 - accuracy: 0.8296 - val_loss: 0.6175 - val_accuracy: 0.8214\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5682 - accuracy: 0.8192 - val_loss: 0.6170 - val_accuracy: 0.8214\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5581 - accuracy: 0.8192 - val_loss: 0.6165 - val_accuracy: 0.8214\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5564 - accuracy: 0.8088 - val_loss: 0.6160 - val_accuracy: 0.8214\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5487 - accuracy: 0.8296 - val_loss: 0.6155 - val_accuracy: 0.8214\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5450 - accuracy: 0.8296 - val_loss: 0.6150 - val_accuracy: 0.8214\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5537 - accuracy: 0.8296 - val_loss: 0.6145 - val_accuracy: 0.8214\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5619 - accuracy: 0.8088 - val_loss: 0.6140 - val_accuracy: 0.8214\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5563 - accuracy: 0.8401 - val_loss: 0.6135 - val_accuracy: 0.8214\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5628 - accuracy: 0.8192 - val_loss: 0.6130 - val_accuracy: 0.8214\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5829 - accuracy: 0.7880 - val_loss: 0.6125 - val_accuracy: 0.8214\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5643 - accuracy: 0.8192 - val_loss: 0.6120 - val_accuracy: 0.8214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5669 - accuracy: 0.8088 - val_loss: 0.6115 - val_accuracy: 0.8214\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5611 - accuracy: 0.8088 - val_loss: 0.6110 - val_accuracy: 0.8214\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5499 - accuracy: 0.8296 - val_loss: 0.6105 - val_accuracy: 0.8214\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5652 - accuracy: 0.8088 - val_loss: 0.6100 - val_accuracy: 0.8214\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5498 - accuracy: 0.8192 - val_loss: 0.6096 - val_accuracy: 0.8214\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5539 - accuracy: 0.8401 - val_loss: 0.6091 - val_accuracy: 0.8214\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5748 - accuracy: 0.8088 - val_loss: 0.6086 - val_accuracy: 0.8214\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5555 - accuracy: 0.8192 - val_loss: 0.6081 - val_accuracy: 0.8214\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5363 - accuracy: 0.8401 - val_loss: 0.6076 - val_accuracy: 0.8214\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5764 - accuracy: 0.7984 - val_loss: 0.6072 - val_accuracy: 0.8214\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5701 - accuracy: 0.7984 - val_loss: 0.6067 - val_accuracy: 0.8214\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.5490 - accuracy: 0.8088 - val_loss: 0.6062 - val_accuracy: 0.8214\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5474 - accuracy: 0.8296 - val_loss: 0.6057 - val_accuracy: 0.8214\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5451 - accuracy: 0.8192 - val_loss: 0.6053 - val_accuracy: 0.8214\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5471 - accuracy: 0.8296 - val_loss: 0.6048 - val_accuracy: 0.8214\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5432 - accuracy: 0.8296 - val_loss: 0.6043 - val_accuracy: 0.8214\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.5652 - accuracy: 0.7984 - val_loss: 0.6039 - val_accuracy: 0.8214\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5446 - accuracy: 0.8296 - val_loss: 0.6034 - val_accuracy: 0.8214\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5652 - accuracy: 0.8192 - val_loss: 0.6029 - val_accuracy: 0.8214\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5499 - accuracy: 0.8192 - val_loss: 0.6025 - val_accuracy: 0.8214\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5556 - accuracy: 0.8192 - val_loss: 0.6020 - val_accuracy: 0.8214\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5353 - accuracy: 0.8401 - val_loss: 0.6016 - val_accuracy: 0.8214\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5354 - accuracy: 0.8505 - val_loss: 0.6011 - val_accuracy: 0.8214\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5399 - accuracy: 0.8296 - val_loss: 0.6006 - val_accuracy: 0.8214\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5424 - accuracy: 0.8088 - val_loss: 0.6002 - val_accuracy: 0.8214\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5345 - accuracy: 0.8296 - val_loss: 0.5997 - val_accuracy: 0.8214\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5453 - accuracy: 0.8296 - val_loss: 0.5993 - val_accuracy: 0.8214\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5288 - accuracy: 0.8401 - val_loss: 0.5989 - val_accuracy: 0.8214\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5556 - accuracy: 0.8296 - val_loss: 0.5984 - val_accuracy: 0.8214\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5315 - accuracy: 0.8296 - val_loss: 0.5980 - val_accuracy: 0.8214\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5568 - accuracy: 0.7984 - val_loss: 0.5975 - val_accuracy: 0.8214\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5629 - accuracy: 0.7880 - val_loss: 0.5971 - val_accuracy: 0.8214\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5194 - accuracy: 0.8505 - val_loss: 0.5966 - val_accuracy: 0.8214\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5451 - accuracy: 0.8192 - val_loss: 0.5962 - val_accuracy: 0.8214\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5494 - accuracy: 0.8192 - val_loss: 0.5957 - val_accuracy: 0.8214\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5385 - accuracy: 0.8192 - val_loss: 0.5953 - val_accuracy: 0.8214\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5181 - accuracy: 0.8401 - val_loss: 0.5949 - val_accuracy: 0.8214\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5275 - accuracy: 0.8296 - val_loss: 0.5944 - val_accuracy: 0.8214\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5446 - accuracy: 0.8296 - val_loss: 0.5940 - val_accuracy: 0.8214\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5443 - accuracy: 0.8192 - val_loss: 0.5936 - val_accuracy: 0.8214\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5397 - accuracy: 0.8296 - val_loss: 0.5931 - val_accuracy: 0.8214\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5378 - accuracy: 0.8088 - val_loss: 0.5927 - val_accuracy: 0.8214\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5307 - accuracy: 0.8192 - val_loss: 0.5923 - val_accuracy: 0.8214\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5304 - accuracy: 0.8505 - val_loss: 0.5918 - val_accuracy: 0.8214\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5335 - accuracy: 0.8296 - val_loss: 0.5914 - val_accuracy: 0.8214\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5444 - accuracy: 0.8088 - val_loss: 0.5910 - val_accuracy: 0.8214\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5372 - accuracy: 0.8192 - val_loss: 0.5906 - val_accuracy: 0.8214\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5309 - accuracy: 0.8401 - val_loss: 0.5901 - val_accuracy: 0.8214\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5422 - accuracy: 0.8296 - val_loss: 0.5897 - val_accuracy: 0.8214\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5387 - accuracy: 0.8192 - val_loss: 0.5893 - val_accuracy: 0.8214\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5457 - accuracy: 0.8088 - val_loss: 0.5889 - val_accuracy: 0.8214\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5083 - accuracy: 0.8505 - val_loss: 0.5885 - val_accuracy: 0.8214\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5409 - accuracy: 0.8088 - val_loss: 0.5880 - val_accuracy: 0.8214\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5343 - accuracy: 0.8296 - val_loss: 0.5876 - val_accuracy: 0.8214\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5448 - accuracy: 0.8296 - val_loss: 0.5872 - val_accuracy: 0.8214\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5429 - accuracy: 0.8088 - val_loss: 0.5868 - val_accuracy: 0.8214\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5346 - accuracy: 0.8296 - val_loss: 0.5864 - val_accuracy: 0.8214\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5112 - accuracy: 0.8401 - val_loss: 0.5860 - val_accuracy: 0.8214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5349 - accuracy: 0.8296 - val_loss: 0.5856 - val_accuracy: 0.8214\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.5366 - accuracy: 0.7984 - val_loss: 0.5852 - val_accuracy: 0.8214\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5271 - accuracy: 0.8401 - val_loss: 0.5848 - val_accuracy: 0.8214\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5447 - accuracy: 0.8088 - val_loss: 0.5844 - val_accuracy: 0.8214\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5124 - accuracy: 0.8296 - val_loss: 0.5839 - val_accuracy: 0.8214\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5276 - accuracy: 0.8192 - val_loss: 0.5835 - val_accuracy: 0.8214\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5394 - accuracy: 0.8192 - val_loss: 0.5831 - val_accuracy: 0.8214\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5083 - accuracy: 0.8401 - val_loss: 0.5827 - val_accuracy: 0.8214\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5304 - accuracy: 0.8088 - val_loss: 0.5823 - val_accuracy: 0.8214\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.5406 - accuracy: 0.7984 - val_loss: 0.5819 - val_accuracy: 0.8214\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.5285 - accuracy: 0.8192 - val_loss: 0.5815 - val_accuracy: 0.8214\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5273 - accuracy: 0.8192 - val_loss: 0.5811 - val_accuracy: 0.8214\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5218 - accuracy: 0.8505 - val_loss: 0.5807 - val_accuracy: 0.8214\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5211 - accuracy: 0.8296 - val_loss: 0.5803 - val_accuracy: 0.8214\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5276 - accuracy: 0.8296 - val_loss: 0.5799 - val_accuracy: 0.8214\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5418 - accuracy: 0.8088 - val_loss: 0.5795 - val_accuracy: 0.8214\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5249 - accuracy: 0.8192 - val_loss: 0.5791 - val_accuracy: 0.8214\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5134 - accuracy: 0.8296 - val_loss: 0.5787 - val_accuracy: 0.8214\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5209 - accuracy: 0.8088 - val_loss: 0.5783 - val_accuracy: 0.8214\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5365 - accuracy: 0.7984 - val_loss: 0.5779 - val_accuracy: 0.8214\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5129 - accuracy: 0.8296 - val_loss: 0.5775 - val_accuracy: 0.8214\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5295 - accuracy: 0.8088 - val_loss: 0.5771 - val_accuracy: 0.8214\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5164 - accuracy: 0.8401 - val_loss: 0.5767 - val_accuracy: 0.8214\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5308 - accuracy: 0.8192 - val_loss: 0.5764 - val_accuracy: 0.8214\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.5290 - accuracy: 0.8296 - val_loss: 0.5760 - val_accuracy: 0.8214\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5219 - accuracy: 0.8296 - val_loss: 0.5756 - val_accuracy: 0.8214\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5103 - accuracy: 0.8296 - val_loss: 0.5752 - val_accuracy: 0.8214\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5183 - accuracy: 0.8296 - val_loss: 0.5748 - val_accuracy: 0.8214\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5262 - accuracy: 0.8088 - val_loss: 0.5744 - val_accuracy: 0.8214\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5187 - accuracy: 0.8088 - val_loss: 0.5741 - val_accuracy: 0.8214\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5083 - accuracy: 0.8296 - val_loss: 0.5737 - val_accuracy: 0.8214\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5339 - accuracy: 0.8088 - val_loss: 0.5733 - val_accuracy: 0.8214\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5205 - accuracy: 0.8192 - val_loss: 0.5729 - val_accuracy: 0.8214\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5342 - accuracy: 0.7984 - val_loss: 0.5725 - val_accuracy: 0.8214\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5450 - accuracy: 0.7776 - val_loss: 0.5722 - val_accuracy: 0.8214\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5206 - accuracy: 0.7984 - val_loss: 0.5718 - val_accuracy: 0.8214\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5059 - accuracy: 0.8401 - val_loss: 0.5714 - val_accuracy: 0.8214\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5227 - accuracy: 0.7984 - val_loss: 0.5710 - val_accuracy: 0.8214\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5054 - accuracy: 0.8296 - val_loss: 0.5707 - val_accuracy: 0.8214\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5244 - accuracy: 0.8088 - val_loss: 0.5703 - val_accuracy: 0.8214\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5058 - accuracy: 0.8505 - val_loss: 0.5699 - val_accuracy: 0.8214\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5312 - accuracy: 0.7984 - val_loss: 0.5695 - val_accuracy: 0.8214\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.5089 - accuracy: 0.8192 - val_loss: 0.5692 - val_accuracy: 0.8214\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5139 - accuracy: 0.8088 - val_loss: 0.5688 - val_accuracy: 0.8214\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5349 - accuracy: 0.8088 - val_loss: 0.5684 - val_accuracy: 0.8214\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.5014 - accuracy: 0.8296 - val_loss: 0.5681 - val_accuracy: 0.8214\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5085 - accuracy: 0.8296 - val_loss: 0.5677 - val_accuracy: 0.8214\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5086 - accuracy: 0.8192 - val_loss: 0.5673 - val_accuracy: 0.8214\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5172 - accuracy: 0.8088 - val_loss: 0.5670 - val_accuracy: 0.8214\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4873 - accuracy: 0.8505 - val_loss: 0.5666 - val_accuracy: 0.8214\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5008 - accuracy: 0.8505 - val_loss: 0.5663 - val_accuracy: 0.8214\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4985 - accuracy: 0.8505 - val_loss: 0.5659 - val_accuracy: 0.8214\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4963 - accuracy: 0.8296 - val_loss: 0.5656 - val_accuracy: 0.8214\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4978 - accuracy: 0.8296 - val_loss: 0.5652 - val_accuracy: 0.8214\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5213 - accuracy: 0.8192 - val_loss: 0.5648 - val_accuracy: 0.8214\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5166 - accuracy: 0.8088 - val_loss: 0.5645 - val_accuracy: 0.8571\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4994 - accuracy: 0.8401 - val_loss: 0.5641 - val_accuracy: 0.8571\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5105 - accuracy: 0.8296 - val_loss: 0.5638 - val_accuracy: 0.8571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5156 - accuracy: 0.7984 - val_loss: 0.5634 - val_accuracy: 0.8571\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4990 - accuracy: 0.8192 - val_loss: 0.5631 - val_accuracy: 0.8571\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5021 - accuracy: 0.8296 - val_loss: 0.5627 - val_accuracy: 0.8571\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5082 - accuracy: 0.8192 - val_loss: 0.5624 - val_accuracy: 0.8571\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5248 - accuracy: 0.8088 - val_loss: 0.5620 - val_accuracy: 0.8571\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5212 - accuracy: 0.8192 - val_loss: 0.5617 - val_accuracy: 0.8571\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4931 - accuracy: 0.8192 - val_loss: 0.5613 - val_accuracy: 0.8571\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5380 - accuracy: 0.7880 - val_loss: 0.5610 - val_accuracy: 0.8571\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5124 - accuracy: 0.7984 - val_loss: 0.5606 - val_accuracy: 0.8571\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5114 - accuracy: 0.8088 - val_loss: 0.5603 - val_accuracy: 0.8571\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5056 - accuracy: 0.8192 - val_loss: 0.5599 - val_accuracy: 0.8571\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5150 - accuracy: 0.8401 - val_loss: 0.5596 - val_accuracy: 0.8571\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5055 - accuracy: 0.8192 - val_loss: 0.5593 - val_accuracy: 0.8571\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5099 - accuracy: 0.8192 - val_loss: 0.5589 - val_accuracy: 0.8571\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4975 - accuracy: 0.8296 - val_loss: 0.5586 - val_accuracy: 0.8571\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5227 - accuracy: 0.8296 - val_loss: 0.5582 - val_accuracy: 0.8571\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5006 - accuracy: 0.8192 - val_loss: 0.5579 - val_accuracy: 0.8571\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4785 - accuracy: 0.8505 - val_loss: 0.5576 - val_accuracy: 0.8571\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5006 - accuracy: 0.8401 - val_loss: 0.5572 - val_accuracy: 0.8571\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5308 - accuracy: 0.7984 - val_loss: 0.5569 - val_accuracy: 0.8571\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5069 - accuracy: 0.8192 - val_loss: 0.5565 - val_accuracy: 0.8571\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5034 - accuracy: 0.8401 - val_loss: 0.5562 - val_accuracy: 0.8571\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5150 - accuracy: 0.8192 - val_loss: 0.5559 - val_accuracy: 0.8571\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4968 - accuracy: 0.8088 - val_loss: 0.5555 - val_accuracy: 0.8571\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4942 - accuracy: 0.8296 - val_loss: 0.5552 - val_accuracy: 0.8571\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5010 - accuracy: 0.8296 - val_loss: 0.5549 - val_accuracy: 0.8571\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4995 - accuracy: 0.8088 - val_loss: 0.5545 - val_accuracy: 0.8571\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5118 - accuracy: 0.7984 - val_loss: 0.5542 - val_accuracy: 0.8571\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4943 - accuracy: 0.8401 - val_loss: 0.5539 - val_accuracy: 0.8571\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4827 - accuracy: 0.8401 - val_loss: 0.5535 - val_accuracy: 0.8571\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4987 - accuracy: 0.8296 - val_loss: 0.5532 - val_accuracy: 0.8571\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4858 - accuracy: 0.8296 - val_loss: 0.5529 - val_accuracy: 0.8571\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4888 - accuracy: 0.8192 - val_loss: 0.5526 - val_accuracy: 0.8571\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5092 - accuracy: 0.8192 - val_loss: 0.5522 - val_accuracy: 0.8571\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4978 - accuracy: 0.8192 - val_loss: 0.5519 - val_accuracy: 0.8571\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4940 - accuracy: 0.8401 - val_loss: 0.5516 - val_accuracy: 0.8571\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs_num, validation_split=0.3, callbacks=[extractor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "distinct-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_output = extractor.get_testing_results()\n",
    "\n",
    "# all_predictions = []\n",
    "# for epoch in extractor.get_stored_predictions():\n",
    "#     epoch_predictions = []\n",
    "#     for dp in epoch:\n",
    "#         epoch_predictions.append(int(dp.argmax()))                       \n",
    "#     all_predictions.append(epoch_predictions)\n",
    "\n",
    "# extractor.set_stored_predictions(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "exotic-rebate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[    epoch           actual  prediction       confidence_score  \\\n",
       " 0       0  [1.0, 0.0, 0.0]           2  [0.575, 0.622, 0.128]   \n",
       " 1       0  [0.0, 0.0, 1.0]           2  [0.237, 0.086, 0.793]   \n",
       " 2       0  [1.0, 0.0, 0.0]           2  [0.627, 0.781, 0.056]   \n",
       " 3       0  [0.0, 0.0, 1.0]           2  [0.359, 0.412, 0.542]   \n",
       " 4       0  [1.0, 0.0, 0.0]           2  [0.756, 0.791, 0.173]   \n",
       " 5       0  [1.0, 0.0, 0.0]           1  [0.805, 0.939, 0.068]   \n",
       " 6       0  [1.0, 0.0, 0.0]           2   [0.396, 0.353, 0.15]   \n",
       " 7       0  [1.0, 0.0, 0.0]           2  [0.247, 0.313, 0.063]   \n",
       " 8       0  [0.0, 0.0, 1.0]           2   [0.241, 0.16, 0.705]   \n",
       " 9       0  [0.0, 0.0, 1.0]           2  [0.436, 0.511, 0.578]   \n",
       " 10      0  [0.0, 0.0, 1.0]           2  [0.158, 0.159, 0.694]   \n",
       " 11      0  [0.0, 0.0, 1.0]           2  [0.139, 0.173, 0.652]   \n",
       " 12      0  [0.0, 0.0, 1.0]           2  [0.274, 0.225, 0.692]   \n",
       " 13      0  [1.0, 0.0, 0.0]           2  [0.494, 0.589, 0.093]   \n",
       " 14      0  [1.0, 0.0, 0.0]           2    [0.598, 0.753, 0.0]   \n",
       " 15      0  [0.0, 0.0, 1.0]           2  [0.297, 0.275, 0.651]   \n",
       " 16      0  [0.0, 0.0, 1.0]           2  [0.342, 0.435, 0.534]   \n",
       " 17      0  [1.0, 0.0, 0.0]           2  [0.726, 0.731, 0.219]   \n",
       " 18      0  [1.0, 0.0, 0.0]           2   [0.562, 0.593, 0.15]   \n",
       " 19      0  [1.0, 0.0, 0.0]           2  [0.574, 0.555, 0.211]   \n",
       " 20      0  [1.0, 0.0, 0.0]           2  [0.377, 0.493, 0.026]   \n",
       " 21      0  [1.0, 0.0, 0.0]           2  [0.692, 0.704, 0.164]   \n",
       " 22      0  [1.0, 0.0, 0.0]           2    [0.562, 0.513, 0.2]   \n",
       " 23      0  [1.0, 0.0, 0.0]           1   [0.78, 0.908, 0.095]   \n",
       " 24      0  [1.0, 0.0, 0.0]           2  [0.599, 0.637, 0.152]   \n",
       " 25      0  [1.0, 0.0, 0.0]           2  [0.583, 0.653, 0.107]   \n",
       " 26      0  [1.0, 0.0, 0.0]           2       [1.0, 1.0, 0.26]   \n",
       " 27      0  [1.0, 0.0, 0.0]           2   [0.546, 0.65, 0.122]   \n",
       " 28      0  [1.0, 0.0, 0.0]           2  [0.493, 0.532, 0.128]   \n",
       " 29      0  [1.0, 0.0, 0.0]           2   [0.644, 0.684, 0.15]   \n",
       " 30      0  [0.0, 1.0, 0.0]           2   [0.482, 0.528, 0.51]   \n",
       " 31      0  [0.0, 1.0, 0.0]           2  [0.269, 0.286, 0.531]   \n",
       " 32      0  [0.0, 1.0, 0.0]           2  [0.757, 0.839, 0.395]   \n",
       " 33      0  [0.0, 1.0, 0.0]           2   [0.398, 0.269, 0.66]   \n",
       " 34      0  [0.0, 1.0, 0.0]           2  [0.867, 0.875, 0.503]   \n",
       " 35      0  [0.0, 1.0, 0.0]           2  [0.342, 0.216, 0.633]   \n",
       " 36      0  [0.0, 1.0, 0.0]           2  [0.415, 0.524, 0.453]   \n",
       " 37      0  [0.0, 1.0, 0.0]           2  [0.302, 0.407, 0.483]   \n",
       " 38      0  [0.0, 1.0, 0.0]           2  [0.431, 0.326, 0.613]   \n",
       " 39      0  [0.0, 1.0, 0.0]           2  [0.426, 0.431, 0.548]   \n",
       " 40      0  [0.0, 1.0, 0.0]           2  [0.636, 0.576, 0.511]   \n",
       " 41      0  [0.0, 1.0, 0.0]           2   [0.683, 0.66, 0.527]   \n",
       " 42      0  [0.0, 1.0, 0.0]           2  [0.559, 0.526, 0.527]   \n",
       " 43      0  [0.0, 1.0, 0.0]           2   [0.44, 0.455, 0.611]   \n",
       " 44      0  [0.0, 1.0, 0.0]           2  [0.267, 0.352, 0.447]   \n",
       " 45      0  [0.0, 1.0, 0.0]           2  [0.315, 0.212, 0.644]   \n",
       " 46      0  [0.0, 1.0, 0.0]           2  [0.568, 0.618, 0.509]   \n",
       " 47      0  [0.0, 1.0, 0.0]           2  [0.387, 0.386, 0.549]   \n",
       " 48      0  [0.0, 1.0, 0.0]           2  [0.542, 0.577, 0.501]   \n",
       " 49      0  [0.0, 1.0, 0.0]           2  [0.473, 0.517, 0.479]   \n",
       " 50      0  [0.0, 1.0, 0.0]           2   [0.733, 0.77, 0.403]   \n",
       " 51      0  [0.0, 0.0, 1.0]           2  [0.317, 0.336, 0.646]   \n",
       " 52      0  [0.0, 0.0, 1.0]           2    [0.218, 0.07, 0.85]   \n",
       " 53      0  [0.0, 0.0, 1.0]           2   [0.407, 0.29, 0.804]   \n",
       " 54      0  [0.0, 0.0, 1.0]           2      [0.0, 0.0, 0.697]   \n",
       " 55      0  [0.0, 0.0, 1.0]           2  [0.284, 0.269, 0.659]   \n",
       " 56      0  [0.0, 0.0, 1.0]           2    [0.225, 0.011, 1.0]   \n",
       " 57      0  [0.0, 0.0, 1.0]           2  [0.611, 0.537, 0.706]   \n",
       " 58      0  [0.0, 0.0, 1.0]           2  [0.431, 0.378, 0.662]   \n",
       " 59      0  [0.0, 0.0, 1.0]           2   [0.156, 0.17, 0.647]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0      10  [1.0, 0.0, 0.0]           2  [0.654, 0.585, 0.122]   \n",
       " 1      10  [0.0, 0.0, 1.0]           2   [0.22, 0.085, 0.787]   \n",
       " 2      10  [1.0, 0.0, 0.0]           1    [0.7, 0.754, 0.058]   \n",
       " 3      10  [0.0, 0.0, 1.0]           2  [0.346, 0.429, 0.546]   \n",
       " 4      10  [1.0, 0.0, 0.0]           0  [0.807, 0.767, 0.172]   \n",
       " 5      10  [1.0, 0.0, 0.0]           1   [0.86, 0.919, 0.073]   \n",
       " 6      10  [1.0, 0.0, 0.0]           2  [0.505, 0.293, 0.134]   \n",
       " 7      10  [1.0, 0.0, 0.0]           2   [0.373, 0.25, 0.048]   \n",
       " 8      10  [0.0, 0.0, 1.0]           2  [0.218, 0.169, 0.704]   \n",
       " 9      10  [0.0, 0.0, 1.0]           2    [0.4, 0.543, 0.587]   \n",
       " 10     10  [0.0, 0.0, 1.0]           2  [0.139, 0.172, 0.695]   \n",
       " 11     10  [0.0, 0.0, 1.0]           2  [0.123, 0.188, 0.653]   \n",
       " 12     10  [0.0, 0.0, 1.0]           2  [0.244, 0.241, 0.693]   \n",
       " 13     10  [1.0, 0.0, 0.0]           2  [0.581, 0.551, 0.088]   \n",
       " 14     10  [1.0, 0.0, 0.0]           1    [0.689, 0.716, 0.0]   \n",
       " 15     10  [0.0, 0.0, 1.0]           2  [0.275, 0.289, 0.652]   \n",
       " 16     10  [0.0, 0.0, 1.0]           2  [0.328, 0.455, 0.539]   \n",
       " 17     10  [1.0, 0.0, 0.0]           2  [0.772, 0.708, 0.217]   \n",
       " 18     10  [1.0, 0.0, 0.0]           2   [0.64, 0.554, 0.143]   \n",
       " 19     10  [1.0, 0.0, 0.0]           2   [0.641, 0.52, 0.204]   \n",
       " 20     10  [1.0, 0.0, 0.0]           2  [0.495, 0.439, 0.017]   \n",
       " 21     10  [1.0, 0.0, 0.0]           2   [0.753, 0.673, 0.16]   \n",
       " 22     10  [1.0, 0.0, 0.0]           2   [0.64, 0.469, 0.189]   \n",
       " 23     10  [1.0, 0.0, 0.0]           1      [0.83, 0.89, 0.1]   \n",
       " 24     10  [1.0, 0.0, 0.0]           2  [0.672, 0.603, 0.147]   \n",
       " 25     10  [1.0, 0.0, 0.0]           2  [0.661, 0.617, 0.103]   \n",
       " 26     10  [1.0, 0.0, 0.0]           0      [1.0, 1.0, 0.267]   \n",
       " 27     10  [1.0, 0.0, 0.0]           2   [0.615, 0.622, 0.12]   \n",
       " 28     10  [1.0, 0.0, 0.0]           2  [0.584, 0.488, 0.119]   \n",
       " 29     10  [1.0, 0.0, 0.0]           2  [0.711, 0.652, 0.147]   \n",
       " 30     10  [0.0, 1.0, 0.0]           2   [0.47, 0.543, 0.513]   \n",
       " 31     10  [0.0, 1.0, 0.0]           2  [0.279, 0.285, 0.528]   \n",
       " 32     10  [0.0, 1.0, 0.0]           2   [0.734, 0.86, 0.405]   \n",
       " 33     10  [0.0, 1.0, 0.0]           2  [0.389, 0.268, 0.655]   \n",
       " 34     10  [0.0, 1.0, 0.0]           2  [0.816, 0.907, 0.515]   \n",
       " 35     10  [0.0, 1.0, 0.0]           2  [0.343, 0.208, 0.625]   \n",
       " 36     10  [0.0, 1.0, 0.0]           2  [0.411, 0.539, 0.458]   \n",
       " 37     10  [0.0, 1.0, 0.0]           2   [0.302, 0.42, 0.486]   \n",
       " 38     10  [0.0, 1.0, 0.0]           2  [0.423, 0.326, 0.609]   \n",
       " 39     10  [0.0, 1.0, 0.0]           2   [0.415, 0.443, 0.55]   \n",
       " 40     10  [0.0, 1.0, 0.0]           2  [0.622, 0.582, 0.513]   \n",
       " 41     10  [0.0, 1.0, 0.0]           2   [0.653, 0.68, 0.533]   \n",
       " 42     10  [0.0, 1.0, 0.0]           2  [0.545, 0.536, 0.528]   \n",
       " 43     10  [0.0, 1.0, 0.0]           2   [0.41, 0.477, 0.615]   \n",
       " 44     10  [0.0, 1.0, 0.0]           2  [0.285, 0.352, 0.446]   \n",
       " 45     10  [0.0, 1.0, 0.0]           2  [0.313, 0.208, 0.638]   \n",
       " 46     10  [0.0, 1.0, 0.0]           2  [0.546, 0.638, 0.514]   \n",
       " 47     10  [0.0, 1.0, 0.0]           2  [0.384, 0.392, 0.548]   \n",
       " 48     10  [0.0, 1.0, 0.0]           2  [0.524, 0.594, 0.506]   \n",
       " 49     10  [0.0, 1.0, 0.0]           2  [0.469, 0.527, 0.481]   \n",
       " 50     10  [0.0, 1.0, 0.0]           2  [0.715, 0.786, 0.411]   \n",
       " 51     10  [0.0, 0.0, 1.0]           2  [0.292, 0.355, 0.649]   \n",
       " 52     10  [0.0, 0.0, 1.0]           2  [0.187, 0.076, 0.845]   \n",
       " 53     10  [0.0, 0.0, 1.0]           2  [0.353, 0.315, 0.807]   \n",
       " 54     10  [0.0, 0.0, 1.0]           2      [0.0, 0.0, 0.693]   \n",
       " 55     10  [0.0, 0.0, 1.0]           2  [0.265, 0.282, 0.659]   \n",
       " 56     10  [0.0, 0.0, 1.0]           2    [0.159, 0.029, 1.0]   \n",
       " 57     10  [0.0, 0.0, 1.0]           2    [0.55, 0.57, 0.714]   \n",
       " 58     10  [0.0, 0.0, 1.0]           2  [0.396, 0.398, 0.665]   \n",
       " 59     10  [0.0, 0.0, 1.0]           2  [0.149, 0.179, 0.645]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0      20  [1.0, 0.0, 0.0]           0  [0.718, 0.543, 0.117]   \n",
       " 1      20  [0.0, 0.0, 1.0]           2  [0.205, 0.084, 0.782]   \n",
       " 2      20  [1.0, 0.0, 0.0]           0  [0.759, 0.723, 0.058]   \n",
       " 3      20  [0.0, 0.0, 1.0]           2  [0.335, 0.444, 0.551]   \n",
       " 4      20  [1.0, 0.0, 0.0]           0  [0.847, 0.741, 0.169]   \n",
       " 5      20  [1.0, 0.0, 0.0]           0  [0.903, 0.897, 0.075]   \n",
       " 6      20  [1.0, 0.0, 0.0]           0  [0.592, 0.228, 0.121]   \n",
       " 7      20  [1.0, 0.0, 0.0]           2  [0.475, 0.182, 0.039]   \n",
       " 8      20  [0.0, 0.0, 1.0]           2  [0.199, 0.178, 0.706]   \n",
       " 9      20  [0.0, 0.0, 1.0]           2  [0.371, 0.574, 0.595]   \n",
       " 10     20  [0.0, 0.0, 1.0]           2  [0.124, 0.186, 0.698]   \n",
       " 11     20  [0.0, 0.0, 1.0]           2   [0.11, 0.202, 0.659]   \n",
       " 12     20  [0.0, 0.0, 1.0]           2  [0.221, 0.257, 0.697]   \n",
       " 13     20  [1.0, 0.0, 0.0]           0   [0.65, 0.508, 0.085]   \n",
       " 14     20  [1.0, 0.0, 0.0]           0    [0.762, 0.675, 0.0]   \n",
       " 15     20  [0.0, 0.0, 1.0]           2  [0.257, 0.303, 0.655]   \n",
       " 16     20  [0.0, 0.0, 1.0]           2  [0.316, 0.474, 0.545]   \n",
       " 17     20  [1.0, 0.0, 0.0]           0  [0.809, 0.683, 0.213]   \n",
       " 18     20  [1.0, 0.0, 0.0]           0  [0.703, 0.512, 0.137]   \n",
       " 19     20  [1.0, 0.0, 0.0]           2  [0.693, 0.482, 0.197]   \n",
       " 20     20  [1.0, 0.0, 0.0]           0   [0.591, 0.379, 0.01]   \n",
       " 21     20  [1.0, 0.0, 0.0]           0   [0.803, 0.64, 0.155]   \n",
       " 22     20  [1.0, 0.0, 0.0]           0  [0.703, 0.422, 0.179]   \n",
       " 23     20  [1.0, 0.0, 0.0]           0   [0.871, 0.87, 0.102]   \n",
       " 24     20  [1.0, 0.0, 0.0]           0   [0.73, 0.565, 0.141]   \n",
       " 25     20  [1.0, 0.0, 0.0]           0    [0.723, 0.578, 0.1]   \n",
       " 26     20  [1.0, 0.0, 0.0]           0      [1.0, 1.0, 0.269]   \n",
       " 27     20  [1.0, 0.0, 0.0]           0   [0.67, 0.591, 0.119]   \n",
       " 28     20  [1.0, 0.0, 0.0]           0  [0.657, 0.439, 0.112]   \n",
       " 29     20  [1.0, 0.0, 0.0]           0  [0.765, 0.617, 0.142]   \n",
       " 30     20  [0.0, 1.0, 0.0]           2   [0.46, 0.557, 0.516]   \n",
       " 31     20  [0.0, 1.0, 0.0]           2  [0.286, 0.283, 0.528]   \n",
       " 32     20  [0.0, 1.0, 0.0]           2  [0.716, 0.881, 0.412]   \n",
       " 33     20  [0.0, 1.0, 0.0]           2   [0.381, 0.266, 0.65]   \n",
       " 34     20  [0.0, 1.0, 0.0]           2  [0.775, 0.939, 0.522]   \n",
       " 35     20  [0.0, 1.0, 0.0]           2     [0.344, 0.2, 0.62]   \n",
       " 36     20  [0.0, 1.0, 0.0]           2  [0.408, 0.553, 0.463]   \n",
       " 37     20  [0.0, 1.0, 0.0]           2  [0.302, 0.431, 0.491]   \n",
       " 38     20  [0.0, 1.0, 0.0]           2  [0.417, 0.325, 0.605]   \n",
       " 39     20  [0.0, 1.0, 0.0]           2  [0.405, 0.453, 0.552]   \n",
       " 40     20  [0.0, 1.0, 0.0]           2  [0.611, 0.589, 0.512]   \n",
       " 41     20  [0.0, 1.0, 0.0]           2  [0.629, 0.699, 0.535]   \n",
       " 42     20  [0.0, 1.0, 0.0]           2  [0.533, 0.545, 0.529]   \n",
       " 43     20  [0.0, 1.0, 0.0]           2     [0.386, 0.5, 0.62]   \n",
       " 44     20  [0.0, 1.0, 0.0]           2   [0.299, 0.35, 0.448]   \n",
       " 45     20  [0.0, 1.0, 0.0]           2  [0.311, 0.204, 0.634]   \n",
       " 46     20  [0.0, 1.0, 0.0]           2  [0.528, 0.658, 0.518]   \n",
       " 47     20  [0.0, 1.0, 0.0]           2   [0.38, 0.397, 0.548]   \n",
       " 48     20  [0.0, 1.0, 0.0]           2    [0.51, 0.61, 0.509]   \n",
       " 49     20  [0.0, 1.0, 0.0]           2  [0.465, 0.536, 0.484]   \n",
       " 50     20  [0.0, 1.0, 0.0]           2  [0.701, 0.801, 0.416]   \n",
       " 51     20  [0.0, 0.0, 1.0]           2  [0.272, 0.375, 0.652]   \n",
       " 52     20  [0.0, 0.0, 1.0]           2  [0.163, 0.084, 0.842]   \n",
       " 53     20  [0.0, 0.0, 1.0]           2   [0.31, 0.341, 0.809]   \n",
       " 54     20  [0.0, 0.0, 1.0]           2      [0.0, 0.0, 0.694]   \n",
       " 55     20  [0.0, 0.0, 1.0]           2   [0.25, 0.295, 0.661]   \n",
       " 56     20  [0.0, 0.0, 1.0]           2    [0.108, 0.051, 1.0]   \n",
       " 57     20  [0.0, 0.0, 1.0]           2  [0.501, 0.605, 0.718]   \n",
       " 58     20  [0.0, 0.0, 1.0]           2  [0.368, 0.418, 0.668]   \n",
       " 59     20  [0.0, 0.0, 1.0]           2  [0.143, 0.186, 0.647]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0      30  [1.0, 0.0, 0.0]           0  [0.769, 0.499, 0.112]   \n",
       " 1      30  [0.0, 0.0, 1.0]           2  [0.194, 0.085, 0.778]   \n",
       " 2      30  [1.0, 0.0, 0.0]           0   [0.805, 0.69, 0.059]   \n",
       " 3      30  [0.0, 0.0, 1.0]           2  [0.325, 0.458, 0.556]   \n",
       " 4      30  [1.0, 0.0, 0.0]           0   [0.88, 0.714, 0.164]   \n",
       " 5      30  [1.0, 0.0, 0.0]           0  [0.938, 0.873, 0.074]   \n",
       " 6      30  [1.0, 0.0, 0.0]           0  [0.662, 0.161, 0.111]   \n",
       " 7      30  [1.0, 0.0, 0.0]           0   [0.557, 0.11, 0.034]   \n",
       " 8      30  [0.0, 0.0, 1.0]           2   [0.184, 0.188, 0.71]   \n",
       " 9      30  [0.0, 0.0, 1.0]           2  [0.347, 0.604, 0.603]   \n",
       " 10     30  [0.0, 0.0, 1.0]           2  [0.111, 0.199, 0.704]   \n",
       " 11     30  [0.0, 0.0, 1.0]           2  [0.099, 0.216, 0.667]   \n",
       " 12     30  [0.0, 0.0, 1.0]           2  [0.202, 0.273, 0.703]   \n",
       " 13     30  [1.0, 0.0, 0.0]           0  [0.706, 0.463, 0.083]   \n",
       " 14     30  [1.0, 0.0, 0.0]           0     [0.82, 0.631, 0.0]   \n",
       " 15     30  [0.0, 0.0, 1.0]           2  [0.243, 0.317, 0.659]   \n",
       " 16     30  [0.0, 0.0, 1.0]           2  [0.306, 0.492, 0.552]   \n",
       " 17     30  [1.0, 0.0, 0.0]           0  [0.838, 0.657, 0.207]   \n",
       " 18     30  [1.0, 0.0, 0.0]           0  [0.754, 0.469, 0.131]   \n",
       " 19     30  [1.0, 0.0, 0.0]           0  [0.735, 0.442, 0.191]   \n",
       " 20     30  [1.0, 0.0, 0.0]           0  [0.668, 0.316, 0.006]   \n",
       " 21     30  [1.0, 0.0, 0.0]           0   [0.842, 0.604, 0.15]   \n",
       " 22     30  [1.0, 0.0, 0.0]           0  [0.753, 0.373, 0.171]   \n",
       " 23     30  [1.0, 0.0, 0.0]           0  [0.903, 0.849, 0.101]   \n",
       " 24     30  [1.0, 0.0, 0.0]           0  [0.776, 0.525, 0.136]   \n",
       " 25     30  [1.0, 0.0, 0.0]           0  [0.773, 0.536, 0.096]   \n",
       " 26     30  [1.0, 0.0, 0.0]           0      [1.0, 1.0, 0.266]   \n",
       " 27     30  [1.0, 0.0, 0.0]           0  [0.713, 0.557, 0.119]   \n",
       " 28     30  [1.0, 0.0, 0.0]           0  [0.715, 0.388, 0.106]   \n",
       " 29     30  [1.0, 0.0, 0.0]           0  [0.808, 0.581, 0.137]   \n",
       " 30     30  [0.0, 1.0, 0.0]           2  [0.451, 0.571, 0.519]   \n",
       " 31     30  [0.0, 1.0, 0.0]           2   [0.291, 0.28, 0.529]   \n",
       " 32     30  [0.0, 1.0, 0.0]           2  [0.701, 0.902, 0.415]   \n",
       " 33     30  [0.0, 1.0, 0.0]           2  [0.373, 0.265, 0.646]   \n",
       " 34     30  [0.0, 1.0, 0.0]           2  [0.742, 0.973, 0.524]   \n",
       " 35     30  [0.0, 1.0, 0.0]           2  [0.344, 0.191, 0.616]   \n",
       " 36     30  [0.0, 1.0, 0.0]           2  [0.405, 0.565, 0.468]   \n",
       " 37     30  [0.0, 1.0, 0.0]           2  [0.301, 0.441, 0.498]   \n",
       " 38     30  [0.0, 1.0, 0.0]           2  [0.411, 0.325, 0.602]   \n",
       " 39     30  [0.0, 1.0, 0.0]           2  [0.397, 0.463, 0.554]   \n",
       " 40     30  [0.0, 1.0, 0.0]           2   [0.602, 0.595, 0.51]   \n",
       " 41     30  [0.0, 1.0, 0.0]           2  [0.609, 0.718, 0.535]   \n",
       " 42     30  [0.0, 1.0, 0.0]           2  [0.523, 0.553, 0.528]   \n",
       " 43     30  [0.0, 1.0, 0.0]           2  [0.366, 0.522, 0.623]   \n",
       " 44     30  [0.0, 1.0, 0.0]           2   [0.31, 0.347, 0.452]   \n",
       " 45     30  [0.0, 1.0, 0.0]           2  [0.308, 0.199, 0.632]   \n",
       " 46     30  [0.0, 1.0, 0.0]           2  [0.513, 0.678, 0.521]   \n",
       " 47     30  [0.0, 1.0, 0.0]           2  [0.377, 0.402, 0.549]   \n",
       " 48     30  [0.0, 1.0, 0.0]           2  [0.499, 0.625, 0.512]   \n",
       " 49     30  [0.0, 1.0, 0.0]           2  [0.461, 0.544, 0.486]   \n",
       " 50     30  [0.0, 1.0, 0.0]           2  [0.689, 0.816, 0.418]   \n",
       " 51     30  [0.0, 0.0, 1.0]           2  [0.256, 0.394, 0.657]   \n",
       " 52     30  [0.0, 0.0, 1.0]           2   [0.143, 0.094, 0.84]   \n",
       " 53     30  [0.0, 0.0, 1.0]           2   [0.276, 0.368, 0.81]   \n",
       " 54     30  [0.0, 0.0, 1.0]           2        [0.0, 0.0, 0.7]   \n",
       " 55     30  [0.0, 0.0, 1.0]           2  [0.238, 0.307, 0.664]   \n",
       " 56     30  [0.0, 0.0, 1.0]           2    [0.069, 0.077, 1.0]   \n",
       " 57     30  [0.0, 0.0, 1.0]           2    [0.462, 0.64, 0.72]   \n",
       " 58     30  [0.0, 0.0, 1.0]           2  [0.345, 0.438, 0.671]   \n",
       " 59     30  [0.0, 0.0, 1.0]           2  [0.137, 0.193, 0.652]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0      40  [1.0, 0.0, 0.0]           0   [0.81, 0.451, 0.108]   \n",
       " 1      40  [0.0, 0.0, 1.0]           2  [0.184, 0.087, 0.775]   \n",
       " 2      40  [1.0, 0.0, 0.0]           0   [0.843, 0.65, 0.059]   \n",
       " 3      40  [0.0, 0.0, 1.0]           2  [0.317, 0.469, 0.561]   \n",
       " 4      40  [1.0, 0.0, 0.0]           0  [0.906, 0.681, 0.159]   \n",
       " 5      40  [1.0, 0.0, 0.0]           0  [0.967, 0.843, 0.073]   \n",
       " 6      40  [1.0, 0.0, 0.0]           0    [0.72, 0.09, 0.103]   \n",
       " 7      40  [1.0, 0.0, 0.0]           0  [0.624, 0.035, 0.031]   \n",
       " 8      40  [0.0, 0.0, 1.0]           2  [0.171, 0.196, 0.714]   \n",
       " 9      40  [0.0, 0.0, 1.0]           2   [0.327, 0.631, 0.61]   \n",
       " 10     40  [0.0, 0.0, 1.0]           2  [0.101, 0.212, 0.711]   \n",
       " 11     40  [0.0, 0.0, 1.0]           2  [0.091, 0.229, 0.676]   \n",
       " 12     40  [0.0, 0.0, 1.0]           2  [0.186, 0.287, 0.708]   \n",
       " 13     40  [1.0, 0.0, 0.0]           0  [0.751, 0.412, 0.081]   \n",
       " 14     40  [1.0, 0.0, 0.0]           0     [0.867, 0.58, 0.0]   \n",
       " 15     40  [0.0, 0.0, 1.0]           2  [0.231, 0.329, 0.664]   \n",
       " 16     40  [0.0, 0.0, 1.0]           2  [0.297, 0.506, 0.558]   \n",
       " 17     40  [1.0, 0.0, 0.0]           0  [0.862, 0.626, 0.201]   \n",
       " 18     40  [1.0, 0.0, 0.0]           0   [0.795, 0.42, 0.126]   \n",
       " 19     40  [1.0, 0.0, 0.0]           0   [0.77, 0.398, 0.185]   \n",
       " 20     40  [1.0, 0.0, 0.0]           0   [0.73, 0.248, 0.004]   \n",
       " 21     40  [1.0, 0.0, 0.0]           0  [0.874, 0.564, 0.145]   \n",
       " 22     40  [1.0, 0.0, 0.0]           0   [0.794, 0.32, 0.163]   \n",
       " 23     40  [1.0, 0.0, 0.0]           0     [0.929, 0.82, 0.1]   \n",
       " 24     40  [1.0, 0.0, 0.0]           0   [0.814, 0.48, 0.131]   \n",
       " 25     40  [1.0, 0.0, 0.0]           0  [0.814, 0.489, 0.093]   \n",
       " 26     40  [1.0, 0.0, 0.0]           0    [1.0, 0.993, 0.261]   \n",
       " 27     40  [1.0, 0.0, 0.0]           0  [0.749, 0.517, 0.119]   \n",
       " 28     40  [1.0, 0.0, 0.0]           0  [0.762, 0.333, 0.101]   \n",
       " 29     40  [1.0, 0.0, 0.0]           0  [0.842, 0.539, 0.133]   \n",
       " 30     40  [0.0, 1.0, 0.0]           2   [0.443, 0.58, 0.521]   \n",
       " 31     40  [0.0, 1.0, 0.0]           2  [0.295, 0.274, 0.531]   \n",
       " 32     40  [0.0, 1.0, 0.0]           1  [0.689, 0.917, 0.417]   \n",
       " 33     40  [0.0, 1.0, 0.0]           2  [0.367, 0.262, 0.642]   \n",
       " 34     40  [0.0, 1.0, 0.0]           2    [0.715, 1.0, 0.524]   \n",
       " 35     40  [0.0, 1.0, 0.0]           2  [0.343, 0.182, 0.613]   \n",
       " 36     40  [0.0, 1.0, 0.0]           2  [0.402, 0.572, 0.473]   \n",
       " 37     40  [0.0, 1.0, 0.0]           2  [0.299, 0.446, 0.504]   \n",
       " 38     40  [0.0, 1.0, 0.0]           2  [0.405, 0.322, 0.599]   \n",
       " 39     40  [0.0, 1.0, 0.0]           2  [0.389, 0.469, 0.555]   \n",
       " 40     40  [0.0, 1.0, 0.0]           2  [0.594, 0.598, 0.507]   \n",
       " 41     40  [0.0, 1.0, 0.0]           2  [0.592, 0.732, 0.535]   \n",
       " 42     40  [0.0, 1.0, 0.0]           2  [0.515, 0.558, 0.527]   \n",
       " 43     40  [0.0, 1.0, 0.0]           2  [0.349, 0.541, 0.626]   \n",
       " 44     40  [0.0, 1.0, 0.0]           2   [0.318, 0.34, 0.456]   \n",
       " 45     40  [0.0, 1.0, 0.0]           2   [0.306, 0.193, 0.63]   \n",
       " 46     40  [0.0, 1.0, 0.0]           2  [0.501, 0.693, 0.522]   \n",
       " 47     40  [0.0, 1.0, 0.0]           2  [0.374, 0.403, 0.549]   \n",
       " 48     40  [0.0, 1.0, 0.0]           2  [0.489, 0.636, 0.514]   \n",
       " 49     40  [0.0, 1.0, 0.0]           2  [0.458, 0.547, 0.487]   \n",
       " 50     40  [0.0, 1.0, 0.0]           2  [0.679, 0.825, 0.419]   \n",
       " 51     40  [0.0, 0.0, 1.0]           2  [0.242, 0.411, 0.661]   \n",
       " 52     40  [0.0, 0.0, 1.0]           2  [0.127, 0.104, 0.839]   \n",
       " 53     40  [0.0, 0.0, 1.0]           2  [0.248, 0.395, 0.811]   \n",
       " 54     40  [0.0, 0.0, 1.0]           2      [0.0, 0.0, 0.707]   \n",
       " 55     40  [0.0, 0.0, 1.0]           2  [0.228, 0.318, 0.667]   \n",
       " 56     40  [0.0, 0.0, 1.0]           2    [0.037, 0.104, 1.0]   \n",
       " 57     40  [0.0, 0.0, 1.0]           2    [0.43, 0.672, 0.72]   \n",
       " 58     40  [0.0, 0.0, 1.0]           2  [0.325, 0.456, 0.673]   \n",
       " 59     40  [0.0, 0.0, 1.0]           2  [0.133, 0.199, 0.658]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0      50  [1.0, 0.0, 0.0]           0  [0.845, 0.414, 0.104]   \n",
       " 1      50  [0.0, 0.0, 1.0]           2  [0.176, 0.122, 0.773]   \n",
       " 2      50  [1.0, 0.0, 0.0]           0  [0.875, 0.608, 0.058]   \n",
       " 3      50  [0.0, 0.0, 1.0]           2   [0.31, 0.487, 0.566]   \n",
       " 4      50  [1.0, 0.0, 0.0]           0  [0.928, 0.644, 0.153]   \n",
       " 5      50  [1.0, 0.0, 0.0]           0   [0.99, 0.797, 0.071]   \n",
       " 6      50  [1.0, 0.0, 0.0]           0  [0.767, 0.057, 0.097]   \n",
       " 7      50  [1.0, 0.0, 0.0]           0    [0.679, 0.0, 0.029]   \n",
       " 8      50  [0.0, 0.0, 1.0]           2   [0.161, 0.23, 0.718]   \n",
       " 9      50  [0.0, 0.0, 1.0]           2  [0.311, 0.653, 0.616]   \n",
       " 10     50  [0.0, 0.0, 1.0]           2  [0.093, 0.248, 0.717]   \n",
       " 11     50  [0.0, 0.0, 1.0]           2  [0.083, 0.264, 0.685]   \n",
       " 12     50  [0.0, 0.0, 1.0]           2  [0.173, 0.321, 0.714]   \n",
       " 13     50  [1.0, 0.0, 0.0]           0   [0.789, 0.376, 0.08]   \n",
       " 14     50  [1.0, 0.0, 0.0]           0    [0.907, 0.533, 0.0]   \n",
       " 15     50  [0.0, 0.0, 1.0]           2   [0.22, 0.357, 0.668]   \n",
       " 16     50  [0.0, 0.0, 1.0]           2   [0.29, 0.525, 0.564]   \n",
       " 17     50  [1.0, 0.0, 0.0]           0  [0.882, 0.594, 0.195]   \n",
       " 18     50  [1.0, 0.0, 0.0]           0  [0.829, 0.385, 0.122]   \n",
       " 19     50  [1.0, 0.0, 0.0]           0   [0.798, 0.368, 0.18]   \n",
       " 20     50  [1.0, 0.0, 0.0]           0  [0.783, 0.207, 0.002]   \n",
       " 21     50  [1.0, 0.0, 0.0]           0     [0.9, 0.527, 0.14]   \n",
       " 22     50  [1.0, 0.0, 0.0]           0  [0.827, 0.288, 0.156]   \n",
       " 23     50  [1.0, 0.0, 0.0]           0  [0.951, 0.778, 0.098]   \n",
       " 24     50  [1.0, 0.0, 0.0]           0  [0.845, 0.444, 0.127]   \n",
       " 25     50  [1.0, 0.0, 0.0]           0  [0.848, 0.451, 0.091]   \n",
       " 26     50  [1.0, 0.0, 0.0]           0    [1.0, 0.962, 0.255]   \n",
       " 27     50  [1.0, 0.0, 0.0]           0  [0.779, 0.484, 0.119]   \n",
       " 28     50  [1.0, 0.0, 0.0]           0  [0.802, 0.297, 0.097]   \n",
       " 29     50  [1.0, 0.0, 0.0]           0  [0.871, 0.503, 0.128]   \n",
       " 30     50  [0.0, 1.0, 0.0]           2  [0.437, 0.589, 0.522]   \n",
       " 31     50  [0.0, 1.0, 0.0]           2  [0.297, 0.289, 0.534]   \n",
       " 32     50  [0.0, 1.0, 0.0]           1   [0.678, 0.91, 0.417]   \n",
       " 33     50  [0.0, 1.0, 0.0]           2  [0.361, 0.282, 0.639]   \n",
       " 34     50  [0.0, 1.0, 0.0]           2    [0.692, 1.0, 0.523]   \n",
       " 35     50  [0.0, 1.0, 0.0]           2     [0.342, 0.2, 0.61]   \n",
       " 36     50  [0.0, 1.0, 0.0]           2   [0.399, 0.58, 0.477]   \n",
       " 37     50  [0.0, 1.0, 0.0]           2   [0.298, 0.461, 0.51]   \n",
       " 38     50  [0.0, 1.0, 0.0]           2  [0.401, 0.337, 0.597]   \n",
       " 39     50  [0.0, 1.0, 0.0]           2  [0.383, 0.483, 0.557]   \n",
       " 40     50  [0.0, 1.0, 0.0]           2    [0.587, 0.6, 0.504]   \n",
       " 41     50  [0.0, 1.0, 0.0]           2  [0.578, 0.737, 0.533]   \n",
       " 42     50  [0.0, 1.0, 0.0]           2  [0.507, 0.565, 0.526]   \n",
       " 43     50  [0.0, 1.0, 0.0]           2  [0.335, 0.563, 0.629]   \n",
       " 44     50  [0.0, 1.0, 0.0]           2   [0.324, 0.349, 0.46]   \n",
       " 45     50  [0.0, 1.0, 0.0]           2  [0.303, 0.214, 0.629]   \n",
       " 46     50  [0.0, 1.0, 0.0]           2    [0.491, 0.7, 0.524]   \n",
       " 47     50  [0.0, 1.0, 0.0]           2   [0.371, 0.417, 0.55]   \n",
       " 48     50  [0.0, 1.0, 0.0]           2   [0.48, 0.644, 0.515]   \n",
       " 49     50  [0.0, 1.0, 0.0]           2  [0.455, 0.554, 0.489]   \n",
       " 50     50  [0.0, 1.0, 0.0]           2   [0.67, 0.818, 0.419]   \n",
       " 51     50  [0.0, 0.0, 1.0]           2  [0.231, 0.439, 0.665]   \n",
       " 52     50  [0.0, 0.0, 1.0]           2  [0.114, 0.147, 0.839]   \n",
       " 53     50  [0.0, 0.0, 1.0]           2  [0.225, 0.433, 0.811]   \n",
       " 54     50  [0.0, 0.0, 1.0]           2    [0.0, 0.039, 0.715]   \n",
       " 55     50  [0.0, 0.0, 1.0]           2   [0.219, 0.346, 0.67]   \n",
       " 56     50  [0.0, 0.0, 1.0]           2    [0.013, 0.163, 1.0]   \n",
       " 57     50  [0.0, 0.0, 1.0]           2  [0.403, 0.698, 0.721]   \n",
       " 58     50  [0.0, 0.0, 1.0]           2  [0.309, 0.482, 0.676]   \n",
       " 59     50  [0.0, 0.0, 1.0]           2   [0.129, 0.23, 0.663]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0      60  [1.0, 0.0, 0.0]           0      [0.866, 0.4, 0.1]   \n",
       " 1      60  [0.0, 0.0, 1.0]           2  [0.173, 0.179, 0.772]   \n",
       " 2      60  [1.0, 0.0, 0.0]           0  [0.893, 0.584, 0.058]   \n",
       " 3      60  [0.0, 0.0, 1.0]           2  [0.305, 0.517, 0.571]   \n",
       " 4      60  [1.0, 0.0, 0.0]           0  [0.937, 0.624, 0.148]   \n",
       " 5      60  [1.0, 0.0, 0.0]           0    [1.0, 0.764, 0.069]   \n",
       " 6      60  [1.0, 0.0, 0.0]           0  [0.801, 0.057, 0.092]   \n",
       " 7      60  [1.0, 0.0, 0.0]           0    [0.721, 0.0, 0.029]   \n",
       " 8      60  [0.0, 0.0, 1.0]           2  [0.156, 0.281, 0.723]   \n",
       " 9      60  [0.0, 0.0, 1.0]           2  [0.299, 0.681, 0.622]   \n",
       " 10     60  [0.0, 0.0, 1.0]           2  [0.091, 0.301, 0.724]   \n",
       " 11     60  [0.0, 0.0, 1.0]           2  [0.083, 0.315, 0.694]   \n",
       " 12     60  [0.0, 0.0, 1.0]           2  [0.166, 0.369, 0.719]   \n",
       " 13     60  [1.0, 0.0, 0.0]           0  [0.813, 0.363, 0.079]   \n",
       " 14     60  [1.0, 0.0, 0.0]           0    [0.931, 0.506, 0.0]   \n",
       " 15     60  [0.0, 0.0, 1.0]           2    [0.215, 0.4, 0.672]   \n",
       " 16     60  [0.0, 0.0, 1.0]           2   [0.285, 0.554, 0.57]   \n",
       " 17     60  [1.0, 0.0, 0.0]           0    [0.89, 0.579, 0.19]   \n",
       " 18     60  [1.0, 0.0, 0.0]           0   [0.85, 0.373, 0.118]   \n",
       " 19     60  [1.0, 0.0, 0.0]           0  [0.815, 0.363, 0.176]   \n",
       " 20     60  [1.0, 0.0, 0.0]           0   [0.82, 0.194, 0.002]   \n",
       " 21     60  [1.0, 0.0, 0.0]           0  [0.914, 0.511, 0.135]   \n",
       " 22     60  [1.0, 0.0, 0.0]           0   [0.848, 0.282, 0.15]   \n",
       " 23     60  [1.0, 0.0, 0.0]           0   [0.96, 0.749, 0.096]   \n",
       " 24     60  [1.0, 0.0, 0.0]           0  [0.864, 0.431, 0.123]   \n",
       " 25     60  [1.0, 0.0, 0.0]           0  [0.868, 0.435, 0.088]   \n",
       " 26     60  [1.0, 0.0, 0.0]           0    [0.99, 0.936, 0.25]   \n",
       " 27     60  [1.0, 0.0, 0.0]           0  [0.797, 0.471, 0.118]   \n",
       " 28     60  [1.0, 0.0, 0.0]           0  [0.828, 0.288, 0.094]   \n",
       " 29     60  [1.0, 0.0, 0.0]           0  [0.887, 0.487, 0.124]   \n",
       " 30     60  [0.0, 1.0, 0.0]           2  [0.431, 0.609, 0.524]   \n",
       " 31     60  [0.0, 1.0, 0.0]           2  [0.301, 0.322, 0.537]   \n",
       " 32     60  [0.0, 1.0, 0.0]           1  [0.665, 0.906, 0.417]   \n",
       " 33     60  [0.0, 1.0, 0.0]           2  [0.357, 0.321, 0.637]   \n",
       " 34     60  [0.0, 1.0, 0.0]           2    [0.668, 1.0, 0.521]   \n",
       " 35     60  [0.0, 1.0, 0.0]           2  [0.342, 0.239, 0.609]   \n",
       " 36     60  [0.0, 1.0, 0.0]           2  [0.396, 0.599, 0.482]   \n",
       " 37     60  [0.0, 1.0, 0.0]           2  [0.298, 0.488, 0.517]   \n",
       " 38     60  [0.0, 1.0, 0.0]           2  [0.397, 0.371, 0.595]   \n",
       " 39     60  [0.0, 1.0, 0.0]           2   [0.378, 0.51, 0.559]   \n",
       " 40     60  [0.0, 1.0, 0.0]           2  [0.578, 0.614, 0.501]   \n",
       " 41     60  [0.0, 1.0, 0.0]           2  [0.564, 0.749, 0.532]   \n",
       " 42     60  [0.0, 1.0, 0.0]           2    [0.5, 0.584, 0.525]   \n",
       " 43     60  [0.0, 1.0, 0.0]           2  [0.325, 0.594, 0.632]   \n",
       " 44     60  [0.0, 1.0, 0.0]           2  [0.331, 0.375, 0.465]   \n",
       " 45     60  [0.0, 1.0, 0.0]           2  [0.302, 0.256, 0.628]   \n",
       " 46     60  [0.0, 1.0, 0.0]           2   [0.48, 0.715, 0.524]   \n",
       " 47     60  [0.0, 1.0, 0.0]           2  [0.369, 0.446, 0.552]   \n",
       " 48     60  [0.0, 1.0, 0.0]           2   [0.471, 0.66, 0.516]   \n",
       " 49     60  [0.0, 1.0, 0.0]           2   [0.451, 0.572, 0.49]   \n",
       " 50     60  [0.0, 1.0, 0.0]           1  [0.659, 0.818, 0.419]   \n",
       " 51     60  [0.0, 0.0, 1.0]           2  [0.225, 0.479, 0.669]   \n",
       " 52     60  [0.0, 0.0, 1.0]           2  [0.108, 0.209, 0.838]   \n",
       " 53     60  [0.0, 0.0, 1.0]           2  [0.209, 0.483, 0.812]   \n",
       " 54     60  [0.0, 0.0, 1.0]           2    [0.007, 0.1, 0.723]   \n",
       " 55     60  [0.0, 0.0, 1.0]           2  [0.215, 0.389, 0.673]   \n",
       " 56     60  [0.0, 0.0, 1.0]           2       [0.0, 0.24, 1.0]   \n",
       " 57     60  [0.0, 0.0, 1.0]           2   [0.381, 0.729, 0.72]   \n",
       " 58     60  [0.0, 0.0, 1.0]           2  [0.298, 0.519, 0.678]   \n",
       " 59     60  [0.0, 0.0, 1.0]           2  [0.131, 0.278, 0.669]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0      70  [1.0, 0.0, 0.0]           0  [0.878, 0.388, 0.097]   \n",
       " 1      70  [0.0, 0.0, 1.0]           2  [0.177, 0.228, 0.771]   \n",
       " 2      70  [1.0, 0.0, 0.0]           0  [0.902, 0.563, 0.058]   \n",
       " 3      70  [0.0, 0.0, 1.0]           2  [0.305, 0.542, 0.575]   \n",
       " 4      70  [1.0, 0.0, 0.0]           0  [0.938, 0.607, 0.143]   \n",
       " 5      70  [1.0, 0.0, 0.0]           0    [1.0, 0.736, 0.066]   \n",
       " 6      70  [1.0, 0.0, 0.0]           0  [0.823, 0.057, 0.088]   \n",
       " 7      70  [1.0, 0.0, 0.0]           0    [0.752, 0.0, 0.029]   \n",
       " 8      70  [0.0, 0.0, 1.0]           2  [0.159, 0.325, 0.727]   \n",
       " 9      70  [0.0, 0.0, 1.0]           2  [0.293, 0.705, 0.627]   \n",
       " 10     70  [0.0, 0.0, 1.0]           2   [0.098, 0.346, 0.73]   \n",
       " 11     70  [0.0, 0.0, 1.0]           2  [0.091, 0.358, 0.702]   \n",
       " 12     70  [0.0, 0.0, 1.0]           2  [0.167, 0.411, 0.724]   \n",
       " 13     70  [1.0, 0.0, 0.0]           0  [0.829, 0.352, 0.078]   \n",
       " 14     70  [1.0, 0.0, 0.0]           0    [0.944, 0.483, 0.0]   \n",
       " 15     70  [0.0, 0.0, 1.0]           2  [0.216, 0.436, 0.676]   \n",
       " 16     70  [0.0, 0.0, 1.0]           2  [0.286, 0.579, 0.575]   \n",
       " 17     70  [1.0, 0.0, 0.0]           0  [0.891, 0.567, 0.185]   \n",
       " 18     70  [1.0, 0.0, 0.0]           0  [0.861, 0.364, 0.114]   \n",
       " 19     70  [1.0, 0.0, 0.0]           0  [0.824, 0.358, 0.172]   \n",
       " 20     70  [1.0, 0.0, 0.0]           0  [0.845, 0.184, 0.001]   \n",
       " 21     70  [1.0, 0.0, 0.0]           0   [0.919, 0.496, 0.13]   \n",
       " 22     70  [1.0, 0.0, 0.0]           0   [0.86, 0.278, 0.145]   \n",
       " 23     70  [1.0, 0.0, 0.0]           0   [0.96, 0.724, 0.094]   \n",
       " 24     70  [1.0, 0.0, 0.0]           0    [0.873, 0.42, 0.12]   \n",
       " 25     70  [1.0, 0.0, 0.0]           0  [0.879, 0.422, 0.086]   \n",
       " 26     70  [1.0, 0.0, 0.0]           0  [0.975, 0.915, 0.244]   \n",
       " 27     70  [1.0, 0.0, 0.0]           0  [0.807, 0.459, 0.118]   \n",
       " 28     70  [1.0, 0.0, 0.0]           0   [0.844, 0.28, 0.091]   \n",
       " 29     70  [1.0, 0.0, 0.0]           0  [0.895, 0.473, 0.121]   \n",
       " 30     70  [0.0, 1.0, 0.0]           2  [0.427, 0.626, 0.525]   \n",
       " 31     70  [0.0, 1.0, 0.0]           2   [0.308, 0.351, 0.54]   \n",
       " 32     70  [0.0, 1.0, 0.0]           1  [0.652, 0.903, 0.417]   \n",
       " 33     70  [0.0, 1.0, 0.0]           2  [0.357, 0.354, 0.635]   \n",
       " 34     70  [0.0, 1.0, 0.0]           1    [0.647, 1.0, 0.518]   \n",
       " 35     70  [0.0, 1.0, 0.0]           2  [0.345, 0.273, 0.607]   \n",
       " 36     70  [0.0, 1.0, 0.0]           2  [0.397, 0.614, 0.485]   \n",
       " 37     70  [0.0, 1.0, 0.0]           2  [0.302, 0.511, 0.522]   \n",
       " 38     70  [0.0, 1.0, 0.0]           2  [0.396, 0.399, 0.594]   \n",
       " 39     70  [0.0, 1.0, 0.0]           2  [0.376, 0.533, 0.561]   \n",
       " 40     70  [0.0, 1.0, 0.0]           2   [0.57, 0.626, 0.498]   \n",
       " 41     70  [0.0, 1.0, 0.0]           2    [0.552, 0.76, 0.53]   \n",
       " 42     70  [0.0, 1.0, 0.0]           2    [0.494, 0.6, 0.524]   \n",
       " 43     70  [0.0, 1.0, 0.0]           2    [0.32, 0.62, 0.634]   \n",
       " 44     70  [0.0, 1.0, 0.0]           2  [0.339, 0.397, 0.469]   \n",
       " 45     70  [0.0, 1.0, 0.0]           2  [0.306, 0.291, 0.628]   \n",
       " 46     70  [0.0, 1.0, 0.0]           2  [0.473, 0.728, 0.525]   \n",
       " 47     70  [0.0, 1.0, 0.0]           2    [0.37, 0.47, 0.553]   \n",
       " 48     70  [0.0, 1.0, 0.0]           2  [0.466, 0.675, 0.517]   \n",
       " 49     70  [0.0, 1.0, 0.0]           2  [0.449, 0.588, 0.491]   \n",
       " 50     70  [0.0, 1.0, 0.0]           1  [0.647, 0.818, 0.418]   \n",
       " 51     70  [0.0, 0.0, 1.0]           2  [0.225, 0.513, 0.673]   \n",
       " 52     70  [0.0, 0.0, 1.0]           2  [0.111, 0.263, 0.838]   \n",
       " 53     70  [0.0, 0.0, 1.0]           2  [0.203, 0.526, 0.812]   \n",
       " 54     70  [0.0, 0.0, 1.0]           2   [0.022, 0.152, 0.73]   \n",
       " 55     70  [0.0, 0.0, 1.0]           2  [0.217, 0.426, 0.676]   \n",
       " 56     70  [0.0, 0.0, 1.0]           2      [0.0, 0.305, 1.0]   \n",
       " 57     70  [0.0, 0.0, 1.0]           2   [0.366, 0.755, 0.72]   \n",
       " 58     70  [0.0, 0.0, 1.0]           2    [0.292, 0.55, 0.68]   \n",
       " 59     70  [0.0, 0.0, 1.0]           2   [0.139, 0.32, 0.675]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0      80  [1.0, 0.0, 0.0]           0  [0.887, 0.378, 0.094]   \n",
       " 1      80  [0.0, 0.0, 1.0]           2   [0.179, 0.27, 0.771]   \n",
       " 2      80  [1.0, 0.0, 0.0]           0  [0.909, 0.545, 0.057]   \n",
       " 3      80  [0.0, 0.0, 1.0]           2  [0.305, 0.564, 0.579]   \n",
       " 4      80  [1.0, 0.0, 0.0]           0  [0.939, 0.592, 0.139]   \n",
       " 5      80  [1.0, 0.0, 0.0]           0    [1.0, 0.712, 0.064]   \n",
       " 6      80  [1.0, 0.0, 0.0]           0  [0.842, 0.057, 0.084]   \n",
       " 7      80  [1.0, 0.0, 0.0]           0    [0.777, 0.0, 0.029]   \n",
       " 8      80  [0.0, 0.0, 1.0]           2  [0.161, 0.363, 0.732]   \n",
       " 9      80  [0.0, 0.0, 1.0]           2  [0.287, 0.725, 0.631]   \n",
       " 10     80  [0.0, 0.0, 1.0]           2  [0.103, 0.385, 0.736]   \n",
       " 11     80  [0.0, 0.0, 1.0]           2  [0.097, 0.395, 0.709]   \n",
       " 12     80  [0.0, 0.0, 1.0]           2  [0.167, 0.446, 0.729]   \n",
       " 13     80  [1.0, 0.0, 0.0]           0  [0.841, 0.343, 0.077]   \n",
       " 14     80  [1.0, 0.0, 0.0]           0    [0.955, 0.463, 0.0]   \n",
       " 15     80  [0.0, 0.0, 1.0]           2   [0.216, 0.468, 0.68]   \n",
       " 16     80  [0.0, 0.0, 1.0]           2     [0.286, 0.6, 0.58]   \n",
       " 17     80  [1.0, 0.0, 0.0]           0   [0.892, 0.556, 0.18]   \n",
       " 18     80  [1.0, 0.0, 0.0]           0  [0.871, 0.356, 0.111]   \n",
       " 19     80  [1.0, 0.0, 0.0]           0  [0.831, 0.353, 0.168]   \n",
       " 20     80  [1.0, 0.0, 0.0]           0  [0.866, 0.175, 0.001]   \n",
       " 21     80  [1.0, 0.0, 0.0]           0  [0.923, 0.484, 0.127]   \n",
       " 22     80  [1.0, 0.0, 0.0]           0    [0.87, 0.273, 0.14]   \n",
       " 23     80  [1.0, 0.0, 0.0]           0   [0.96, 0.703, 0.092]   \n",
       " 24     80  [1.0, 0.0, 0.0]           0   [0.881, 0.41, 0.116]   \n",
       " 25     80  [1.0, 0.0, 0.0]           0   [0.888, 0.41, 0.084]   \n",
       " 26     80  [1.0, 0.0, 0.0]           0  [0.962, 0.896, 0.238]   \n",
       " 27     80  [1.0, 0.0, 0.0]           0  [0.816, 0.449, 0.118]   \n",
       " 28     80  [1.0, 0.0, 0.0]           0  [0.858, 0.273, 0.088]   \n",
       " 29     80  [1.0, 0.0, 0.0]           0  [0.901, 0.461, 0.117]   \n",
       " 30     80  [0.0, 1.0, 0.0]           2  [0.424, 0.641, 0.526]   \n",
       " 31     80  [0.0, 1.0, 0.0]           2  [0.313, 0.376, 0.542]   \n",
       " 32     80  [0.0, 1.0, 0.0]           1    [0.641, 0.9, 0.416]   \n",
       " 33     80  [0.0, 1.0, 0.0]           2  [0.356, 0.382, 0.633]   \n",
       " 34     80  [0.0, 1.0, 0.0]           1    [0.629, 1.0, 0.516]   \n",
       " 35     80  [0.0, 1.0, 0.0]           2  [0.347, 0.303, 0.606]   \n",
       " 36     80  [0.0, 1.0, 0.0]           2  [0.396, 0.628, 0.489]   \n",
       " 37     80  [0.0, 1.0, 0.0]           2  [0.305, 0.531, 0.527]   \n",
       " 38     80  [0.0, 1.0, 0.0]           2  [0.394, 0.424, 0.592]   \n",
       " 39     80  [0.0, 1.0, 0.0]           2  [0.374, 0.553, 0.562]   \n",
       " 40     80  [0.0, 1.0, 0.0]           2  [0.563, 0.636, 0.495]   \n",
       " 41     80  [0.0, 1.0, 0.0]           2  [0.541, 0.769, 0.528]   \n",
       " 42     80  [0.0, 1.0, 0.0]           2  [0.488, 0.614, 0.523]   \n",
       " 43     80  [0.0, 1.0, 0.0]           2  [0.315, 0.643, 0.636]   \n",
       " 44     80  [0.0, 1.0, 0.0]           2  [0.345, 0.417, 0.473]   \n",
       " 45     80  [0.0, 1.0, 0.0]           2  [0.308, 0.322, 0.628]   \n",
       " 46     80  [0.0, 1.0, 0.0]           2  [0.466, 0.739, 0.525]   \n",
       " 47     80  [0.0, 1.0, 0.0]           2   [0.37, 0.491, 0.554]   \n",
       " 48     80  [0.0, 1.0, 0.0]           2   [0.46, 0.687, 0.518]   \n",
       " 49     80  [0.0, 1.0, 0.0]           2  [0.447, 0.601, 0.493]   \n",
       " 50     80  [0.0, 1.0, 0.0]           1  [0.638, 0.817, 0.417]   \n",
       " 51     80  [0.0, 0.0, 1.0]           2  [0.224, 0.542, 0.676]   \n",
       " 52     80  [0.0, 0.0, 1.0]           2  [0.113, 0.309, 0.839]   \n",
       " 53     80  [0.0, 0.0, 1.0]           2  [0.196, 0.563, 0.813]   \n",
       " 54     80  [0.0, 0.0, 1.0]           2  [0.034, 0.197, 0.737]   \n",
       " 55     80  [0.0, 0.0, 1.0]           2  [0.218, 0.458, 0.679]   \n",
       " 56     80  [0.0, 0.0, 1.0]           2      [0.0, 0.363, 1.0]   \n",
       " 57     80  [0.0, 0.0, 1.0]           2  [0.353, 0.778, 0.719]   \n",
       " 58     80  [0.0, 0.0, 1.0]           2  [0.287, 0.578, 0.682]   \n",
       " 59     80  [0.0, 0.0, 1.0]           2   [0.145, 0.356, 0.68]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0      90  [1.0, 0.0, 0.0]           0  [0.895, 0.369, 0.092]   \n",
       " 1      90  [0.0, 0.0, 1.0]           2    [0.18, 0.307, 0.77]   \n",
       " 2      90  [1.0, 0.0, 0.0]           0  [0.915, 0.529, 0.057]   \n",
       " 3      90  [0.0, 0.0, 1.0]           2  [0.303, 0.582, 0.583]   \n",
       " 4      90  [1.0, 0.0, 0.0]           0  [0.939, 0.579, 0.135]   \n",
       " 5      90  [1.0, 0.0, 0.0]           0    [1.0, 0.691, 0.062]   \n",
       " 6      90  [1.0, 0.0, 0.0]           0  [0.857, 0.057, 0.081]   \n",
       " 7      90  [1.0, 0.0, 0.0]           0    [0.798, 0.0, 0.029]   \n",
       " 8      90  [0.0, 0.0, 1.0]           2  [0.161, 0.396, 0.736]   \n",
       " 9      90  [0.0, 0.0, 1.0]           2  [0.282, 0.743, 0.636]   \n",
       " 10     90  [0.0, 0.0, 1.0]           2  [0.106, 0.419, 0.741]   \n",
       " 11     90  [0.0, 0.0, 1.0]           2  [0.101, 0.427, 0.716]   \n",
       " 12     90  [0.0, 0.0, 1.0]           2  [0.166, 0.477, 0.733]   \n",
       " 13     90  [1.0, 0.0, 0.0]           0  [0.852, 0.334, 0.076]   \n",
       " 14     90  [1.0, 0.0, 0.0]           0    [0.963, 0.446, 0.0]   \n",
       " 15     90  [0.0, 0.0, 1.0]           2  [0.215, 0.495, 0.684]   \n",
       " 16     90  [0.0, 0.0, 1.0]           2  [0.285, 0.618, 0.584]   \n",
       " 17     90  [1.0, 0.0, 0.0]           0  [0.893, 0.547, 0.176]   \n",
       " 18     90  [1.0, 0.0, 0.0]           0  [0.879, 0.348, 0.108]   \n",
       " 19     90  [1.0, 0.0, 0.0]           0   [0.837, 0.35, 0.165]   \n",
       " 20     90  [1.0, 0.0, 0.0]           0    [0.883, 0.167, 0.0]   \n",
       " 21     90  [1.0, 0.0, 0.0]           0  [0.926, 0.474, 0.123]   \n",
       " 22     90  [1.0, 0.0, 0.0]           0   [0.877, 0.27, 0.136]   \n",
       " 23     90  [1.0, 0.0, 0.0]           0   [0.96, 0.684, 0.089]   \n",
       " 24     90  [1.0, 0.0, 0.0]           0  [0.888, 0.401, 0.113]   \n",
       " 25     90  [1.0, 0.0, 0.0]           0  [0.896, 0.399, 0.083]   \n",
       " 26     90  [1.0, 0.0, 0.0]           0   [0.952, 0.88, 0.233]   \n",
       " 27     90  [1.0, 0.0, 0.0]           0   [0.823, 0.44, 0.118]   \n",
       " 28     90  [1.0, 0.0, 0.0]           0  [0.868, 0.267, 0.086]   \n",
       " 29     90  [1.0, 0.0, 0.0]           0  [0.905, 0.451, 0.114]   \n",
       " 30     90  [0.0, 1.0, 0.0]           2   [0.42, 0.653, 0.527]   \n",
       " 31     90  [0.0, 1.0, 0.0]           2  [0.316, 0.397, 0.545]   \n",
       " 32     90  [0.0, 1.0, 0.0]           1  [0.632, 0.897, 0.415]   \n",
       " 33     90  [0.0, 1.0, 0.0]           2  [0.354, 0.407, 0.632]   \n",
       " 34     90  [0.0, 1.0, 0.0]           1    [0.614, 1.0, 0.514]   \n",
       " 35     90  [0.0, 1.0, 0.0]           2  [0.347, 0.328, 0.605]   \n",
       " 36     90  [0.0, 1.0, 0.0]           2  [0.395, 0.639, 0.492]   \n",
       " 37     90  [0.0, 1.0, 0.0]           2  [0.306, 0.547, 0.532]   \n",
       " 38     90  [0.0, 1.0, 0.0]           2  [0.392, 0.445, 0.591]   \n",
       " 39     90  [0.0, 1.0, 0.0]           2   [0.372, 0.57, 0.564]   \n",
       " 40     90  [0.0, 1.0, 0.0]           2  [0.557, 0.646, 0.493]   \n",
       " 41     90  [0.0, 1.0, 0.0]           2  [0.532, 0.777, 0.527]   \n",
       " 42     90  [0.0, 1.0, 0.0]           2  [0.483, 0.626, 0.522]   \n",
       " 43     90  [0.0, 1.0, 0.0]           2  [0.311, 0.662, 0.638]   \n",
       " 44     90  [0.0, 1.0, 0.0]           2  [0.349, 0.433, 0.477]   \n",
       " 45     90  [0.0, 1.0, 0.0]           2  [0.309, 0.349, 0.628]   \n",
       " 46     90  [0.0, 1.0, 0.0]           2   [0.46, 0.749, 0.526]   \n",
       " 47     90  [0.0, 1.0, 0.0]           2  [0.369, 0.509, 0.555]   \n",
       " 48     90  [0.0, 1.0, 0.0]           2  [0.455, 0.697, 0.518]   \n",
       " 49     90  [0.0, 1.0, 0.0]           2  [0.445, 0.613, 0.494]   \n",
       " 50     90  [0.0, 1.0, 0.0]           1  [0.629, 0.817, 0.417]   \n",
       " 51     90  [0.0, 0.0, 1.0]           2  [0.223, 0.567, 0.679]   \n",
       " 52     90  [0.0, 0.0, 1.0]           2   [0.114, 0.35, 0.839]   \n",
       " 53     90  [0.0, 0.0, 1.0]           2  [0.191, 0.595, 0.813]   \n",
       " 54     90  [0.0, 0.0, 1.0]           2  [0.042, 0.235, 0.744]   \n",
       " 55     90  [0.0, 0.0, 1.0]           2  [0.218, 0.485, 0.682]   \n",
       " 56     90  [0.0, 0.0, 1.0]           2      [0.0, 0.413, 1.0]   \n",
       " 57     90  [0.0, 0.0, 1.0]           2  [0.341, 0.798, 0.719]   \n",
       " 58     90  [0.0, 0.0, 1.0]           2  [0.282, 0.602, 0.684]   \n",
       " 59     90  [0.0, 0.0, 1.0]           2  [0.149, 0.387, 0.685]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     100  [1.0, 0.0, 0.0]           0   [0.902, 0.361, 0.09]   \n",
       " 1     100  [0.0, 0.0, 1.0]           2    [0.181, 0.34, 0.77]   \n",
       " 2     100  [1.0, 0.0, 0.0]           0   [0.92, 0.515, 0.056]   \n",
       " 3     100  [0.0, 0.0, 1.0]           2  [0.302, 0.599, 0.586]   \n",
       " 4     100  [1.0, 0.0, 0.0]           0   [0.94, 0.567, 0.131]   \n",
       " 5     100  [1.0, 0.0, 0.0]           0     [1.0, 0.672, 0.06]   \n",
       " 6     100  [1.0, 0.0, 0.0]           0   [0.87, 0.058, 0.078]   \n",
       " 7     100  [1.0, 0.0, 0.0]           0    [0.815, 0.0, 0.029]   \n",
       " 8     100  [0.0, 0.0, 1.0]           2  [0.161, 0.425, 0.739]   \n",
       " 9     100  [0.0, 0.0, 1.0]           2  [0.277, 0.758, 0.639]   \n",
       " 10    100  [0.0, 0.0, 1.0]           2  [0.108, 0.449, 0.746]   \n",
       " 11    100  [0.0, 0.0, 1.0]           2  [0.103, 0.455, 0.723]   \n",
       " 12    100  [0.0, 0.0, 1.0]           2  [0.165, 0.504, 0.737]   \n",
       " 13    100  [1.0, 0.0, 0.0]           0   [0.86, 0.327, 0.076]   \n",
       " 14    100  [1.0, 0.0, 0.0]           0      [0.97, 0.43, 0.0]   \n",
       " 15    100  [0.0, 0.0, 1.0]           2  [0.214, 0.519, 0.687]   \n",
       " 16    100  [0.0, 0.0, 1.0]           2  [0.283, 0.634, 0.588]   \n",
       " 17    100  [1.0, 0.0, 0.0]           0  [0.893, 0.539, 0.172]   \n",
       " 18    100  [1.0, 0.0, 0.0]           0  [0.886, 0.342, 0.106]   \n",
       " 19    100  [1.0, 0.0, 0.0]           0  [0.842, 0.346, 0.162]   \n",
       " 20    100  [1.0, 0.0, 0.0]           0    [0.897, 0.161, 0.0]   \n",
       " 21    100  [1.0, 0.0, 0.0]           0   [0.928, 0.464, 0.12]   \n",
       " 22    100  [1.0, 0.0, 0.0]           0  [0.884, 0.267, 0.132]   \n",
       " 23    100  [1.0, 0.0, 0.0]           0   [0.96, 0.667, 0.088]   \n",
       " 24    100  [1.0, 0.0, 0.0]           0  [0.893, 0.394, 0.111]   \n",
       " 25    100  [1.0, 0.0, 0.0]           0   [0.902, 0.39, 0.081]   \n",
       " 26    100  [1.0, 0.0, 0.0]           0  [0.943, 0.866, 0.228]   \n",
       " 27    100  [1.0, 0.0, 0.0]           0  [0.828, 0.432, 0.117]   \n",
       " 28    100  [1.0, 0.0, 0.0]           0  [0.878, 0.262, 0.084]   \n",
       " 29    100  [1.0, 0.0, 0.0]           0  [0.909, 0.442, 0.111]   \n",
       " 30    100  [0.0, 1.0, 0.0]           2  [0.417, 0.664, 0.528]   \n",
       " 31    100  [0.0, 1.0, 0.0]           2  [0.318, 0.416, 0.547]   \n",
       " 32    100  [0.0, 1.0, 0.0]           1  [0.624, 0.895, 0.415]   \n",
       " 33    100  [0.0, 1.0, 0.0]           2    [0.352, 0.43, 0.63]   \n",
       " 34    100  [0.0, 1.0, 0.0]           1    [0.601, 1.0, 0.512]   \n",
       " 35    100  [0.0, 1.0, 0.0]           2  [0.347, 0.351, 0.605]   \n",
       " 36    100  [0.0, 1.0, 0.0]           2  [0.393, 0.649, 0.494]   \n",
       " 37    100  [0.0, 1.0, 0.0]           2  [0.307, 0.562, 0.536]   \n",
       " 38    100  [0.0, 1.0, 0.0]           2    [0.39, 0.464, 0.59]   \n",
       " 39    100  [0.0, 1.0, 0.0]           2  [0.369, 0.585, 0.565]   \n",
       " 40    100  [0.0, 1.0, 0.0]           2  [0.551, 0.654, 0.491]   \n",
       " 41    100  [0.0, 1.0, 0.0]           2  [0.523, 0.783, 0.525]   \n",
       " 42    100  [0.0, 1.0, 0.0]           2  [0.478, 0.637, 0.522]   \n",
       " 43    100  [0.0, 1.0, 0.0]           2   [0.306, 0.679, 0.64]   \n",
       " 44    100  [0.0, 1.0, 0.0]           2   [0.352, 0.447, 0.48]   \n",
       " 45    100  [0.0, 1.0, 0.0]           2  [0.309, 0.373, 0.628]   \n",
       " 46    100  [0.0, 1.0, 0.0]           2  [0.454, 0.757, 0.526]   \n",
       " 47    100  [0.0, 1.0, 0.0]           2  [0.368, 0.526, 0.556]   \n",
       " 48    100  [0.0, 1.0, 0.0]           2   [0.45, 0.706, 0.519]   \n",
       " 49    100  [0.0, 1.0, 0.0]           2  [0.442, 0.623, 0.494]   \n",
       " 50    100  [0.0, 1.0, 0.0]           1  [0.622, 0.816, 0.416]   \n",
       " 51    100  [0.0, 0.0, 1.0]           2   [0.221, 0.59, 0.682]   \n",
       " 52    100  [0.0, 0.0, 1.0]           2  [0.115, 0.386, 0.839]   \n",
       " 53    100  [0.0, 0.0, 1.0]           2  [0.186, 0.624, 0.814]   \n",
       " 54    100  [0.0, 0.0, 1.0]           2    [0.049, 0.27, 0.75]   \n",
       " 55    100  [0.0, 0.0, 1.0]           2   [0.217, 0.51, 0.684]   \n",
       " 56    100  [0.0, 0.0, 1.0]           2      [0.0, 0.457, 1.0]   \n",
       " 57    100  [0.0, 0.0, 1.0]           2  [0.331, 0.816, 0.718]   \n",
       " 58    100  [0.0, 0.0, 1.0]           2  [0.278, 0.623, 0.685]   \n",
       " 59    100  [0.0, 0.0, 1.0]           2   [0.151, 0.414, 0.69]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     110  [1.0, 0.0, 0.0]           0  [0.907, 0.354, 0.087]   \n",
       " 1     110  [0.0, 0.0, 1.0]           2    [0.18, 0.369, 0.77]   \n",
       " 2     110  [1.0, 0.0, 0.0]           0  [0.924, 0.503, 0.056]   \n",
       " 3     110  [0.0, 0.0, 1.0]           2     [0.3, 0.613, 0.59]   \n",
       " 4     110  [1.0, 0.0, 0.0]           0   [0.94, 0.557, 0.128]   \n",
       " 5     110  [1.0, 0.0, 0.0]           0    [1.0, 0.656, 0.058]   \n",
       " 6     110  [1.0, 0.0, 0.0]           0  [0.881, 0.058, 0.076]   \n",
       " 7     110  [1.0, 0.0, 0.0]           0      [0.83, 0.0, 0.03]   \n",
       " 8     110  [0.0, 0.0, 1.0]           2   [0.161, 0.45, 0.743]   \n",
       " 9     110  [0.0, 0.0, 1.0]           2  [0.273, 0.771, 0.643]   \n",
       " 10    110  [0.0, 0.0, 1.0]           2   [0.11, 0.474, 0.751]   \n",
       " 11    110  [0.0, 0.0, 1.0]           2  [0.105, 0.479, 0.729]   \n",
       " 12    110  [0.0, 0.0, 1.0]           2  [0.164, 0.527, 0.741]   \n",
       " 13    110  [1.0, 0.0, 0.0]           0   [0.868, 0.32, 0.075]   \n",
       " 14    110  [1.0, 0.0, 0.0]           0    [0.977, 0.417, 0.0]   \n",
       " 15    110  [0.0, 0.0, 1.0]           2    [0.213, 0.54, 0.69]   \n",
       " 16    110  [0.0, 0.0, 1.0]           2  [0.282, 0.647, 0.592]   \n",
       " 17    110  [1.0, 0.0, 0.0]           0  [0.893, 0.532, 0.168]   \n",
       " 18    110  [1.0, 0.0, 0.0]           0  [0.891, 0.336, 0.103]   \n",
       " 19    110  [1.0, 0.0, 0.0]           0  [0.846, 0.343, 0.159]   \n",
       " 20    110  [1.0, 0.0, 0.0]           0    [0.909, 0.155, 0.0]   \n",
       " 21    110  [1.0, 0.0, 0.0]           0  [0.931, 0.456, 0.116]   \n",
       " 22    110  [1.0, 0.0, 0.0]           0  [0.889, 0.265, 0.128]   \n",
       " 23    110  [1.0, 0.0, 0.0]           0   [0.96, 0.653, 0.086]   \n",
       " 24    110  [1.0, 0.0, 0.0]           0  [0.898, 0.387, 0.108]   \n",
       " 25    110  [1.0, 0.0, 0.0]           0   [0.907, 0.382, 0.08]   \n",
       " 26    110  [1.0, 0.0, 0.0]           0  [0.936, 0.854, 0.224]   \n",
       " 27    110  [1.0, 0.0, 0.0]           0  [0.833, 0.425, 0.117]   \n",
       " 28    110  [1.0, 0.0, 0.0]           0  [0.885, 0.257, 0.082]   \n",
       " 29    110  [1.0, 0.0, 0.0]           0  [0.913, 0.434, 0.109]   \n",
       " 30    110  [0.0, 1.0, 0.0]           2  [0.413, 0.674, 0.529]   \n",
       " 31    110  [0.0, 1.0, 0.0]           2   [0.319, 0.431, 0.55]   \n",
       " 32    110  [0.0, 1.0, 0.0]           1  [0.616, 0.892, 0.414]   \n",
       " 33    110  [0.0, 1.0, 0.0]           2   [0.35, 0.449, 0.629]   \n",
       " 34    110  [0.0, 1.0, 0.0]           1      [0.59, 1.0, 0.51]   \n",
       " 35    110  [0.0, 1.0, 0.0]           2  [0.347, 0.371, 0.604]   \n",
       " 36    110  [0.0, 1.0, 0.0]           2  [0.391, 0.657, 0.497]   \n",
       " 37    110  [0.0, 1.0, 0.0]           2   [0.306, 0.575, 0.54]   \n",
       " 38    110  [0.0, 1.0, 0.0]           2   [0.387, 0.48, 0.589]   \n",
       " 39    110  [0.0, 1.0, 0.0]           2  [0.366, 0.598, 0.566]   \n",
       " 40    110  [0.0, 1.0, 0.0]           2  [0.546, 0.661, 0.489]   \n",
       " 41    110  [0.0, 1.0, 0.0]           2  [0.516, 0.789, 0.524]   \n",
       " 42    110  [0.0, 1.0, 0.0]           2  [0.474, 0.646, 0.521]   \n",
       " 43    110  [0.0, 1.0, 0.0]           2  [0.302, 0.694, 0.642]   \n",
       " 44    110  [0.0, 1.0, 0.0]           2  [0.354, 0.459, 0.483]   \n",
       " 45    110  [0.0, 1.0, 0.0]           2  [0.308, 0.393, 0.628]   \n",
       " 46    110  [0.0, 1.0, 0.0]           2  [0.449, 0.764, 0.526]   \n",
       " 47    110  [0.0, 1.0, 0.0]           2  [0.366, 0.539, 0.557]   \n",
       " 48    110  [0.0, 1.0, 0.0]           2  [0.445, 0.714, 0.519]   \n",
       " 49    110  [0.0, 1.0, 0.0]           2   [0.44, 0.631, 0.495]   \n",
       " 50    110  [0.0, 1.0, 0.0]           1  [0.615, 0.815, 0.415]   \n",
       " 51    110  [0.0, 0.0, 1.0]           2  [0.219, 0.609, 0.685]   \n",
       " 52    110  [0.0, 0.0, 1.0]           2   [0.114, 0.417, 0.84]   \n",
       " 53    110  [0.0, 0.0, 1.0]           2  [0.181, 0.649, 0.814]   \n",
       " 54    110  [0.0, 0.0, 1.0]           2  [0.055, 0.299, 0.756]   \n",
       " 55    110  [0.0, 0.0, 1.0]           2   [0.216, 0.53, 0.687]   \n",
       " 56    110  [0.0, 0.0, 1.0]           2      [0.0, 0.496, 1.0]   \n",
       " 57    110  [0.0, 0.0, 1.0]           2  [0.322, 0.831, 0.718]   \n",
       " 58    110  [0.0, 0.0, 1.0]           2   [0.273, 0.64, 0.687]   \n",
       " 59    110  [0.0, 0.0, 1.0]           2  [0.153, 0.437, 0.694]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     120  [1.0, 0.0, 0.0]           0  [0.912, 0.348, 0.086]   \n",
       " 1     120  [0.0, 0.0, 1.0]           2   [0.18, 0.394, 0.771]   \n",
       " 2     120  [1.0, 0.0, 0.0]           0  [0.928, 0.492, 0.055]   \n",
       " 3     120  [0.0, 0.0, 1.0]           2  [0.297, 0.625, 0.593]   \n",
       " 4     120  [1.0, 0.0, 0.0]           0  [0.941, 0.548, 0.124]   \n",
       " 5     120  [1.0, 0.0, 0.0]           0    [1.0, 0.641, 0.056]   \n",
       " 6     120  [1.0, 0.0, 0.0]           0   [0.89, 0.059, 0.074]   \n",
       " 7     120  [1.0, 0.0, 0.0]           0     [0.843, 0.0, 0.03]   \n",
       " 8     120  [0.0, 0.0, 1.0]           2   [0.16, 0.472, 0.746]   \n",
       " 9     120  [0.0, 0.0, 1.0]           2  [0.268, 0.783, 0.646]   \n",
       " 10    120  [0.0, 0.0, 1.0]           2   [0.11, 0.497, 0.756]   \n",
       " 11    120  [0.0, 0.0, 1.0]           2  [0.106, 0.501, 0.735]   \n",
       " 12    120  [0.0, 0.0, 1.0]           2  [0.162, 0.548, 0.745]   \n",
       " 13    120  [1.0, 0.0, 0.0]           0  [0.874, 0.314, 0.075]   \n",
       " 14    120  [1.0, 0.0, 0.0]           0    [0.982, 0.405, 0.0]   \n",
       " 15    120  [0.0, 0.0, 1.0]           2  [0.211, 0.558, 0.693]   \n",
       " 16    120  [0.0, 0.0, 1.0]           2  [0.279, 0.659, 0.596]   \n",
       " 17    120  [1.0, 0.0, 0.0]           0  [0.894, 0.526, 0.165]   \n",
       " 18    120  [1.0, 0.0, 0.0]           0  [0.896, 0.332, 0.101]   \n",
       " 19    120  [1.0, 0.0, 0.0]           0   [0.85, 0.341, 0.157]   \n",
       " 20    120  [1.0, 0.0, 0.0]           0     [0.92, 0.149, 0.0]   \n",
       " 21    120  [1.0, 0.0, 0.0]           0  [0.932, 0.449, 0.114]   \n",
       " 22    120  [1.0, 0.0, 0.0]           0  [0.894, 0.262, 0.125]   \n",
       " 23    120  [1.0, 0.0, 0.0]           0    [0.96, 0.64, 0.084]   \n",
       " 24    120  [1.0, 0.0, 0.0]           0  [0.901, 0.381, 0.106]   \n",
       " 25    120  [1.0, 0.0, 0.0]           0  [0.912, 0.375, 0.078]   \n",
       " 26    120  [1.0, 0.0, 0.0]           0  [0.929, 0.843, 0.219]   \n",
       " 27    120  [1.0, 0.0, 0.0]           0  [0.837, 0.419, 0.117]   \n",
       " 28    120  [1.0, 0.0, 0.0]           0   [0.892, 0.253, 0.08]   \n",
       " 29    120  [1.0, 0.0, 0.0]           0  [0.916, 0.427, 0.106]   \n",
       " 30    120  [0.0, 1.0, 0.0]           2    [0.41, 0.682, 0.53]   \n",
       " 31    120  [0.0, 1.0, 0.0]           2  [0.319, 0.446, 0.552]   \n",
       " 32    120  [0.0, 1.0, 0.0]           1   [0.609, 0.89, 0.413]   \n",
       " 33    120  [0.0, 1.0, 0.0]           2  [0.347, 0.466, 0.629]   \n",
       " 34    120  [0.0, 1.0, 0.0]           1    [0.579, 1.0, 0.508]   \n",
       " 35    120  [0.0, 1.0, 0.0]           2  [0.346, 0.388, 0.604]   \n",
       " 36    120  [0.0, 1.0, 0.0]           2    [0.389, 0.664, 0.5]   \n",
       " 37    120  [0.0, 1.0, 0.0]           2  [0.306, 0.585, 0.544]   \n",
       " 38    120  [0.0, 1.0, 0.0]           2  [0.384, 0.495, 0.589]   \n",
       " 39    120  [0.0, 1.0, 0.0]           2   [0.363, 0.61, 0.568]   \n",
       " 40    120  [0.0, 1.0, 0.0]           2  [0.541, 0.667, 0.487]   \n",
       " 41    120  [0.0, 1.0, 0.0]           2  [0.509, 0.795, 0.523]   \n",
       " 42    120  [0.0, 1.0, 0.0]           2   [0.469, 0.654, 0.52]   \n",
       " 43    120  [0.0, 1.0, 0.0]           2  [0.298, 0.707, 0.643]   \n",
       " 44    120  [0.0, 1.0, 0.0]           2   [0.355, 0.47, 0.486]   \n",
       " 45    120  [0.0, 1.0, 0.0]           2  [0.307, 0.411, 0.629]   \n",
       " 46    120  [0.0, 1.0, 0.0]           2  [0.444, 0.771, 0.526]   \n",
       " 47    120  [0.0, 1.0, 0.0]           2  [0.364, 0.552, 0.558]   \n",
       " 48    120  [0.0, 1.0, 0.0]           2   [0.441, 0.721, 0.52]   \n",
       " 49    120  [0.0, 1.0, 0.0]           2  [0.437, 0.639, 0.496]   \n",
       " 50    120  [0.0, 1.0, 0.0]           1  [0.609, 0.815, 0.414]   \n",
       " 51    120  [0.0, 0.0, 1.0]           2  [0.217, 0.626, 0.688]   \n",
       " 52    120  [0.0, 0.0, 1.0]           2  [0.114, 0.445, 0.841]   \n",
       " 53    120  [0.0, 0.0, 1.0]           2  [0.176, 0.671, 0.815]   \n",
       " 54    120  [0.0, 0.0, 1.0]           2  [0.059, 0.325, 0.762]   \n",
       " 55    120  [0.0, 0.0, 1.0]           2   [0.214, 0.549, 0.69]   \n",
       " 56    120  [0.0, 0.0, 1.0]           2       [0.0, 0.53, 1.0]   \n",
       " 57    120  [0.0, 0.0, 1.0]           2  [0.314, 0.845, 0.718]   \n",
       " 58    120  [0.0, 0.0, 1.0]           2  [0.269, 0.656, 0.689]   \n",
       " 59    120  [0.0, 0.0, 1.0]           2  [0.154, 0.458, 0.698]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     130  [1.0, 0.0, 0.0]           0  [0.916, 0.342, 0.084]   \n",
       " 1     130  [0.0, 0.0, 1.0]           2  [0.179, 0.417, 0.771]   \n",
       " 2     130  [1.0, 0.0, 0.0]           0  [0.931, 0.482, 0.055]   \n",
       " 3     130  [0.0, 0.0, 1.0]           2  [0.295, 0.636, 0.596]   \n",
       " 4     130  [1.0, 0.0, 0.0]           0   [0.941, 0.54, 0.121]   \n",
       " 5     130  [1.0, 0.0, 0.0]           0    [1.0, 0.628, 0.054]   \n",
       " 6     130  [1.0, 0.0, 0.0]           0  [0.898, 0.059, 0.072]   \n",
       " 7     130  [1.0, 0.0, 0.0]           0    [0.854, 0.0, 0.031]   \n",
       " 8     130  [0.0, 0.0, 1.0]           2  [0.159, 0.492, 0.749]   \n",
       " 9     130  [0.0, 0.0, 1.0]           2  [0.264, 0.793, 0.649]   \n",
       " 10    130  [0.0, 0.0, 1.0]           2   [0.111, 0.517, 0.76]   \n",
       " 11    130  [0.0, 0.0, 1.0]           2    [0.107, 0.52, 0.74]   \n",
       " 12    130  [0.0, 0.0, 1.0]           2   [0.16, 0.567, 0.748]   \n",
       " 13    130  [1.0, 0.0, 0.0]           0  [0.879, 0.309, 0.074]   \n",
       " 14    130  [1.0, 0.0, 0.0]           0    [0.986, 0.394, 0.0]   \n",
       " 15    130  [0.0, 0.0, 1.0]           2  [0.209, 0.574, 0.696]   \n",
       " 16    130  [0.0, 0.0, 1.0]           2   [0.277, 0.67, 0.599]   \n",
       " 17    130  [1.0, 0.0, 0.0]           0   [0.894, 0.52, 0.162]   \n",
       " 18    130  [1.0, 0.0, 0.0]           0    [0.9, 0.327, 0.099]   \n",
       " 19    130  [1.0, 0.0, 0.0]           0  [0.853, 0.338, 0.154]   \n",
       " 20    130  [1.0, 0.0, 0.0]           0    [0.929, 0.144, 0.0]   \n",
       " 21    130  [1.0, 0.0, 0.0]           0  [0.934, 0.442, 0.111]   \n",
       " 22    130  [1.0, 0.0, 0.0]           0   [0.898, 0.26, 0.122]   \n",
       " 23    130  [1.0, 0.0, 0.0]           0   [0.96, 0.628, 0.082]   \n",
       " 24    130  [1.0, 0.0, 0.0]           0  [0.905, 0.376, 0.104]   \n",
       " 25    130  [1.0, 0.0, 0.0]           0  [0.916, 0.368, 0.077]   \n",
       " 26    130  [1.0, 0.0, 0.0]           0  [0.924, 0.833, 0.215]   \n",
       " 27    130  [1.0, 0.0, 0.0]           0  [0.841, 0.413, 0.116]   \n",
       " 28    130  [1.0, 0.0, 0.0]           0   [0.898, 0.25, 0.079]   \n",
       " 29    130  [1.0, 0.0, 0.0]           0  [0.918, 0.421, 0.104]   \n",
       " 30    130  [0.0, 1.0, 0.0]           2    [0.406, 0.69, 0.53]   \n",
       " 31    130  [0.0, 1.0, 0.0]           2  [0.319, 0.458, 0.554]   \n",
       " 32    130  [0.0, 1.0, 0.0]           1  [0.603, 0.889, 0.412]   \n",
       " 33    130  [0.0, 1.0, 0.0]           2  [0.344, 0.482, 0.628]   \n",
       " 34    130  [0.0, 1.0, 0.0]           1     [0.57, 1.0, 0.506]   \n",
       " 35    130  [0.0, 1.0, 0.0]           2  [0.344, 0.404, 0.604]   \n",
       " 36    130  [0.0, 1.0, 0.0]           2  [0.387, 0.671, 0.502]   \n",
       " 37    130  [0.0, 1.0, 0.0]           2  [0.305, 0.595, 0.548]   \n",
       " 38    130  [0.0, 1.0, 0.0]           2  [0.382, 0.508, 0.588]   \n",
       " 39    130  [0.0, 1.0, 0.0]           2    [0.36, 0.62, 0.569]   \n",
       " 40    130  [0.0, 1.0, 0.0]           2  [0.536, 0.673, 0.485]   \n",
       " 41    130  [0.0, 1.0, 0.0]           2  [0.503, 0.799, 0.521]   \n",
       " 42    130  [0.0, 1.0, 0.0]           2   [0.465, 0.662, 0.52]   \n",
       " 43    130  [0.0, 1.0, 0.0]           2  [0.293, 0.719, 0.645]   \n",
       " 44    130  [0.0, 1.0, 0.0]           2   [0.355, 0.48, 0.489]   \n",
       " 45    130  [0.0, 1.0, 0.0]           2  [0.306, 0.428, 0.629]   \n",
       " 46    130  [0.0, 1.0, 0.0]           2  [0.439, 0.776, 0.526]   \n",
       " 47    130  [0.0, 1.0, 0.0]           2  [0.362, 0.563, 0.559]   \n",
       " 48    130  [0.0, 1.0, 0.0]           2   [0.436, 0.727, 0.52]   \n",
       " 49    130  [0.0, 1.0, 0.0]           2  [0.434, 0.646, 0.497]   \n",
       " 50    130  [0.0, 1.0, 0.0]           1  [0.603, 0.814, 0.413]   \n",
       " 51    130  [0.0, 0.0, 1.0]           2   [0.214, 0.642, 0.69]   \n",
       " 52    130  [0.0, 0.0, 1.0]           2  [0.113, 0.471, 0.841]   \n",
       " 53    130  [0.0, 0.0, 1.0]           2  [0.172, 0.691, 0.815]   \n",
       " 54    130  [0.0, 0.0, 1.0]           2  [0.062, 0.349, 0.767]   \n",
       " 55    130  [0.0, 0.0, 1.0]           2  [0.213, 0.566, 0.692]   \n",
       " 56    130  [0.0, 0.0, 1.0]           2      [0.0, 0.562, 1.0]   \n",
       " 57    130  [0.0, 0.0, 1.0]           2  [0.306, 0.857, 0.718]   \n",
       " 58    130  [0.0, 0.0, 1.0]           2   [0.264, 0.671, 0.69]   \n",
       " 59    130  [0.0, 0.0, 1.0]           2  [0.155, 0.477, 0.702]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     140  [1.0, 0.0, 0.0]           0   [0.92, 0.337, 0.082]   \n",
       " 1     140  [0.0, 0.0, 1.0]           2  [0.177, 0.439, 0.771]   \n",
       " 2     140  [1.0, 0.0, 0.0]           0  [0.934, 0.473, 0.054]   \n",
       " 3     140  [0.0, 0.0, 1.0]           2  [0.292, 0.646, 0.598]   \n",
       " 4     140  [1.0, 0.0, 0.0]           0  [0.941, 0.533, 0.118]   \n",
       " 5     140  [1.0, 0.0, 0.0]           0    [1.0, 0.616, 0.053]   \n",
       " 6     140  [1.0, 0.0, 0.0]           0   [0.905, 0.059, 0.07]   \n",
       " 7     140  [1.0, 0.0, 0.0]           0    [0.864, 0.0, 0.031]   \n",
       " 8     140  [0.0, 0.0, 1.0]           2   [0.157, 0.51, 0.752]   \n",
       " 9     140  [0.0, 0.0, 1.0]           2  [0.259, 0.803, 0.652]   \n",
       " 10    140  [0.0, 0.0, 1.0]           2  [0.111, 0.536, 0.764]   \n",
       " 11    140  [0.0, 0.0, 1.0]           2  [0.107, 0.538, 0.745]   \n",
       " 12    140  [0.0, 0.0, 1.0]           2  [0.158, 0.584, 0.751]   \n",
       " 13    140  [1.0, 0.0, 0.0]           0  [0.884, 0.304, 0.074]   \n",
       " 14    140  [1.0, 0.0, 0.0]           0     [0.99, 0.384, 0.0]   \n",
       " 15    140  [0.0, 0.0, 1.0]           2   [0.207, 0.59, 0.698]   \n",
       " 16    140  [0.0, 0.0, 1.0]           2   [0.274, 0.68, 0.602]   \n",
       " 17    140  [1.0, 0.0, 0.0]           0  [0.894, 0.515, 0.159]   \n",
       " 18    140  [1.0, 0.0, 0.0]           0  [0.904, 0.323, 0.097]   \n",
       " 19    140  [1.0, 0.0, 0.0]           0  [0.856, 0.336, 0.152]   \n",
       " 20    140  [1.0, 0.0, 0.0]           0     [0.937, 0.14, 0.0]   \n",
       " 21    140  [1.0, 0.0, 0.0]           0  [0.936, 0.436, 0.108]   \n",
       " 22    140  [1.0, 0.0, 0.0]           0   [0.902, 0.258, 0.12]   \n",
       " 23    140  [1.0, 0.0, 0.0]           0  [0.961, 0.617, 0.081]   \n",
       " 24    140  [1.0, 0.0, 0.0]           0  [0.908, 0.371, 0.102]   \n",
       " 25    140  [1.0, 0.0, 0.0]           0  [0.919, 0.362, 0.076]   \n",
       " 26    140  [1.0, 0.0, 0.0]           0  [0.919, 0.824, 0.211]   \n",
       " 27    140  [1.0, 0.0, 0.0]           0  [0.844, 0.408, 0.116]   \n",
       " 28    140  [1.0, 0.0, 0.0]           0  [0.903, 0.246, 0.077]   \n",
       " 29    140  [1.0, 0.0, 0.0]           0  [0.921, 0.415, 0.102]   \n",
       " 30    140  [0.0, 1.0, 0.0]           2  [0.403, 0.697, 0.531]   \n",
       " 31    140  [0.0, 1.0, 0.0]           2   [0.318, 0.47, 0.556]   \n",
       " 32    140  [0.0, 1.0, 0.0]           1  [0.597, 0.887, 0.411]   \n",
       " 33    140  [0.0, 1.0, 0.0]           2  [0.341, 0.496, 0.627]   \n",
       " 34    140  [0.0, 1.0, 0.0]           1    [0.561, 1.0, 0.504]   \n",
       " 35    140  [0.0, 1.0, 0.0]           2  [0.342, 0.418, 0.604]   \n",
       " 36    140  [0.0, 1.0, 0.0]           2  [0.384, 0.677, 0.504]   \n",
       " 37    140  [0.0, 1.0, 0.0]           2  [0.303, 0.604, 0.551]   \n",
       " 38    140  [0.0, 1.0, 0.0]           2  [0.379, 0.521, 0.587]   \n",
       " 39    140  [0.0, 1.0, 0.0]           2    [0.357, 0.63, 0.57]   \n",
       " 40    140  [0.0, 1.0, 0.0]           2  [0.531, 0.678, 0.483]   \n",
       " 41    140  [0.0, 1.0, 0.0]           2   [0.496, 0.804, 0.52]   \n",
       " 42    140  [0.0, 1.0, 0.0]           2  [0.461, 0.669, 0.519]   \n",
       " 43    140  [0.0, 1.0, 0.0]           2   [0.289, 0.73, 0.646]   \n",
       " 44    140  [0.0, 1.0, 0.0]           2  [0.355, 0.488, 0.492]   \n",
       " 45    140  [0.0, 1.0, 0.0]           2  [0.304, 0.443, 0.629]   \n",
       " 46    140  [0.0, 1.0, 0.0]           2  [0.434, 0.782, 0.527]   \n",
       " 47    140  [0.0, 1.0, 0.0]           2   [0.36, 0.573, 0.559]   \n",
       " 48    140  [0.0, 1.0, 0.0]           2  [0.432, 0.733, 0.521]   \n",
       " 49    140  [0.0, 1.0, 0.0]           2  [0.431, 0.652, 0.497]   \n",
       " 50    140  [0.0, 1.0, 0.0]           1  [0.598, 0.814, 0.412]   \n",
       " 51    140  [0.0, 0.0, 1.0]           2  [0.212, 0.656, 0.693]   \n",
       " 52    140  [0.0, 0.0, 1.0]           2  [0.112, 0.494, 0.842]   \n",
       " 53    140  [0.0, 0.0, 1.0]           2   [0.167, 0.71, 0.816]   \n",
       " 54    140  [0.0, 0.0, 1.0]           2  [0.065, 0.371, 0.772]   \n",
       " 55    140  [0.0, 0.0, 1.0]           2  [0.211, 0.582, 0.694]   \n",
       " 56    140  [0.0, 0.0, 1.0]           2      [0.0, 0.591, 1.0]   \n",
       " 57    140  [0.0, 0.0, 1.0]           2  [0.299, 0.869, 0.717]   \n",
       " 58    140  [0.0, 0.0, 1.0]           2   [0.26, 0.684, 0.691]   \n",
       " 59    140  [0.0, 0.0, 1.0]           2  [0.155, 0.494, 0.706]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     150  [1.0, 0.0, 0.0]           0  [0.923, 0.333, 0.081]   \n",
       " 1     150  [0.0, 0.0, 1.0]           2  [0.176, 0.458, 0.772]   \n",
       " 2     150  [1.0, 0.0, 0.0]           0  [0.936, 0.464, 0.054]   \n",
       " 3     150  [0.0, 0.0, 1.0]           2  [0.289, 0.655, 0.601]   \n",
       " 4     150  [1.0, 0.0, 0.0]           0  [0.941, 0.527, 0.116]   \n",
       " 5     150  [1.0, 0.0, 0.0]           0    [1.0, 0.605, 0.051]   \n",
       " 6     150  [1.0, 0.0, 0.0]           0   [0.911, 0.06, 0.069]   \n",
       " 7     150  [1.0, 0.0, 0.0]           0    [0.872, 0.0, 0.032]   \n",
       " 8     150  [0.0, 0.0, 1.0]           2  [0.156, 0.526, 0.755]   \n",
       " 9     150  [0.0, 0.0, 1.0]           2  [0.255, 0.811, 0.655]   \n",
       " 10    150  [0.0, 0.0, 1.0]           2   [0.11, 0.553, 0.768]   \n",
       " 11    150  [0.0, 0.0, 1.0]           2   [0.107, 0.553, 0.75]   \n",
       " 12    150  [0.0, 0.0, 1.0]           2  [0.156, 0.599, 0.754]   \n",
       " 13    150  [1.0, 0.0, 0.0]           0    [0.888, 0.3, 0.073]   \n",
       " 14    150  [1.0, 0.0, 0.0]           0    [0.994, 0.375, 0.0]   \n",
       " 15    150  [0.0, 0.0, 1.0]           2  [0.204, 0.603, 0.701]   \n",
       " 16    150  [0.0, 0.0, 1.0]           2  [0.272, 0.688, 0.605]   \n",
       " 17    150  [1.0, 0.0, 0.0]           0   [0.894, 0.51, 0.156]   \n",
       " 18    150  [1.0, 0.0, 0.0]           0  [0.907, 0.319, 0.095]   \n",
       " 19    150  [1.0, 0.0, 0.0]           0   [0.858, 0.334, 0.15]   \n",
       " 20    150  [1.0, 0.0, 0.0]           0  [0.943, 0.136, 0.001]   \n",
       " 21    150  [1.0, 0.0, 0.0]           0  [0.937, 0.431, 0.106]   \n",
       " 22    150  [1.0, 0.0, 0.0]           0  [0.905, 0.257, 0.117]   \n",
       " 23    150  [1.0, 0.0, 0.0]           0  [0.961, 0.608, 0.079]   \n",
       " 24    150  [1.0, 0.0, 0.0]           0     [0.91, 0.367, 0.1]   \n",
       " 25    150  [1.0, 0.0, 0.0]           0  [0.922, 0.357, 0.075]   \n",
       " 26    150  [1.0, 0.0, 0.0]           0  [0.915, 0.816, 0.208]   \n",
       " 27    150  [1.0, 0.0, 0.0]           0  [0.847, 0.403, 0.116]   \n",
       " 28    150  [1.0, 0.0, 0.0]           0  [0.907, 0.243, 0.076]   \n",
       " 29    150  [1.0, 0.0, 0.0]           0     [0.923, 0.41, 0.1]   \n",
       " 30    150  [0.0, 1.0, 0.0]           2  [0.399, 0.703, 0.532]   \n",
       " 31    150  [0.0, 1.0, 0.0]           2   [0.317, 0.48, 0.558]   \n",
       " 32    150  [0.0, 1.0, 0.0]           1   [0.592, 0.886, 0.41]   \n",
       " 33    150  [0.0, 1.0, 0.0]           2  [0.339, 0.509, 0.627]   \n",
       " 34    150  [0.0, 1.0, 0.0]           1    [0.553, 1.0, 0.503]   \n",
       " 35    150  [0.0, 1.0, 0.0]           2   [0.34, 0.431, 0.604]   \n",
       " 36    150  [0.0, 1.0, 0.0]           2  [0.382, 0.682, 0.506]   \n",
       " 37    150  [0.0, 1.0, 0.0]           2  [0.301, 0.612, 0.554]   \n",
       " 38    150  [0.0, 1.0, 0.0]           2  [0.376, 0.531, 0.587]   \n",
       " 39    150  [0.0, 1.0, 0.0]           2  [0.354, 0.638, 0.571]   \n",
       " 40    150  [0.0, 1.0, 0.0]           2  [0.527, 0.683, 0.482]   \n",
       " 41    150  [0.0, 1.0, 0.0]           2  [0.491, 0.808, 0.519]   \n",
       " 42    150  [0.0, 1.0, 0.0]           2  [0.457, 0.675, 0.519]   \n",
       " 43    150  [0.0, 1.0, 0.0]           2   [0.285, 0.74, 0.648]   \n",
       " 44    150  [0.0, 1.0, 0.0]           2  [0.355, 0.496, 0.494]   \n",
       " 45    150  [0.0, 1.0, 0.0]           2   [0.302, 0.456, 0.63]   \n",
       " 46    150  [0.0, 1.0, 0.0]           2  [0.429, 0.786, 0.527]   \n",
       " 47    150  [0.0, 1.0, 0.0]           2   [0.357, 0.582, 0.56]   \n",
       " 48    150  [0.0, 1.0, 0.0]           2  [0.428, 0.738, 0.521]   \n",
       " 49    150  [0.0, 1.0, 0.0]           2  [0.428, 0.657, 0.498]   \n",
       " 50    150  [0.0, 1.0, 0.0]           1  [0.593, 0.814, 0.411]   \n",
       " 51    150  [0.0, 0.0, 1.0]           2  [0.209, 0.668, 0.695]   \n",
       " 52    150  [0.0, 0.0, 1.0]           2  [0.111, 0.515, 0.843]   \n",
       " 53    150  [0.0, 0.0, 1.0]           2  [0.163, 0.726, 0.817]   \n",
       " 54    150  [0.0, 0.0, 1.0]           2   [0.067, 0.39, 0.777]   \n",
       " 55    150  [0.0, 0.0, 1.0]           2  [0.209, 0.595, 0.696]   \n",
       " 56    150  [0.0, 0.0, 1.0]           2      [0.0, 0.617, 1.0]   \n",
       " 57    150  [0.0, 0.0, 1.0]           2  [0.292, 0.879, 0.717]   \n",
       " 58    150  [0.0, 0.0, 1.0]           2  [0.256, 0.696, 0.693]   \n",
       " 59    150  [0.0, 0.0, 1.0]           2  [0.154, 0.509, 0.709]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     160  [1.0, 0.0, 0.0]           0  [0.926, 0.328, 0.079]   \n",
       " 1     160  [0.0, 0.0, 1.0]           2  [0.174, 0.475, 0.772]   \n",
       " 2     160  [1.0, 0.0, 0.0]           0  [0.938, 0.457, 0.053]   \n",
       " 3     160  [0.0, 0.0, 1.0]           2  [0.286, 0.663, 0.603]   \n",
       " 4     160  [1.0, 0.0, 0.0]           0  [0.942, 0.521, 0.113]   \n",
       " 5     160  [1.0, 0.0, 0.0]           0     [1.0, 0.596, 0.05]   \n",
       " 6     160  [1.0, 0.0, 0.0]           0   [0.917, 0.06, 0.067]   \n",
       " 7     160  [1.0, 0.0, 0.0]           0     [0.88, 0.0, 0.032]   \n",
       " 8     160  [0.0, 0.0, 1.0]           2  [0.154, 0.542, 0.758]   \n",
       " 9     160  [0.0, 0.0, 1.0]           2  [0.251, 0.819, 0.658]   \n",
       " 10    160  [0.0, 0.0, 1.0]           2   [0.11, 0.568, 0.771]   \n",
       " 11    160  [0.0, 0.0, 1.0]           2  [0.107, 0.568, 0.754]   \n",
       " 12    160  [0.0, 0.0, 1.0]           2  [0.154, 0.613, 0.757]   \n",
       " 13    160  [1.0, 0.0, 0.0]           0  [0.892, 0.296, 0.073]   \n",
       " 14    160  [1.0, 0.0, 0.0]           0    [0.997, 0.366, 0.0]   \n",
       " 15    160  [0.0, 0.0, 1.0]           2  [0.202, 0.616, 0.703]   \n",
       " 16    160  [0.0, 0.0, 1.0]           2  [0.269, 0.696, 0.607]   \n",
       " 17    160  [1.0, 0.0, 0.0]           0  [0.895, 0.506, 0.153]   \n",
       " 18    160  [1.0, 0.0, 0.0]           0   [0.91, 0.316, 0.094]   \n",
       " 19    160  [1.0, 0.0, 0.0]           0   [0.86, 0.333, 0.148]   \n",
       " 20    160  [1.0, 0.0, 0.0]           0   [0.95, 0.132, 0.001]   \n",
       " 21    160  [1.0, 0.0, 0.0]           0  [0.938, 0.426, 0.104]   \n",
       " 22    160  [1.0, 0.0, 0.0]           0  [0.908, 0.255, 0.115]   \n",
       " 23    160  [1.0, 0.0, 0.0]           0  [0.961, 0.599, 0.078]   \n",
       " 24    160  [1.0, 0.0, 0.0]           0  [0.913, 0.363, 0.098]   \n",
       " 25    160  [1.0, 0.0, 0.0]           0  [0.925, 0.352, 0.074]   \n",
       " 26    160  [1.0, 0.0, 0.0]           0  [0.911, 0.809, 0.204]   \n",
       " 27    160  [1.0, 0.0, 0.0]           0  [0.849, 0.399, 0.115]   \n",
       " 28    160  [1.0, 0.0, 0.0]           0   [0.911, 0.24, 0.074]   \n",
       " 29    160  [1.0, 0.0, 0.0]           0  [0.924, 0.406, 0.098]   \n",
       " 30    160  [0.0, 1.0, 0.0]           2  [0.396, 0.708, 0.532]   \n",
       " 31    160  [0.0, 1.0, 0.0]           2    [0.316, 0.49, 0.56]   \n",
       " 32    160  [0.0, 1.0, 0.0]           1   [0.587, 0.884, 0.41]   \n",
       " 33    160  [0.0, 1.0, 0.0]           2  [0.336, 0.521, 0.626]   \n",
       " 34    160  [0.0, 1.0, 0.0]           1    [0.546, 1.0, 0.501]   \n",
       " 35    160  [0.0, 1.0, 0.0]           2  [0.338, 0.443, 0.604]   \n",
       " 36    160  [0.0, 1.0, 0.0]           2  [0.379, 0.687, 0.507]   \n",
       " 37    160  [0.0, 1.0, 0.0]           2   [0.299, 0.62, 0.557]   \n",
       " 38    160  [0.0, 1.0, 0.0]           2  [0.372, 0.542, 0.587]   \n",
       " 39    160  [0.0, 1.0, 0.0]           2   [0.35, 0.646, 0.572]   \n",
       " 40    160  [0.0, 1.0, 0.0]           2   [0.523, 0.687, 0.48]   \n",
       " 41    160  [0.0, 1.0, 0.0]           2  [0.485, 0.812, 0.518]   \n",
       " 42    160  [0.0, 1.0, 0.0]           2   [0.453, 0.68, 0.518]   \n",
       " 43    160  [0.0, 1.0, 0.0]           2  [0.281, 0.749, 0.649]   \n",
       " 44    160  [0.0, 1.0, 0.0]           2  [0.354, 0.503, 0.496]   \n",
       " 45    160  [0.0, 1.0, 0.0]           2    [0.3, 0.469, 0.631]   \n",
       " 46    160  [0.0, 1.0, 0.0]           2  [0.425, 0.791, 0.527]   \n",
       " 47    160  [0.0, 1.0, 0.0]           2  [0.355, 0.591, 0.561]   \n",
       " 48    160  [0.0, 1.0, 0.0]           2  [0.424, 0.743, 0.521]   \n",
       " 49    160  [0.0, 1.0, 0.0]           2  [0.425, 0.663, 0.499]   \n",
       " 50    160  [0.0, 1.0, 0.0]           1  [0.588, 0.813, 0.411]   \n",
       " 51    160  [0.0, 0.0, 1.0]           2   [0.207, 0.68, 0.697]   \n",
       " 52    160  [0.0, 0.0, 1.0]           2   [0.11, 0.535, 0.843]   \n",
       " 53    160  [0.0, 0.0, 1.0]           2   [0.16, 0.742, 0.817]   \n",
       " 54    160  [0.0, 0.0, 1.0]           2  [0.068, 0.407, 0.781]   \n",
       " 55    160  [0.0, 0.0, 1.0]           2  [0.206, 0.608, 0.698]   \n",
       " 56    160  [0.0, 0.0, 1.0]           2      [0.0, 0.641, 1.0]   \n",
       " 57    160  [0.0, 0.0, 1.0]           2  [0.286, 0.888, 0.717]   \n",
       " 58    160  [0.0, 0.0, 1.0]           2  [0.252, 0.707, 0.694]   \n",
       " 59    160  [0.0, 0.0, 1.0]           2  [0.154, 0.523, 0.713]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     170  [1.0, 0.0, 0.0]           0  [0.928, 0.325, 0.078]   \n",
       " 1     170  [0.0, 0.0, 1.0]           2  [0.172, 0.491, 0.773]   \n",
       " 2     170  [1.0, 0.0, 0.0]           0    [0.94, 0.45, 0.053]   \n",
       " 3     170  [0.0, 0.0, 1.0]           2  [0.284, 0.671, 0.605]   \n",
       " 4     170  [1.0, 0.0, 0.0]           0  [0.942, 0.515, 0.111]   \n",
       " 5     170  [1.0, 0.0, 0.0]           0    [1.0, 0.587, 0.049]   \n",
       " 6     170  [1.0, 0.0, 0.0]           0   [0.922, 0.06, 0.066]   \n",
       " 7     170  [1.0, 0.0, 0.0]           0    [0.887, 0.0, 0.032]   \n",
       " 8     170  [0.0, 0.0, 1.0]           2  [0.152, 0.555, 0.761]   \n",
       " 9     170  [0.0, 0.0, 1.0]           2   [0.247, 0.826, 0.66]   \n",
       " 10    170  [0.0, 0.0, 1.0]           2  [0.109, 0.582, 0.775]   \n",
       " 11    170  [0.0, 0.0, 1.0]           2  [0.106, 0.581, 0.758]   \n",
       " 12    170  [0.0, 0.0, 1.0]           2   [0.152, 0.626, 0.76]   \n",
       " 13    170  [1.0, 0.0, 0.0]           0  [0.896, 0.292, 0.072]   \n",
       " 14    170  [1.0, 0.0, 0.0]           0    [0.999, 0.359, 0.0]   \n",
       " 15    170  [0.0, 0.0, 1.0]           2  [0.199, 0.627, 0.706]   \n",
       " 16    170  [0.0, 0.0, 1.0]           2   [0.266, 0.704, 0.61]   \n",
       " 17    170  [1.0, 0.0, 0.0]           0   [0.895, 0.502, 0.15]   \n",
       " 18    170  [1.0, 0.0, 0.0]           0  [0.913, 0.313, 0.092]   \n",
       " 19    170  [1.0, 0.0, 0.0]           0  [0.862, 0.331, 0.146]   \n",
       " 20    170  [1.0, 0.0, 0.0]           0  [0.955, 0.129, 0.001]   \n",
       " 21    170  [1.0, 0.0, 0.0]           0  [0.939, 0.422, 0.102]   \n",
       " 22    170  [1.0, 0.0, 0.0]           0  [0.911, 0.254, 0.113]   \n",
       " 23    170  [1.0, 0.0, 0.0]           0  [0.961, 0.591, 0.076]   \n",
       " 24    170  [1.0, 0.0, 0.0]           0  [0.915, 0.359, 0.097]   \n",
       " 25    170  [1.0, 0.0, 0.0]           0  [0.927, 0.347, 0.073]   \n",
       " 26    170  [1.0, 0.0, 0.0]           0  [0.907, 0.802, 0.201]   \n",
       " 27    170  [1.0, 0.0, 0.0]           0  [0.851, 0.395, 0.115]   \n",
       " 28    170  [1.0, 0.0, 0.0]           0  [0.915, 0.238, 0.073]   \n",
       " 29    170  [1.0, 0.0, 0.0]           0  [0.926, 0.401, 0.097]   \n",
       " 30    170  [0.0, 1.0, 0.0]           2  [0.392, 0.714, 0.533]   \n",
       " 31    170  [0.0, 1.0, 0.0]           2  [0.314, 0.499, 0.562]   \n",
       " 32    170  [0.0, 1.0, 0.0]           1  [0.582, 0.883, 0.409]   \n",
       " 33    170  [0.0, 1.0, 0.0]           2  [0.332, 0.531, 0.626]   \n",
       " 34    170  [0.0, 1.0, 0.0]           1      [0.539, 1.0, 0.5]   \n",
       " 35    170  [0.0, 1.0, 0.0]           2  [0.336, 0.454, 0.604]   \n",
       " 36    170  [0.0, 1.0, 0.0]           2  [0.376, 0.691, 0.509]   \n",
       " 37    170  [0.0, 1.0, 0.0]           2   [0.297, 0.626, 0.56]   \n",
       " 38    170  [0.0, 1.0, 0.0]           2  [0.369, 0.551, 0.587]   \n",
       " 39    170  [0.0, 1.0, 0.0]           2  [0.347, 0.653, 0.573]   \n",
       " 40    170  [0.0, 1.0, 0.0]           2  [0.519, 0.691, 0.479]   \n",
       " 41    170  [0.0, 1.0, 0.0]           2   [0.48, 0.815, 0.517]   \n",
       " 42    170  [0.0, 1.0, 0.0]           2  [0.449, 0.685, 0.518]   \n",
       " 43    170  [0.0, 1.0, 0.0]           2   [0.277, 0.757, 0.65]   \n",
       " 44    170  [0.0, 1.0, 0.0]           2   [0.353, 0.51, 0.499]   \n",
       " 45    170  [0.0, 1.0, 0.0]           2   [0.297, 0.48, 0.631]   \n",
       " 46    170  [0.0, 1.0, 0.0]           2   [0.42, 0.794, 0.527]   \n",
       " 47    170  [0.0, 1.0, 0.0]           2  [0.352, 0.598, 0.562]   \n",
       " 48    170  [0.0, 1.0, 0.0]           2   [0.42, 0.747, 0.522]   \n",
       " 49    170  [0.0, 1.0, 0.0]           2  [0.422, 0.667, 0.499]   \n",
       " 50    170  [0.0, 1.0, 0.0]           1   [0.584, 0.813, 0.41]   \n",
       " 51    170  [0.0, 0.0, 1.0]           2   [0.204, 0.69, 0.699]   \n",
       " 52    170  [0.0, 0.0, 1.0]           2  [0.108, 0.552, 0.844]   \n",
       " 53    170  [0.0, 0.0, 1.0]           2  [0.156, 0.756, 0.818]   \n",
       " 54    170  [0.0, 0.0, 1.0]           2  [0.069, 0.423, 0.785]   \n",
       " 55    170  [0.0, 0.0, 1.0]           2    [0.204, 0.619, 0.7]   \n",
       " 56    170  [0.0, 0.0, 1.0]           2      [0.0, 0.663, 1.0]   \n",
       " 57    170  [0.0, 0.0, 1.0]           2   [0.28, 0.897, 0.717]   \n",
       " 58    170  [0.0, 0.0, 1.0]           2  [0.248, 0.717, 0.696]   \n",
       " 59    170  [0.0, 0.0, 1.0]           2  [0.153, 0.536, 0.716]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     180  [1.0, 0.0, 0.0]           0  [0.929, 0.321, 0.076]   \n",
       " 1     180  [0.0, 0.0, 1.0]           2   [0.17, 0.506, 0.773]   \n",
       " 2     180  [1.0, 0.0, 0.0]           0   [0.94, 0.443, 0.052]   \n",
       " 3     180  [0.0, 0.0, 1.0]           2   [0.28, 0.678, 0.608]   \n",
       " 4     180  [1.0, 0.0, 0.0]           0   [0.94, 0.511, 0.109]   \n",
       " 5     180  [1.0, 0.0, 0.0]           0  [0.998, 0.578, 0.047]   \n",
       " 6     180  [1.0, 0.0, 0.0]           0  [0.925, 0.061, 0.064]   \n",
       " 7     180  [1.0, 0.0, 0.0]           0    [0.891, 0.0, 0.033]   \n",
       " 8     180  [0.0, 0.0, 1.0]           2  [0.149, 0.568, 0.763]   \n",
       " 9     180  [0.0, 0.0, 1.0]           2  [0.242, 0.832, 0.662]   \n",
       " 10    180  [0.0, 0.0, 1.0]           2  [0.107, 0.595, 0.778]   \n",
       " 11    180  [0.0, 0.0, 1.0]           2  [0.105, 0.593, 0.762]   \n",
       " 12    180  [0.0, 0.0, 1.0]           2  [0.149, 0.637, 0.763]   \n",
       " 13    180  [1.0, 0.0, 0.0]           0  [0.897, 0.289, 0.072]   \n",
       " 14    180  [1.0, 0.0, 0.0]           0      [1.0, 0.352, 0.0]   \n",
       " 15    180  [0.0, 0.0, 1.0]           2  [0.196, 0.637, 0.708]   \n",
       " 16    180  [0.0, 0.0, 1.0]           2   [0.263, 0.71, 0.613]   \n",
       " 17    180  [1.0, 0.0, 0.0]           0  [0.893, 0.499, 0.148]   \n",
       " 18    180  [1.0, 0.0, 0.0]           0   [0.913, 0.31, 0.091]   \n",
       " 19    180  [1.0, 0.0, 0.0]           0   [0.862, 0.33, 0.144]   \n",
       " 20    180  [1.0, 0.0, 0.0]           0  [0.958, 0.126, 0.001]   \n",
       " 21    180  [1.0, 0.0, 0.0]           0    [0.938, 0.418, 0.1]   \n",
       " 22    180  [1.0, 0.0, 0.0]           0  [0.911, 0.253, 0.111]   \n",
       " 23    180  [1.0, 0.0, 0.0]           0  [0.959, 0.583, 0.075]   \n",
       " 24    180  [1.0, 0.0, 0.0]           0  [0.915, 0.356, 0.095]   \n",
       " 25    180  [1.0, 0.0, 0.0]           0  [0.928, 0.343, 0.072]   \n",
       " 26    180  [1.0, 0.0, 0.0]           0  [0.902, 0.797, 0.198]   \n",
       " 27    180  [1.0, 0.0, 0.0]           0  [0.852, 0.391, 0.114]   \n",
       " 28    180  [1.0, 0.0, 0.0]           0  [0.916, 0.236, 0.072]   \n",
       " 29    180  [1.0, 0.0, 0.0]           0  [0.926, 0.397, 0.095]   \n",
       " 30    180  [0.0, 1.0, 0.0]           2  [0.388, 0.718, 0.533]   \n",
       " 31    180  [0.0, 1.0, 0.0]           2  [0.312, 0.507, 0.563]   \n",
       " 32    180  [0.0, 1.0, 0.0]           1  [0.577, 0.882, 0.408]   \n",
       " 33    180  [0.0, 1.0, 0.0]           2  [0.329, 0.541, 0.626]   \n",
       " 34    180  [0.0, 1.0, 0.0]           1    [0.531, 1.0, 0.498]   \n",
       " 35    180  [0.0, 1.0, 0.0]           2  [0.333, 0.464, 0.604]   \n",
       " 36    180  [0.0, 1.0, 0.0]           2  [0.372, 0.695, 0.511]   \n",
       " 37    180  [0.0, 1.0, 0.0]           2  [0.294, 0.632, 0.562]   \n",
       " 38    180  [0.0, 1.0, 0.0]           2  [0.365, 0.559, 0.586]   \n",
       " 39    180  [0.0, 1.0, 0.0]           2  [0.343, 0.659, 0.574]   \n",
       " 40    180  [0.0, 1.0, 0.0]           2  [0.514, 0.695, 0.478]   \n",
       " 41    180  [0.0, 1.0, 0.0]           1  [0.474, 0.818, 0.516]   \n",
       " 42    180  [0.0, 1.0, 0.0]           2   [0.444, 0.69, 0.517]   \n",
       " 43    180  [0.0, 1.0, 0.0]           2  [0.273, 0.764, 0.652]   \n",
       " 44    180  [0.0, 1.0, 0.0]           2  [0.351, 0.515, 0.501]   \n",
       " 45    180  [0.0, 1.0, 0.0]           2  [0.294, 0.491, 0.632]   \n",
       " 46    180  [0.0, 1.0, 0.0]           2  [0.415, 0.798, 0.527]   \n",
       " 47    180  [0.0, 1.0, 0.0]           2  [0.348, 0.605, 0.563]   \n",
       " 48    180  [0.0, 1.0, 0.0]           2  [0.415, 0.751, 0.522]   \n",
       " 49    180  [0.0, 1.0, 0.0]           2    [0.418, 0.671, 0.5]   \n",
       " 50    180  [0.0, 1.0, 0.0]           1  [0.579, 0.812, 0.409]   \n",
       " 51    180  [0.0, 0.0, 1.0]           2    [0.201, 0.7, 0.702]   \n",
       " 52    180  [0.0, 0.0, 1.0]           2  [0.106, 0.568, 0.845]   \n",
       " 53    180  [0.0, 0.0, 1.0]           2  [0.152, 0.768, 0.819]   \n",
       " 54    180  [0.0, 0.0, 1.0]           2   [0.07, 0.438, 0.789]   \n",
       " 55    180  [0.0, 0.0, 1.0]           2   [0.201, 0.63, 0.702]   \n",
       " 56    180  [0.0, 0.0, 1.0]           2      [0.0, 0.682, 1.0]   \n",
       " 57    180  [0.0, 0.0, 1.0]           2  [0.273, 0.904, 0.717]   \n",
       " 58    180  [0.0, 0.0, 1.0]           2  [0.243, 0.726, 0.697]   \n",
       " 59    180  [0.0, 0.0, 1.0]           2  [0.151, 0.547, 0.719]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     190  [1.0, 0.0, 0.0]           0  [0.929, 0.318, 0.075]   \n",
       " 1     190  [0.0, 0.0, 1.0]           2   [0.168, 0.52, 0.774]   \n",
       " 2     190  [1.0, 0.0, 0.0]           0   [0.94, 0.438, 0.052]   \n",
       " 3     190  [0.0, 0.0, 1.0]           2   [0.276, 0.684, 0.61]   \n",
       " 4     190  [1.0, 0.0, 0.0]           0  [0.938, 0.506, 0.107]   \n",
       " 5     190  [1.0, 0.0, 0.0]           0  [0.996, 0.571, 0.046]   \n",
       " 6     190  [1.0, 0.0, 0.0]           0  [0.927, 0.061, 0.063]   \n",
       " 7     190  [1.0, 0.0, 0.0]           0    [0.895, 0.0, 0.033]   \n",
       " 8     190  [0.0, 0.0, 1.0]           2  [0.147, 0.579, 0.766]   \n",
       " 9     190  [0.0, 0.0, 1.0]           2  [0.238, 0.838, 0.665]   \n",
       " 10    190  [0.0, 0.0, 1.0]           2  [0.106, 0.606, 0.781]   \n",
       " 11    190  [0.0, 0.0, 1.0]           2  [0.104, 0.603, 0.766]   \n",
       " 12    190  [0.0, 0.0, 1.0]           2  [0.146, 0.648, 0.765]   \n",
       " 13    190  [1.0, 0.0, 0.0]           0  [0.898, 0.285, 0.071]   \n",
       " 14    190  [1.0, 0.0, 0.0]           0      [1.0, 0.346, 0.0]   \n",
       " 15    190  [0.0, 0.0, 1.0]           2   [0.193, 0.646, 0.71]   \n",
       " 16    190  [0.0, 0.0, 1.0]           2  [0.259, 0.716, 0.615]   \n",
       " 17    190  [1.0, 0.0, 0.0]           0  [0.891, 0.496, 0.146]   \n",
       " 18    190  [1.0, 0.0, 0.0]           0  [0.913, 0.308, 0.089]   \n",
       " 19    190  [1.0, 0.0, 0.0]           0  [0.862, 0.328, 0.143]   \n",
       " 20    190  [1.0, 0.0, 0.0]           0  [0.961, 0.124, 0.001]   \n",
       " 21    190  [1.0, 0.0, 0.0]           0  [0.937, 0.414, 0.098]   \n",
       " 22    190  [1.0, 0.0, 0.0]           0  [0.911, 0.252, 0.109]   \n",
       " 23    190  [1.0, 0.0, 0.0]           0  [0.957, 0.577, 0.074]   \n",
       " 24    190  [1.0, 0.0, 0.0]           0  [0.915, 0.353, 0.094]   \n",
       " 25    190  [1.0, 0.0, 0.0]           0  [0.928, 0.339, 0.071]   \n",
       " 26    190  [1.0, 0.0, 0.0]           0  [0.898, 0.791, 0.195]   \n",
       " 27    190  [1.0, 0.0, 0.0]           0  [0.851, 0.387, 0.114]   \n",
       " 28    190  [1.0, 0.0, 0.0]           0  [0.917, 0.234, 0.071]   \n",
       " 29    190  [1.0, 0.0, 0.0]           0  [0.925, 0.394, 0.093]   \n",
       " 30    190  [0.0, 1.0, 0.0]           2  [0.383, 0.722, 0.534]   \n",
       " 31    190  [0.0, 1.0, 0.0]           2  [0.309, 0.514, 0.565]   \n",
       " 32    190  [0.0, 1.0, 0.0]           1  [0.571, 0.881, 0.407]   \n",
       " 33    190  [0.0, 1.0, 0.0]           2   [0.325, 0.55, 0.626]   \n",
       " 34    190  [0.0, 1.0, 0.0]           1    [0.524, 1.0, 0.497]   \n",
       " 35    190  [0.0, 1.0, 0.0]           2   [0.33, 0.473, 0.604]   \n",
       " 36    190  [0.0, 1.0, 0.0]           2  [0.369, 0.699, 0.512]   \n",
       " 37    190  [0.0, 1.0, 0.0]           2  [0.291, 0.637, 0.565]   \n",
       " 38    190  [0.0, 1.0, 0.0]           2  [0.362, 0.567, 0.586]   \n",
       " 39    190  [0.0, 1.0, 0.0]           2  [0.339, 0.665, 0.575]   \n",
       " 40    190  [0.0, 1.0, 0.0]           2  [0.509, 0.698, 0.477]   \n",
       " 41    190  [0.0, 1.0, 0.0]           1  [0.468, 0.821, 0.516]   \n",
       " 42    190  [0.0, 1.0, 0.0]           2  [0.439, 0.694, 0.517]   \n",
       " 43    190  [0.0, 1.0, 0.0]           2  [0.268, 0.771, 0.653]   \n",
       " 44    190  [0.0, 1.0, 0.0]           2   [0.348, 0.52, 0.503]   \n",
       " 45    190  [0.0, 1.0, 0.0]           2    [0.291, 0.5, 0.632]   \n",
       " 46    190  [0.0, 1.0, 0.0]           2   [0.41, 0.801, 0.527]   \n",
       " 47    190  [0.0, 1.0, 0.0]           2  [0.345, 0.611, 0.564]   \n",
       " 48    190  [0.0, 1.0, 0.0]           2   [0.41, 0.754, 0.523]   \n",
       " 49    190  [0.0, 1.0, 0.0]           2    [0.414, 0.675, 0.5]   \n",
       " 50    190  [0.0, 1.0, 0.0]           1  [0.573, 0.812, 0.408]   \n",
       " 51    190  [0.0, 0.0, 1.0]           2  [0.197, 0.709, 0.704]   \n",
       " 52    190  [0.0, 0.0, 1.0]           2  [0.105, 0.583, 0.846]   \n",
       " 53    190  [0.0, 0.0, 1.0]           2   [0.148, 0.78, 0.819]   \n",
       " 54    190  [0.0, 0.0, 1.0]           2   [0.07, 0.451, 0.793]   \n",
       " 55    190  [0.0, 0.0, 1.0]           2  [0.198, 0.639, 0.704]   \n",
       " 56    190  [0.0, 0.0, 1.0]           2      [0.0, 0.701, 1.0]   \n",
       " 57    190  [0.0, 0.0, 1.0]           2  [0.267, 0.911, 0.717]   \n",
       " 58    190  [0.0, 0.0, 1.0]           2  [0.239, 0.734, 0.698]   \n",
       " 59    190  [0.0, 0.0, 1.0]           2   [0.15, 0.558, 0.722]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     200  [1.0, 0.0, 0.0]           0  [0.929, 0.315, 0.074]   \n",
       " 1     200  [0.0, 0.0, 1.0]           2  [0.165, 0.532, 0.774]   \n",
       " 2     200  [1.0, 0.0, 0.0]           0   [0.94, 0.432, 0.052]   \n",
       " 3     200  [0.0, 0.0, 1.0]           2  [0.273, 0.689, 0.612]   \n",
       " 4     200  [1.0, 0.0, 0.0]           0  [0.937, 0.502, 0.104]   \n",
       " 5     200  [1.0, 0.0, 0.0]           0  [0.994, 0.564, 0.045]   \n",
       " 6     200  [1.0, 0.0, 0.0]           0  [0.929, 0.061, 0.062]   \n",
       " 7     200  [1.0, 0.0, 0.0]           0    [0.898, 0.0, 0.033]   \n",
       " 8     200  [0.0, 0.0, 1.0]           2  [0.145, 0.589, 0.768]   \n",
       " 9     200  [0.0, 0.0, 1.0]           2  [0.234, 0.843, 0.667]   \n",
       " 10    200  [0.0, 0.0, 1.0]           2  [0.105, 0.617, 0.784]   \n",
       " 11    200  [0.0, 0.0, 1.0]           2   [0.103, 0.613, 0.77]   \n",
       " 12    200  [0.0, 0.0, 1.0]           2  [0.144, 0.657, 0.768]   \n",
       " 13    200  [1.0, 0.0, 0.0]           0  [0.899, 0.282, 0.071]   \n",
       " 14    200  [1.0, 0.0, 0.0]           0       [1.0, 0.34, 0.0]   \n",
       " 15    200  [0.0, 0.0, 1.0]           2   [0.19, 0.655, 0.712]   \n",
       " 16    200  [0.0, 0.0, 1.0]           2  [0.256, 0.721, 0.617]   \n",
       " 17    200  [1.0, 0.0, 0.0]           0   [0.89, 0.493, 0.144]   \n",
       " 18    200  [1.0, 0.0, 0.0]           0  [0.914, 0.305, 0.088]   \n",
       " 19    200  [1.0, 0.0, 0.0]           0  [0.861, 0.327, 0.141]   \n",
       " 20    200  [1.0, 0.0, 0.0]           0  [0.963, 0.121, 0.001]   \n",
       " 21    200  [1.0, 0.0, 0.0]           0  [0.936, 0.411, 0.096]   \n",
       " 22    200  [1.0, 0.0, 0.0]           0  [0.911, 0.251, 0.107]   \n",
       " 23    200  [1.0, 0.0, 0.0]           0   [0.955, 0.57, 0.072]   \n",
       " 24    200  [1.0, 0.0, 0.0]           0   [0.915, 0.35, 0.092]   \n",
       " 25    200  [1.0, 0.0, 0.0]           0   [0.928, 0.336, 0.07]   \n",
       " 26    200  [1.0, 0.0, 0.0]           0  [0.893, 0.786, 0.192]   \n",
       " 27    200  [1.0, 0.0, 0.0]           0  [0.851, 0.384, 0.113]   \n",
       " 28    200  [1.0, 0.0, 0.0]           0   [0.918, 0.232, 0.07]   \n",
       " 29    200  [1.0, 0.0, 0.0]           0  [0.924, 0.391, 0.092]   \n",
       " 30    200  [0.0, 1.0, 0.0]           2  [0.379, 0.726, 0.535]   \n",
       " 31    200  [0.0, 1.0, 0.0]           2   [0.307, 0.52, 0.567]   \n",
       " 32    200  [0.0, 1.0, 0.0]           1   [0.566, 0.88, 0.407]   \n",
       " 33    200  [0.0, 1.0, 0.0]           2  [0.321, 0.559, 0.625]   \n",
       " 34    200  [0.0, 1.0, 0.0]           1    [0.517, 1.0, 0.496]   \n",
       " 35    200  [0.0, 1.0, 0.0]           2  [0.327, 0.482, 0.604]   \n",
       " 36    200  [0.0, 1.0, 0.0]           2  [0.365, 0.702, 0.514]   \n",
       " 37    200  [0.0, 1.0, 0.0]           2  [0.288, 0.642, 0.567]   \n",
       " 38    200  [0.0, 1.0, 0.0]           2  [0.358, 0.574, 0.586]   \n",
       " 39    200  [0.0, 1.0, 0.0]           2   [0.335, 0.67, 0.576]   \n",
       " 40    200  [0.0, 1.0, 0.0]           2  [0.505, 0.701, 0.475]   \n",
       " 41    200  [0.0, 1.0, 0.0]           1  [0.463, 0.823, 0.515]   \n",
       " 42    200  [0.0, 1.0, 0.0]           2  [0.435, 0.698, 0.517]   \n",
       " 43    200  [0.0, 1.0, 0.0]           2  [0.264, 0.777, 0.654]   \n",
       " 44    200  [0.0, 1.0, 0.0]           2  [0.346, 0.525, 0.505]   \n",
       " 45    200  [0.0, 1.0, 0.0]           2  [0.288, 0.509, 0.633]   \n",
       " 46    200  [0.0, 1.0, 0.0]           2  [0.405, 0.804, 0.527]   \n",
       " 47    200  [0.0, 1.0, 0.0]           2  [0.341, 0.617, 0.564]   \n",
       " 48    200  [0.0, 1.0, 0.0]           2  [0.406, 0.757, 0.523]   \n",
       " 49    200  [0.0, 1.0, 0.0]           2   [0.41, 0.679, 0.501]   \n",
       " 50    200  [0.0, 1.0, 0.0]           1  [0.568, 0.812, 0.408]   \n",
       " 51    200  [0.0, 0.0, 1.0]           2  [0.194, 0.717, 0.705]   \n",
       " 52    200  [0.0, 0.0, 1.0]           2  [0.103, 0.597, 0.847]   \n",
       " 53    200  [0.0, 0.0, 1.0]           2    [0.145, 0.79, 0.82]   \n",
       " 54    200  [0.0, 0.0, 1.0]           2   [0.07, 0.463, 0.797]   \n",
       " 55    200  [0.0, 0.0, 1.0]           2  [0.195, 0.648, 0.706]   \n",
       " 56    200  [0.0, 0.0, 1.0]           2      [0.0, 0.718, 1.0]   \n",
       " 57    200  [0.0, 0.0, 1.0]           2  [0.261, 0.918, 0.717]   \n",
       " 58    200  [0.0, 0.0, 1.0]           2  [0.235, 0.742, 0.699]   \n",
       " 59    200  [0.0, 0.0, 1.0]           2  [0.148, 0.568, 0.725]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     210  [1.0, 0.0, 0.0]           0  [0.929, 0.313, 0.073]   \n",
       " 1     210  [0.0, 0.0, 1.0]           2  [0.163, 0.544, 0.775]   \n",
       " 2     210  [1.0, 0.0, 0.0]           0  [0.939, 0.427, 0.051]   \n",
       " 3     210  [0.0, 0.0, 1.0]           2  [0.269, 0.695, 0.614]   \n",
       " 4     210  [1.0, 0.0, 0.0]           0  [0.935, 0.498, 0.103]   \n",
       " 5     210  [1.0, 0.0, 0.0]           0  [0.992, 0.557, 0.044]   \n",
       " 6     210  [1.0, 0.0, 0.0]           0   [0.93, 0.061, 0.061]   \n",
       " 7     210  [1.0, 0.0, 0.0]           0    [0.901, 0.0, 0.033]   \n",
       " 8     210  [0.0, 0.0, 1.0]           2     [0.142, 0.6, 0.77]   \n",
       " 9     210  [0.0, 0.0, 1.0]           2  [0.229, 0.848, 0.669]   \n",
       " 10    210  [0.0, 0.0, 1.0]           2  [0.103, 0.627, 0.787]   \n",
       " 11    210  [0.0, 0.0, 1.0]           2  [0.101, 0.623, 0.773]   \n",
       " 12    210  [0.0, 0.0, 1.0]           2   [0.141, 0.667, 0.77]   \n",
       " 13    210  [1.0, 0.0, 0.0]           0   [0.899, 0.28, 0.071]   \n",
       " 14    210  [1.0, 0.0, 0.0]           0      [1.0, 0.334, 0.0]   \n",
       " 15    210  [0.0, 0.0, 1.0]           2  [0.187, 0.663, 0.714]   \n",
       " 16    210  [0.0, 0.0, 1.0]           2  [0.252, 0.727, 0.619]   \n",
       " 17    210  [1.0, 0.0, 0.0]           0   [0.888, 0.49, 0.141]   \n",
       " 18    210  [1.0, 0.0, 0.0]           0  [0.914, 0.303, 0.087]   \n",
       " 19    210  [1.0, 0.0, 0.0]           0  [0.861, 0.326, 0.139]   \n",
       " 20    210  [1.0, 0.0, 0.0]           0  [0.965, 0.119, 0.001]   \n",
       " 21    210  [1.0, 0.0, 0.0]           0  [0.935, 0.407, 0.095]   \n",
       " 22    210  [1.0, 0.0, 0.0]           0   [0.912, 0.25, 0.105]   \n",
       " 23    210  [1.0, 0.0, 0.0]           0  [0.953, 0.565, 0.071]   \n",
       " 24    210  [1.0, 0.0, 0.0]           0  [0.914, 0.348, 0.091]   \n",
       " 25    210  [1.0, 0.0, 0.0]           0  [0.928, 0.333, 0.069]   \n",
       " 26    210  [1.0, 0.0, 0.0]           0  [0.889, 0.781, 0.189]   \n",
       " 27    210  [1.0, 0.0, 0.0]           0  [0.851, 0.381, 0.113]   \n",
       " 28    210  [1.0, 0.0, 0.0]           0   [0.919, 0.23, 0.069]   \n",
       " 29    210  [1.0, 0.0, 0.0]           0   [0.924, 0.387, 0.09]   \n",
       " 30    210  [0.0, 1.0, 0.0]           2   [0.375, 0.73, 0.535]   \n",
       " 31    210  [0.0, 1.0, 0.0]           2  [0.304, 0.527, 0.568]   \n",
       " 32    210  [0.0, 1.0, 0.0]           1  [0.561, 0.879, 0.406]   \n",
       " 33    210  [0.0, 1.0, 0.0]           2  [0.317, 0.567, 0.625]   \n",
       " 34    210  [0.0, 1.0, 0.0]           1    [0.511, 1.0, 0.494]   \n",
       " 35    210  [0.0, 1.0, 0.0]           2   [0.323, 0.49, 0.604]   \n",
       " 36    210  [0.0, 1.0, 0.0]           2  [0.361, 0.705, 0.515]   \n",
       " 37    210  [0.0, 1.0, 0.0]           2   [0.285, 0.647, 0.57]   \n",
       " 38    210  [0.0, 1.0, 0.0]           2  [0.354, 0.581, 0.586]   \n",
       " 39    210  [0.0, 1.0, 0.0]           2  [0.331, 0.676, 0.577]   \n",
       " 40    210  [0.0, 1.0, 0.0]           2    [0.5, 0.704, 0.474]   \n",
       " 41    210  [0.0, 1.0, 0.0]           1  [0.458, 0.826, 0.514]   \n",
       " 42    210  [0.0, 1.0, 0.0]           2   [0.43, 0.702, 0.516]   \n",
       " 43    210  [0.0, 1.0, 0.0]           2   [0.26, 0.783, 0.655]   \n",
       " 44    210  [0.0, 1.0, 0.0]           2   [0.344, 0.53, 0.506]   \n",
       " 45    210  [0.0, 1.0, 0.0]           2  [0.285, 0.518, 0.633]   \n",
       " 46    210  [0.0, 1.0, 0.0]           2  [0.401, 0.807, 0.528]   \n",
       " 47    210  [0.0, 1.0, 0.0]           2  [0.338, 0.623, 0.565]   \n",
       " 48    210  [0.0, 1.0, 0.0]           2  [0.401, 0.761, 0.523]   \n",
       " 49    210  [0.0, 1.0, 0.0]           2  [0.406, 0.682, 0.501]   \n",
       " 50    210  [0.0, 1.0, 0.0]           1  [0.564, 0.812, 0.407]   \n",
       " 51    210  [0.0, 0.0, 1.0]           2  [0.191, 0.725, 0.707]   \n",
       " 52    210  [0.0, 0.0, 1.0]           2   [0.101, 0.61, 0.847]   \n",
       " 53    210  [0.0, 0.0, 1.0]           2  [0.141, 0.801, 0.821]   \n",
       " 54    210  [0.0, 0.0, 1.0]           2     [0.07, 0.475, 0.8]   \n",
       " 55    210  [0.0, 0.0, 1.0]           2  [0.192, 0.657, 0.707]   \n",
       " 56    210  [0.0, 0.0, 1.0]           2      [0.0, 0.734, 1.0]   \n",
       " 57    210  [0.0, 0.0, 1.0]           2  [0.256, 0.924, 0.718]   \n",
       " 58    210  [0.0, 0.0, 1.0]           2     [0.23, 0.749, 0.7]   \n",
       " 59    210  [0.0, 0.0, 1.0]           2  [0.146, 0.577, 0.727]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     220  [1.0, 0.0, 0.0]           0   [0.929, 0.31, 0.072]   \n",
       " 1     220  [0.0, 0.0, 1.0]           2   [0.16, 0.555, 0.776]   \n",
       " 2     220  [1.0, 0.0, 0.0]           0  [0.939, 0.423, 0.051]   \n",
       " 3     220  [0.0, 0.0, 1.0]           2  [0.266, 0.699, 0.615]   \n",
       " 4     220  [1.0, 0.0, 0.0]           0  [0.934, 0.495, 0.101]   \n",
       " 5     220  [1.0, 0.0, 0.0]           0  [0.991, 0.552, 0.043]   \n",
       " 6     220  [1.0, 0.0, 0.0]           0   [0.932, 0.062, 0.06]   \n",
       " 7     220  [1.0, 0.0, 0.0]           0    [0.904, 0.0, 0.034]   \n",
       " 8     220  [0.0, 0.0, 1.0]           2   [0.14, 0.608, 0.772]   \n",
       " 9     220  [0.0, 0.0, 1.0]           2  [0.226, 0.852, 0.671]   \n",
       " 10    220  [0.0, 0.0, 1.0]           2  [0.102, 0.636, 0.789]   \n",
       " 11    220  [0.0, 0.0, 1.0]           2     [0.1, 0.63, 0.776]   \n",
       " 12    220  [0.0, 0.0, 1.0]           2  [0.139, 0.675, 0.772]   \n",
       " 13    220  [1.0, 0.0, 0.0]           0     [0.9, 0.277, 0.07]   \n",
       " 14    220  [1.0, 0.0, 0.0]           0      [1.0, 0.329, 0.0]   \n",
       " 15    220  [0.0, 0.0, 1.0]           2   [0.184, 0.67, 0.716]   \n",
       " 16    220  [0.0, 0.0, 1.0]           2  [0.249, 0.731, 0.622]   \n",
       " 17    220  [1.0, 0.0, 0.0]           0  [0.887, 0.488, 0.139]   \n",
       " 18    220  [1.0, 0.0, 0.0]           0  [0.914, 0.301, 0.085]   \n",
       " 19    220  [1.0, 0.0, 0.0]           0  [0.861, 0.325, 0.138]   \n",
       " 20    220  [1.0, 0.0, 0.0]           0  [0.967, 0.117, 0.001]   \n",
       " 21    220  [1.0, 0.0, 0.0]           0  [0.934, 0.405, 0.093]   \n",
       " 22    220  [1.0, 0.0, 0.0]           0  [0.912, 0.249, 0.103]   \n",
       " 23    220  [1.0, 0.0, 0.0]           0   [0.952, 0.559, 0.07]   \n",
       " 24    220  [1.0, 0.0, 0.0]           0   [0.914, 0.346, 0.09]   \n",
       " 25    220  [1.0, 0.0, 0.0]           0   [0.928, 0.33, 0.068]   \n",
       " 26    220  [1.0, 0.0, 0.0]           0  [0.886, 0.777, 0.186]   \n",
       " 27    220  [1.0, 0.0, 0.0]           0  [0.851, 0.378, 0.112]   \n",
       " 28    220  [1.0, 0.0, 0.0]           0   [0.92, 0.228, 0.068]   \n",
       " 29    220  [1.0, 0.0, 0.0]           0  [0.923, 0.385, 0.089]   \n",
       " 30    220  [0.0, 1.0, 0.0]           2  [0.371, 0.733, 0.536]   \n",
       " 31    220  [0.0, 1.0, 0.0]           2   [0.302, 0.532, 0.57]   \n",
       " 32    220  [0.0, 1.0, 0.0]           1  [0.556, 0.878, 0.405]   \n",
       " 33    220  [0.0, 1.0, 0.0]           2  [0.314, 0.574, 0.625]   \n",
       " 34    220  [0.0, 1.0, 0.0]           1    [0.505, 1.0, 0.493]   \n",
       " 35    220  [0.0, 1.0, 0.0]           2   [0.32, 0.497, 0.605]   \n",
       " 36    220  [0.0, 1.0, 0.0]           2  [0.358, 0.708, 0.516]   \n",
       " 37    220  [0.0, 1.0, 0.0]           2  [0.282, 0.651, 0.572]   \n",
       " 38    220  [0.0, 1.0, 0.0]           2   [0.35, 0.587, 0.586]   \n",
       " 39    220  [0.0, 1.0, 0.0]           2   [0.328, 0.68, 0.578]   \n",
       " 40    220  [0.0, 1.0, 0.0]           1  [0.496, 0.707, 0.473]   \n",
       " 41    220  [0.0, 1.0, 0.0]           1  [0.453, 0.828, 0.513]   \n",
       " 42    220  [0.0, 1.0, 0.0]           2  [0.426, 0.705, 0.516]   \n",
       " 43    220  [0.0, 1.0, 0.0]           2  [0.256, 0.788, 0.656]   \n",
       " 44    220  [0.0, 1.0, 0.0]           2  [0.341, 0.534, 0.508]   \n",
       " 45    220  [0.0, 1.0, 0.0]           2  [0.282, 0.525, 0.634]   \n",
       " 46    220  [0.0, 1.0, 0.0]           2   [0.396, 0.81, 0.528]   \n",
       " 47    220  [0.0, 1.0, 0.0]           2  [0.335, 0.628, 0.566]   \n",
       " 48    220  [0.0, 1.0, 0.0]           2  [0.397, 0.763, 0.523]   \n",
       " 49    220  [0.0, 1.0, 0.0]           2  [0.403, 0.685, 0.502]   \n",
       " 50    220  [0.0, 1.0, 0.0]           1  [0.559, 0.811, 0.406]   \n",
       " 51    220  [0.0, 0.0, 1.0]           2  [0.188, 0.731, 0.709]   \n",
       " 52    220  [0.0, 0.0, 1.0]           2  [0.099, 0.622, 0.848]   \n",
       " 53    220  [0.0, 0.0, 1.0]           2   [0.138, 0.81, 0.821]   \n",
       " 54    220  [0.0, 0.0, 1.0]           2   [0.07, 0.485, 0.804]   \n",
       " 55    220  [0.0, 0.0, 1.0]           2   [0.19, 0.664, 0.709]   \n",
       " 56    220  [0.0, 0.0, 1.0]           2      [0.0, 0.748, 1.0]   \n",
       " 57    220  [0.0, 0.0, 1.0]           2   [0.251, 0.93, 0.718]   \n",
       " 58    220  [0.0, 0.0, 1.0]           2  [0.227, 0.755, 0.702]   \n",
       " 59    220  [0.0, 0.0, 1.0]           2   [0.144, 0.585, 0.73]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     230  [1.0, 0.0, 0.0]           0  [0.929, 0.308, 0.071]   \n",
       " 1     230  [0.0, 0.0, 1.0]           2  [0.158, 0.565, 0.776]   \n",
       " 2     230  [1.0, 0.0, 0.0]           0   [0.939, 0.418, 0.05]   \n",
       " 3     230  [0.0, 0.0, 1.0]           2  [0.263, 0.704, 0.617]   \n",
       " 4     230  [1.0, 0.0, 0.0]           0  [0.933, 0.492, 0.099]   \n",
       " 5     230  [1.0, 0.0, 0.0]           0  [0.989, 0.546, 0.042]   \n",
       " 6     230  [1.0, 0.0, 0.0]           0  [0.933, 0.062, 0.059]   \n",
       " 7     230  [1.0, 0.0, 0.0]           0    [0.907, 0.0, 0.034]   \n",
       " 8     230  [0.0, 0.0, 1.0]           2  [0.138, 0.616, 0.775]   \n",
       " 9     230  [0.0, 0.0, 1.0]           2  [0.222, 0.856, 0.673]   \n",
       " 10    230  [0.0, 0.0, 1.0]           2    [0.1, 0.644, 0.792]   \n",
       " 11    230  [0.0, 0.0, 1.0]           2   [0.099, 0.638, 0.78]   \n",
       " 12    230  [0.0, 0.0, 1.0]           2  [0.136, 0.682, 0.774]   \n",
       " 13    230  [1.0, 0.0, 0.0]           0   [0.901, 0.275, 0.07]   \n",
       " 14    230  [1.0, 0.0, 0.0]           0      [1.0, 0.325, 0.0]   \n",
       " 15    230  [0.0, 0.0, 1.0]           2  [0.182, 0.677, 0.718]   \n",
       " 16    230  [0.0, 0.0, 1.0]           2  [0.246, 0.735, 0.623]   \n",
       " 17    230  [1.0, 0.0, 0.0]           0  [0.886, 0.486, 0.137]   \n",
       " 18    230  [1.0, 0.0, 0.0]           0    [0.914, 0.3, 0.084]   \n",
       " 19    230  [1.0, 0.0, 0.0]           0  [0.861, 0.324, 0.137]   \n",
       " 20    230  [1.0, 0.0, 0.0]           0  [0.968, 0.115, 0.001]   \n",
       " 21    230  [1.0, 0.0, 0.0]           0  [0.934, 0.402, 0.091]   \n",
       " 22    230  [1.0, 0.0, 0.0]           0  [0.912, 0.249, 0.102]   \n",
       " 23    230  [1.0, 0.0, 0.0]           0  [0.951, 0.554, 0.069]   \n",
       " 24    230  [1.0, 0.0, 0.0]           0  [0.914, 0.343, 0.088]   \n",
       " 25    230  [1.0, 0.0, 0.0]           0  [0.928, 0.327, 0.067]   \n",
       " 26    230  [1.0, 0.0, 0.0]           0  [0.883, 0.774, 0.184]   \n",
       " 27    230  [1.0, 0.0, 0.0]           0  [0.851, 0.376, 0.112]   \n",
       " 28    230  [1.0, 0.0, 0.0]           0  [0.921, 0.227, 0.067]   \n",
       " 29    230  [1.0, 0.0, 0.0]           0  [0.923, 0.382, 0.088]   \n",
       " 30    230  [0.0, 1.0, 0.0]           2  [0.368, 0.737, 0.536]   \n",
       " 31    230  [0.0, 1.0, 0.0]           2  [0.299, 0.537, 0.571]   \n",
       " 32    230  [0.0, 1.0, 0.0]           1  [0.552, 0.877, 0.404]   \n",
       " 33    230  [0.0, 1.0, 0.0]           2  [0.311, 0.581, 0.625]   \n",
       " 34    230  [0.0, 1.0, 0.0]           1    [0.499, 1.0, 0.492]   \n",
       " 35    230  [0.0, 1.0, 0.0]           2  [0.317, 0.504, 0.605]   \n",
       " 36    230  [0.0, 1.0, 0.0]           2   [0.355, 0.71, 0.518]   \n",
       " 37    230  [0.0, 1.0, 0.0]           2   [0.28, 0.654, 0.574]   \n",
       " 38    230  [0.0, 1.0, 0.0]           2  [0.347, 0.593, 0.586]   \n",
       " 39    230  [0.0, 1.0, 0.0]           2  [0.324, 0.684, 0.578]   \n",
       " 40    230  [0.0, 1.0, 0.0]           1  [0.492, 0.709, 0.472]   \n",
       " 41    230  [0.0, 1.0, 0.0]           1   [0.448, 0.83, 0.513]   \n",
       " 42    230  [0.0, 1.0, 0.0]           2  [0.422, 0.709, 0.516]   \n",
       " 43    230  [0.0, 1.0, 0.0]           2  [0.252, 0.793, 0.657]   \n",
       " 44    230  [0.0, 1.0, 0.0]           2   [0.339, 0.537, 0.51]   \n",
       " 45    230  [0.0, 1.0, 0.0]           2  [0.279, 0.532, 0.635]   \n",
       " 46    230  [0.0, 1.0, 0.0]           1  [0.392, 0.812, 0.528]   \n",
       " 47    230  [0.0, 1.0, 0.0]           2  [0.331, 0.632, 0.566]   \n",
       " 48    230  [0.0, 1.0, 0.0]           2  [0.393, 0.766, 0.524]   \n",
       " 49    230  [0.0, 1.0, 0.0]           2  [0.399, 0.688, 0.502]   \n",
       " 50    230  [0.0, 1.0, 0.0]           1  [0.555, 0.811, 0.405]   \n",
       " 51    230  [0.0, 0.0, 1.0]           2  [0.185, 0.738, 0.711]   \n",
       " 52    230  [0.0, 0.0, 1.0]           2  [0.098, 0.633, 0.849]   \n",
       " 53    230  [0.0, 0.0, 1.0]           2  [0.135, 0.818, 0.822]   \n",
       " 54    230  [0.0, 0.0, 1.0]           2   [0.07, 0.495, 0.807]   \n",
       " 55    230  [0.0, 0.0, 1.0]           2  [0.187, 0.671, 0.711]   \n",
       " 56    230  [0.0, 0.0, 1.0]           2      [0.0, 0.762, 1.0]   \n",
       " 57    230  [0.0, 0.0, 1.0]           2  [0.246, 0.935, 0.718]   \n",
       " 58    230  [0.0, 0.0, 1.0]           2  [0.223, 0.762, 0.703]   \n",
       " 59    230  [0.0, 0.0, 1.0]           2  [0.143, 0.593, 0.733]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     240  [1.0, 0.0, 0.0]           0   [0.929, 0.306, 0.07]   \n",
       " 1     240  [0.0, 0.0, 1.0]           2  [0.156, 0.574, 0.777]   \n",
       " 2     240  [1.0, 0.0, 0.0]           0   [0.939, 0.414, 0.05]   \n",
       " 3     240  [0.0, 0.0, 1.0]           2  [0.259, 0.708, 0.619]   \n",
       " 4     240  [1.0, 0.0, 0.0]           0  [0.932, 0.489, 0.097]   \n",
       " 5     240  [1.0, 0.0, 0.0]           0  [0.988, 0.541, 0.041]   \n",
       " 6     240  [1.0, 0.0, 0.0]           0  [0.935, 0.062, 0.058]   \n",
       " 7     240  [1.0, 0.0, 0.0]           0     [0.91, 0.0, 0.034]   \n",
       " 8     240  [0.0, 0.0, 1.0]           2  [0.136, 0.624, 0.777]   \n",
       " 9     240  [0.0, 0.0, 1.0]           2   [0.218, 0.86, 0.675]   \n",
       " 10    240  [0.0, 0.0, 1.0]           2  [0.099, 0.652, 0.795]   \n",
       " 11    240  [0.0, 0.0, 1.0]           2  [0.097, 0.645, 0.783]   \n",
       " 12    240  [0.0, 0.0, 1.0]           2  [0.134, 0.689, 0.777]   \n",
       " 13    240  [1.0, 0.0, 0.0]           0  [0.901, 0.273, 0.069]   \n",
       " 14    240  [1.0, 0.0, 0.0]           0       [1.0, 0.32, 0.0]   \n",
       " 15    240  [0.0, 0.0, 1.0]           2   [0.179, 0.683, 0.72]   \n",
       " 16    240  [0.0, 0.0, 1.0]           2  [0.243, 0.739, 0.625]   \n",
       " 17    240  [1.0, 0.0, 0.0]           0  [0.885, 0.484, 0.136]   \n",
       " 18    240  [1.0, 0.0, 0.0]           0  [0.914, 0.298, 0.083]   \n",
       " 19    240  [1.0, 0.0, 0.0]           0  [0.861, 0.323, 0.135]   \n",
       " 20    240  [1.0, 0.0, 0.0]           0   [0.97, 0.113, 0.001]   \n",
       " 21    240  [1.0, 0.0, 0.0]           0     [0.933, 0.4, 0.09]   \n",
       " 22    240  [1.0, 0.0, 0.0]           0    [0.912, 0.248, 0.1]   \n",
       " 23    240  [1.0, 0.0, 0.0]           0   [0.949, 0.55, 0.068]   \n",
       " 24    240  [1.0, 0.0, 0.0]           0  [0.914, 0.341, 0.087]   \n",
       " 25    240  [1.0, 0.0, 0.0]           0  [0.928, 0.324, 0.067]   \n",
       " 26    240  [1.0, 0.0, 0.0]           0    [0.88, 0.77, 0.181]   \n",
       " 27    240  [1.0, 0.0, 0.0]           0  [0.851, 0.373, 0.111]   \n",
       " 28    240  [1.0, 0.0, 0.0]           0  [0.921, 0.226, 0.066]   \n",
       " 29    240  [1.0, 0.0, 0.0]           0   [0.923, 0.38, 0.086]   \n",
       " 30    240  [0.0, 1.0, 0.0]           2  [0.364, 0.739, 0.536]   \n",
       " 31    240  [0.0, 1.0, 0.0]           2  [0.297, 0.542, 0.572]   \n",
       " 32    240  [0.0, 1.0, 0.0]           1  [0.548, 0.876, 0.404]   \n",
       " 33    240  [0.0, 1.0, 0.0]           2  [0.307, 0.587, 0.625]   \n",
       " 34    240  [0.0, 1.0, 0.0]           1    [0.493, 1.0, 0.491]   \n",
       " 35    240  [0.0, 1.0, 0.0]           2   [0.314, 0.51, 0.605]   \n",
       " 36    240  [0.0, 1.0, 0.0]           2  [0.351, 0.712, 0.519]   \n",
       " 37    240  [0.0, 1.0, 0.0]           2  [0.277, 0.658, 0.576]   \n",
       " 38    240  [0.0, 1.0, 0.0]           2  [0.343, 0.598, 0.586]   \n",
       " 39    240  [0.0, 1.0, 0.0]           2  [0.321, 0.688, 0.579]   \n",
       " 40    240  [0.0, 1.0, 0.0]           1  [0.488, 0.712, 0.471]   \n",
       " 41    240  [0.0, 1.0, 0.0]           1  [0.443, 0.832, 0.512]   \n",
       " 42    240  [0.0, 1.0, 0.0]           2  [0.419, 0.712, 0.515]   \n",
       " 43    240  [0.0, 1.0, 0.0]           2  [0.248, 0.798, 0.658]   \n",
       " 44    240  [0.0, 1.0, 0.0]           2  [0.337, 0.541, 0.511]   \n",
       " 45    240  [0.0, 1.0, 0.0]           2  [0.276, 0.538, 0.635]   \n",
       " 46    240  [0.0, 1.0, 0.0]           1  [0.388, 0.814, 0.528]   \n",
       " 47    240  [0.0, 1.0, 0.0]           2  [0.328, 0.637, 0.567]   \n",
       " 48    240  [0.0, 1.0, 0.0]           2  [0.389, 0.768, 0.524]   \n",
       " 49    240  [0.0, 1.0, 0.0]           2   [0.396, 0.69, 0.503]   \n",
       " 50    240  [0.0, 1.0, 0.0]           1  [0.551, 0.811, 0.405]   \n",
       " 51    240  [0.0, 0.0, 1.0]           2  [0.182, 0.743, 0.712]   \n",
       " 52    240  [0.0, 0.0, 1.0]           2   [0.096, 0.643, 0.85]   \n",
       " 53    240  [0.0, 0.0, 1.0]           2  [0.132, 0.826, 0.823]   \n",
       " 54    240  [0.0, 0.0, 1.0]           2   [0.069, 0.503, 0.81]   \n",
       " 55    240  [0.0, 0.0, 1.0]           2  [0.184, 0.678, 0.712]   \n",
       " 56    240  [0.0, 0.0, 1.0]           2      [0.0, 0.774, 1.0]   \n",
       " 57    240  [0.0, 0.0, 1.0]           2   [0.241, 0.94, 0.718]   \n",
       " 58    240  [0.0, 0.0, 1.0]           2  [0.219, 0.767, 0.704]   \n",
       " 59    240  [0.0, 0.0, 1.0]           2    [0.141, 0.6, 0.735]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     250  [1.0, 0.0, 0.0]           0   [0.93, 0.304, 0.069]   \n",
       " 1     250  [0.0, 0.0, 1.0]           2  [0.153, 0.582, 0.778]   \n",
       " 2     250  [1.0, 0.0, 0.0]           0  [0.938, 0.411, 0.049]   \n",
       " 3     250  [0.0, 0.0, 1.0]           2  [0.256, 0.711, 0.621]   \n",
       " 4     250  [1.0, 0.0, 0.0]           0  [0.931, 0.487, 0.096]   \n",
       " 5     250  [1.0, 0.0, 0.0]           0   [0.987, 0.537, 0.04]   \n",
       " 6     250  [1.0, 0.0, 0.0]           0  [0.936, 0.063, 0.057]   \n",
       " 7     250  [1.0, 0.0, 0.0]           0    [0.912, 0.0, 0.034]   \n",
       " 8     250  [0.0, 0.0, 1.0]           2   [0.133, 0.63, 0.779]   \n",
       " 9     250  [0.0, 0.0, 1.0]           2  [0.214, 0.863, 0.677]   \n",
       " 10    250  [0.0, 0.0, 1.0]           2  [0.097, 0.658, 0.797]   \n",
       " 11    250  [0.0, 0.0, 1.0]           2  [0.096, 0.651, 0.786]   \n",
       " 12    250  [0.0, 0.0, 1.0]           2  [0.132, 0.695, 0.779]   \n",
       " 13    250  [1.0, 0.0, 0.0]           0  [0.902, 0.271, 0.069]   \n",
       " 14    250  [1.0, 0.0, 0.0]           0      [1.0, 0.316, 0.0]   \n",
       " 15    250  [0.0, 0.0, 1.0]           2  [0.176, 0.688, 0.722]   \n",
       " 16    250  [0.0, 0.0, 1.0]           2   [0.24, 0.742, 0.627]   \n",
       " 17    250  [1.0, 0.0, 0.0]           0  [0.884, 0.482, 0.134]   \n",
       " 18    250  [1.0, 0.0, 0.0]           0  [0.915, 0.297, 0.082]   \n",
       " 19    250  [1.0, 0.0, 0.0]           0  [0.861, 0.322, 0.134]   \n",
       " 20    250  [1.0, 0.0, 0.0]           0  [0.971, 0.111, 0.001]   \n",
       " 21    250  [1.0, 0.0, 0.0]           0  [0.932, 0.398, 0.089]   \n",
       " 22    250  [1.0, 0.0, 0.0]           0  [0.912, 0.248, 0.099]   \n",
       " 23    250  [1.0, 0.0, 0.0]           0  [0.948, 0.546, 0.067]   \n",
       " 24    250  [1.0, 0.0, 0.0]           0   [0.914, 0.34, 0.086]   \n",
       " 25    250  [1.0, 0.0, 0.0]           0  [0.928, 0.322, 0.066]   \n",
       " 26    250  [1.0, 0.0, 0.0]           0  [0.877, 0.767, 0.179]   \n",
       " 27    250  [1.0, 0.0, 0.0]           0  [0.852, 0.371, 0.111]   \n",
       " 28    250  [1.0, 0.0, 0.0]           0  [0.922, 0.224, 0.066]   \n",
       " 29    250  [1.0, 0.0, 0.0]           0  [0.922, 0.378, 0.085]   \n",
       " 30    250  [0.0, 1.0, 0.0]           2   [0.36, 0.742, 0.537]   \n",
       " 31    250  [0.0, 1.0, 0.0]           2  [0.294, 0.546, 0.574]   \n",
       " 32    250  [0.0, 1.0, 0.0]           1  [0.544, 0.875, 0.403]   \n",
       " 33    250  [0.0, 1.0, 0.0]           2  [0.304, 0.593, 0.625]   \n",
       " 34    250  [0.0, 1.0, 0.0]           1     [0.488, 1.0, 0.49]   \n",
       " 35    250  [0.0, 1.0, 0.0]           2  [0.311, 0.516, 0.606]   \n",
       " 36    250  [0.0, 1.0, 0.0]           2   [0.348, 0.714, 0.52]   \n",
       " 37    250  [0.0, 1.0, 0.0]           2   [0.274, 0.66, 0.578]   \n",
       " 38    250  [0.0, 1.0, 0.0]           2   [0.34, 0.603, 0.586]   \n",
       " 39    250  [0.0, 1.0, 0.0]           2   [0.317, 0.691, 0.58]   \n",
       " 40    250  [0.0, 1.0, 0.0]           1   [0.484, 0.714, 0.47]   \n",
       " 41    250  [0.0, 1.0, 0.0]           1  [0.439, 0.834, 0.512]   \n",
       " 42    250  [0.0, 1.0, 0.0]           2  [0.415, 0.714, 0.515]   \n",
       " 43    250  [0.0, 1.0, 0.0]           2   [0.245, 0.802, 0.66]   \n",
       " 44    250  [0.0, 1.0, 0.0]           2  [0.334, 0.543, 0.513]   \n",
       " 45    250  [0.0, 1.0, 0.0]           2  [0.273, 0.544, 0.636]   \n",
       " 46    250  [0.0, 1.0, 0.0]           1  [0.384, 0.816, 0.528]   \n",
       " 47    250  [0.0, 1.0, 0.0]           2   [0.325, 0.64, 0.568]   \n",
       " 48    250  [0.0, 1.0, 0.0]           2   [0.385, 0.77, 0.524]   \n",
       " 49    250  [0.0, 1.0, 0.0]           2  [0.393, 0.692, 0.503]   \n",
       " 50    250  [0.0, 1.0, 0.0]           1   [0.547, 0.81, 0.404]   \n",
       " 51    250  [0.0, 0.0, 1.0]           2  [0.179, 0.748, 0.714]   \n",
       " 52    250  [0.0, 0.0, 1.0]           2  [0.094, 0.652, 0.851]   \n",
       " 53    250  [0.0, 0.0, 1.0]           2  [0.129, 0.833, 0.824]   \n",
       " 54    250  [0.0, 0.0, 1.0]           2  [0.069, 0.511, 0.813]   \n",
       " 55    250  [0.0, 0.0, 1.0]           2  [0.181, 0.683, 0.714]   \n",
       " 56    250  [0.0, 0.0, 1.0]           2      [0.0, 0.785, 1.0]   \n",
       " 57    250  [0.0, 0.0, 1.0]           2  [0.236, 0.944, 0.718]   \n",
       " 58    250  [0.0, 0.0, 1.0]           2  [0.216, 0.772, 0.705]   \n",
       " 59    250  [0.0, 0.0, 1.0]           2  [0.139, 0.606, 0.738]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     260  [1.0, 0.0, 0.0]           0   [0.93, 0.302, 0.068]   \n",
       " 1     260  [0.0, 0.0, 1.0]           2  [0.151, 0.591, 0.778]   \n",
       " 2     260  [1.0, 0.0, 0.0]           0  [0.938, 0.407, 0.049]   \n",
       " 3     260  [0.0, 0.0, 1.0]           2  [0.253, 0.714, 0.623]   \n",
       " 4     260  [1.0, 0.0, 0.0]           0   [0.93, 0.484, 0.094]   \n",
       " 5     260  [1.0, 0.0, 0.0]           0  [0.986, 0.532, 0.039]   \n",
       " 6     260  [1.0, 0.0, 0.0]           0  [0.937, 0.063, 0.056]   \n",
       " 7     260  [1.0, 0.0, 0.0]           0    [0.914, 0.0, 0.034]   \n",
       " 8     260  [0.0, 0.0, 1.0]           2  [0.131, 0.637, 0.781]   \n",
       " 9     260  [0.0, 0.0, 1.0]           2  [0.211, 0.866, 0.678]   \n",
       " 10    260  [0.0, 0.0, 1.0]           2    [0.096, 0.664, 0.8]   \n",
       " 11    260  [0.0, 0.0, 1.0]           2  [0.095, 0.657, 0.789]   \n",
       " 12    260  [0.0, 0.0, 1.0]           2  [0.129, 0.701, 0.781]   \n",
       " 13    260  [1.0, 0.0, 0.0]           0  [0.903, 0.269, 0.069]   \n",
       " 14    260  [1.0, 0.0, 0.0]           0      [1.0, 0.313, 0.0]   \n",
       " 15    260  [0.0, 0.0, 1.0]           2  [0.173, 0.694, 0.724]   \n",
       " 16    260  [0.0, 0.0, 1.0]           2  [0.237, 0.745, 0.629]   \n",
       " 17    260  [1.0, 0.0, 0.0]           0  [0.883, 0.481, 0.132]   \n",
       " 18    260  [1.0, 0.0, 0.0]           0  [0.915, 0.295, 0.081]   \n",
       " 19    260  [1.0, 0.0, 0.0]           0  [0.861, 0.322, 0.133]   \n",
       " 20    260  [1.0, 0.0, 0.0]           0   [0.973, 0.11, 0.002]   \n",
       " 21    260  [1.0, 0.0, 0.0]           0  [0.932, 0.396, 0.087]   \n",
       " 22    260  [1.0, 0.0, 0.0]           0  [0.913, 0.247, 0.097]   \n",
       " 23    260  [1.0, 0.0, 0.0]           0  [0.947, 0.542, 0.066]   \n",
       " 24    260  [1.0, 0.0, 0.0]           0  [0.914, 0.338, 0.085]   \n",
       " 25    260  [1.0, 0.0, 0.0]           0   [0.928, 0.32, 0.065]   \n",
       " 26    260  [1.0, 0.0, 0.0]           0  [0.874, 0.764, 0.177]   \n",
       " 27    260  [1.0, 0.0, 0.0]           0  [0.852, 0.369, 0.111]   \n",
       " 28    260  [1.0, 0.0, 0.0]           0  [0.923, 0.223, 0.065]   \n",
       " 29    260  [1.0, 0.0, 0.0]           0  [0.922, 0.376, 0.084]   \n",
       " 30    260  [0.0, 1.0, 0.0]           2  [0.357, 0.744, 0.538]   \n",
       " 31    260  [0.0, 1.0, 0.0]           2   [0.291, 0.55, 0.575]   \n",
       " 32    260  [0.0, 1.0, 0.0]           1   [0.54, 0.875, 0.402]   \n",
       " 33    260  [0.0, 1.0, 0.0]           2  [0.301, 0.598, 0.625]   \n",
       " 34    260  [0.0, 1.0, 0.0]           1    [0.483, 1.0, 0.489]   \n",
       " 35    260  [0.0, 1.0, 0.0]           2  [0.308, 0.521, 0.606]   \n",
       " 36    260  [0.0, 1.0, 0.0]           2  [0.345, 0.716, 0.521]   \n",
       " 37    260  [0.0, 1.0, 0.0]           2   [0.271, 0.663, 0.58]   \n",
       " 38    260  [0.0, 1.0, 0.0]           2  [0.337, 0.607, 0.586]   \n",
       " 39    260  [0.0, 1.0, 0.0]           2  [0.314, 0.695, 0.581]   \n",
       " 40    260  [0.0, 1.0, 0.0]           1   [0.481, 0.716, 0.47]   \n",
       " 41    260  [0.0, 1.0, 0.0]           1  [0.435, 0.836, 0.511]   \n",
       " 42    260  [0.0, 1.0, 0.0]           2  [0.411, 0.717, 0.515]   \n",
       " 43    260  [0.0, 1.0, 0.0]           2  [0.241, 0.805, 0.661]   \n",
       " 44    260  [0.0, 1.0, 0.0]           2  [0.332, 0.546, 0.514]   \n",
       " 45    260  [0.0, 1.0, 0.0]           2    [0.27, 0.55, 0.637]   \n",
       " 46    260  [0.0, 1.0, 0.0]           1   [0.38, 0.818, 0.528]   \n",
       " 47    260  [0.0, 1.0, 0.0]           2  [0.322, 0.644, 0.569]   \n",
       " 48    260  [0.0, 1.0, 0.0]           2  [0.381, 0.772, 0.525]   \n",
       " 49    260  [0.0, 1.0, 0.0]           2  [0.389, 0.695, 0.504]   \n",
       " 50    260  [0.0, 1.0, 0.0]           1   [0.544, 0.81, 0.404]   \n",
       " 51    260  [0.0, 0.0, 1.0]           2  [0.177, 0.753, 0.716]   \n",
       " 52    260  [0.0, 0.0, 1.0]           2  [0.093, 0.661, 0.851]   \n",
       " 53    260  [0.0, 0.0, 1.0]           2   [0.126, 0.84, 0.824]   \n",
       " 54    260  [0.0, 0.0, 1.0]           2  [0.068, 0.518, 0.816]   \n",
       " 55    260  [0.0, 0.0, 1.0]           2  [0.179, 0.689, 0.716]   \n",
       " 56    260  [0.0, 0.0, 1.0]           2      [0.0, 0.796, 1.0]   \n",
       " 57    260  [0.0, 0.0, 1.0]           2  [0.232, 0.948, 0.719]   \n",
       " 58    260  [0.0, 0.0, 1.0]           2  [0.212, 0.777, 0.706]   \n",
       " 59    260  [0.0, 0.0, 1.0]           2   [0.137, 0.611, 0.74]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     270  [1.0, 0.0, 0.0]           0   [0.93, 0.301, 0.067]   \n",
       " 1     270  [0.0, 0.0, 1.0]           2  [0.149, 0.598, 0.779]   \n",
       " 2     270  [1.0, 0.0, 0.0]           0  [0.938, 0.404, 0.049]   \n",
       " 3     270  [0.0, 0.0, 1.0]           2   [0.25, 0.717, 0.624]   \n",
       " 4     270  [1.0, 0.0, 0.0]           0  [0.929, 0.482, 0.093]   \n",
       " 5     270  [1.0, 0.0, 0.0]           0  [0.985, 0.528, 0.038]   \n",
       " 6     270  [1.0, 0.0, 0.0]           0  [0.939, 0.064, 0.056]   \n",
       " 7     270  [1.0, 0.0, 0.0]           0    [0.916, 0.0, 0.034]   \n",
       " 8     270  [0.0, 0.0, 1.0]           2  [0.129, 0.642, 0.783]   \n",
       " 9     270  [0.0, 0.0, 1.0]           2   [0.208, 0.869, 0.68]   \n",
       " 10    270  [0.0, 0.0, 1.0]           2   [0.094, 0.67, 0.802]   \n",
       " 11    270  [0.0, 0.0, 1.0]           2  [0.093, 0.662, 0.792]   \n",
       " 12    270  [0.0, 0.0, 1.0]           2  [0.127, 0.706, 0.783]   \n",
       " 13    270  [1.0, 0.0, 0.0]           0  [0.903, 0.267, 0.068]   \n",
       " 14    270  [1.0, 0.0, 0.0]           0      [1.0, 0.309, 0.0]   \n",
       " 15    270  [0.0, 0.0, 1.0]           2  [0.171, 0.698, 0.726]   \n",
       " 16    270  [0.0, 0.0, 1.0]           2  [0.234, 0.748, 0.631]   \n",
       " 17    270  [1.0, 0.0, 0.0]           0   [0.882, 0.479, 0.13]   \n",
       " 18    270  [1.0, 0.0, 0.0]           0   [0.915, 0.294, 0.08]   \n",
       " 19    270  [1.0, 0.0, 0.0]           0  [0.861, 0.321, 0.131]   \n",
       " 20    270  [1.0, 0.0, 0.0]           0  [0.974, 0.108, 0.002]   \n",
       " 21    270  [1.0, 0.0, 0.0]           0  [0.931, 0.394, 0.086]   \n",
       " 22    270  [1.0, 0.0, 0.0]           0  [0.913, 0.247, 0.096]   \n",
       " 23    270  [1.0, 0.0, 0.0]           0  [0.946, 0.538, 0.065]   \n",
       " 24    270  [1.0, 0.0, 0.0]           0  [0.914, 0.337, 0.084]   \n",
       " 25    270  [1.0, 0.0, 0.0]           0  [0.928, 0.318, 0.065]   \n",
       " 26    270  [1.0, 0.0, 0.0]           0  [0.872, 0.762, 0.174]   \n",
       " 27    270  [1.0, 0.0, 0.0]           0   [0.852, 0.367, 0.11]   \n",
       " 28    270  [1.0, 0.0, 0.0]           0  [0.923, 0.222, 0.064]   \n",
       " 29    270  [1.0, 0.0, 0.0]           0  [0.922, 0.374, 0.083]   \n",
       " 30    270  [0.0, 1.0, 0.0]           2  [0.353, 0.746, 0.538]   \n",
       " 31    270  [0.0, 1.0, 0.0]           2  [0.289, 0.554, 0.577]   \n",
       " 32    270  [0.0, 1.0, 0.0]           1  [0.536, 0.874, 0.402]   \n",
       " 33    270  [0.0, 1.0, 0.0]           2  [0.297, 0.604, 0.626]   \n",
       " 34    270  [0.0, 1.0, 0.0]           1    [0.478, 1.0, 0.488]   \n",
       " 35    270  [0.0, 1.0, 0.0]           2  [0.306, 0.526, 0.606]   \n",
       " 36    270  [0.0, 1.0, 0.0]           2  [0.342, 0.718, 0.523]   \n",
       " 37    270  [0.0, 1.0, 0.0]           2  [0.268, 0.666, 0.582]   \n",
       " 38    270  [0.0, 1.0, 0.0]           2  [0.333, 0.612, 0.586]   \n",
       " 39    270  [0.0, 1.0, 0.0]           2   [0.31, 0.698, 0.582]   \n",
       " 40    270  [0.0, 1.0, 0.0]           1  [0.477, 0.718, 0.469]   \n",
       " 41    270  [0.0, 1.0, 0.0]           1   [0.431, 0.837, 0.51]   \n",
       " 42    270  [0.0, 1.0, 0.0]           2  [0.407, 0.719, 0.515]   \n",
       " 43    270  [0.0, 1.0, 0.0]           2  [0.237, 0.809, 0.662]   \n",
       " 44    270  [0.0, 1.0, 0.0]           2  [0.329, 0.549, 0.516]   \n",
       " 45    270  [0.0, 1.0, 0.0]           2  [0.268, 0.555, 0.638]   \n",
       " 46    270  [0.0, 1.0, 0.0]           1   [0.376, 0.82, 0.528]   \n",
       " 47    270  [0.0, 1.0, 0.0]           2  [0.319, 0.647, 0.569]   \n",
       " 48    270  [0.0, 1.0, 0.0]           2  [0.378, 0.774, 0.525]   \n",
       " 49    270  [0.0, 1.0, 0.0]           2  [0.386, 0.697, 0.504]   \n",
       " 50    270  [0.0, 1.0, 0.0]           1    [0.54, 0.81, 0.403]   \n",
       " 51    270  [0.0, 0.0, 1.0]           2  [0.174, 0.758, 0.717]   \n",
       " 52    270  [0.0, 0.0, 1.0]           2  [0.091, 0.669, 0.852]   \n",
       " 53    270  [0.0, 0.0, 1.0]           2  [0.123, 0.846, 0.825]   \n",
       " 54    270  [0.0, 0.0, 1.0]           2  [0.068, 0.525, 0.819]   \n",
       " 55    270  [0.0, 0.0, 1.0]           2  [0.176, 0.694, 0.717]   \n",
       " 56    270  [0.0, 0.0, 1.0]           2      [0.0, 0.806, 1.0]   \n",
       " 57    270  [0.0, 0.0, 1.0]           2  [0.228, 0.952, 0.719]   \n",
       " 58    270  [0.0, 0.0, 1.0]           2  [0.209, 0.781, 0.708]   \n",
       " 59    270  [0.0, 0.0, 1.0]           2  [0.135, 0.617, 0.742]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     280  [1.0, 0.0, 0.0]           0   [0.93, 0.299, 0.066]   \n",
       " 1     280  [0.0, 0.0, 1.0]           2   [0.147, 0.605, 0.78]   \n",
       " 2     280  [1.0, 0.0, 0.0]           0  [0.938, 0.401, 0.048]   \n",
       " 3     280  [0.0, 0.0, 1.0]           2   [0.247, 0.72, 0.626]   \n",
       " 4     280  [1.0, 0.0, 0.0]           0   [0.928, 0.48, 0.091]   \n",
       " 5     280  [1.0, 0.0, 0.0]           0  [0.984, 0.525, 0.037]   \n",
       " 6     280  [1.0, 0.0, 0.0]           0   [0.94, 0.064, 0.055]   \n",
       " 7     280  [1.0, 0.0, 0.0]           0    [0.918, 0.0, 0.035]   \n",
       " 8     280  [0.0, 0.0, 1.0]           2  [0.127, 0.648, 0.785]   \n",
       " 9     280  [0.0, 0.0, 1.0]           2  [0.204, 0.871, 0.682]   \n",
       " 10    280  [0.0, 0.0, 1.0]           2  [0.093, 0.676, 0.804]   \n",
       " 11    280  [0.0, 0.0, 1.0]           2  [0.092, 0.667, 0.794]   \n",
       " 12    280  [0.0, 0.0, 1.0]           2  [0.125, 0.712, 0.785]   \n",
       " 13    280  [1.0, 0.0, 0.0]           0  [0.904, 0.265, 0.068]   \n",
       " 14    280  [1.0, 0.0, 0.0]           0      [1.0, 0.306, 0.0]   \n",
       " 15    280  [0.0, 0.0, 1.0]           2  [0.168, 0.703, 0.727]   \n",
       " 16    280  [0.0, 0.0, 1.0]           2  [0.231, 0.751, 0.633]   \n",
       " 17    280  [1.0, 0.0, 0.0]           0  [0.881, 0.478, 0.129]   \n",
       " 18    280  [1.0, 0.0, 0.0]           0  [0.915, 0.293, 0.079]   \n",
       " 19    280  [1.0, 0.0, 0.0]           0   [0.861, 0.321, 0.13]   \n",
       " 20    280  [1.0, 0.0, 0.0]           0  [0.975, 0.107, 0.002]   \n",
       " 21    280  [1.0, 0.0, 0.0]           0  [0.931, 0.392, 0.085]   \n",
       " 22    280  [1.0, 0.0, 0.0]           0  [0.913, 0.246, 0.095]   \n",
       " 23    280  [1.0, 0.0, 0.0]           0  [0.946, 0.535, 0.064]   \n",
       " 24    280  [1.0, 0.0, 0.0]           0  [0.914, 0.335, 0.083]   \n",
       " 25    280  [1.0, 0.0, 0.0]           0  [0.929, 0.316, 0.064]   \n",
       " 26    280  [1.0, 0.0, 0.0]           0   [0.87, 0.759, 0.172]   \n",
       " 27    280  [1.0, 0.0, 0.0]           0   [0.852, 0.365, 0.11]   \n",
       " 28    280  [1.0, 0.0, 0.0]           0  [0.924, 0.221, 0.063]   \n",
       " 29    280  [1.0, 0.0, 0.0]           0  [0.922, 0.372, 0.082]   \n",
       " 30    280  [0.0, 1.0, 0.0]           2   [0.35, 0.749, 0.538]   \n",
       " 31    280  [0.0, 1.0, 0.0]           2  [0.286, 0.557, 0.578]   \n",
       " 32    280  [0.0, 1.0, 0.0]           1  [0.532, 0.874, 0.401]   \n",
       " 33    280  [0.0, 1.0, 0.0]           2  [0.294, 0.609, 0.626]   \n",
       " 34    280  [0.0, 1.0, 0.0]           1    [0.474, 1.0, 0.487]   \n",
       " 35    280  [0.0, 1.0, 0.0]           2  [0.303, 0.531, 0.607]   \n",
       " 36    280  [0.0, 1.0, 0.0]           2  [0.339, 0.719, 0.524]   \n",
       " 37    280  [0.0, 1.0, 0.0]           2  [0.265, 0.668, 0.584]   \n",
       " 38    280  [0.0, 1.0, 0.0]           2   [0.33, 0.616, 0.586]   \n",
       " 39    280  [0.0, 1.0, 0.0]           2  [0.307, 0.701, 0.583]   \n",
       " 40    280  [0.0, 1.0, 0.0]           1   [0.474, 0.72, 0.468]   \n",
       " 41    280  [0.0, 1.0, 0.0]           1   [0.427, 0.839, 0.51]   \n",
       " 42    280  [0.0, 1.0, 0.0]           2  [0.404, 0.721, 0.515]   \n",
       " 43    280  [0.0, 1.0, 0.0]           2  [0.234, 0.812, 0.663]   \n",
       " 44    280  [0.0, 1.0, 0.0]           2  [0.327, 0.551, 0.517]   \n",
       " 45    280  [0.0, 1.0, 0.0]           2   [0.265, 0.56, 0.638]   \n",
       " 46    280  [0.0, 1.0, 0.0]           1  [0.373, 0.821, 0.529]   \n",
       " 47    280  [0.0, 1.0, 0.0]           2   [0.316, 0.651, 0.57]   \n",
       " 48    280  [0.0, 1.0, 0.0]           1  [0.374, 0.776, 0.525]   \n",
       " 49    280  [0.0, 1.0, 0.0]           2  [0.383, 0.698, 0.504]   \n",
       " 50    280  [0.0, 1.0, 0.0]           1   [0.537, 0.81, 0.402]   \n",
       " 51    280  [0.0, 0.0, 1.0]           2  [0.171, 0.762, 0.719]   \n",
       " 52    280  [0.0, 0.0, 1.0]           2  [0.089, 0.677, 0.853]   \n",
       " 53    280  [0.0, 0.0, 1.0]           2  [0.121, 0.852, 0.826]   \n",
       " 54    280  [0.0, 0.0, 1.0]           2  [0.067, 0.531, 0.821]   \n",
       " 55    280  [0.0, 0.0, 1.0]           2  [0.174, 0.699, 0.719]   \n",
       " 56    280  [0.0, 0.0, 1.0]           2      [0.0, 0.815, 1.0]   \n",
       " 57    280  [0.0, 0.0, 1.0]           2  [0.224, 0.955, 0.719]   \n",
       " 58    280  [0.0, 0.0, 1.0]           2  [0.205, 0.785, 0.709]   \n",
       " 59    280  [0.0, 0.0, 1.0]           2  [0.133, 0.622, 0.744]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     290  [1.0, 0.0, 0.0]           0   [0.93, 0.298, 0.065]   \n",
       " 1     290  [0.0, 0.0, 1.0]           2   [0.145, 0.613, 0.78]   \n",
       " 2     290  [1.0, 0.0, 0.0]           0  [0.938, 0.398, 0.048]   \n",
       " 3     290  [0.0, 0.0, 1.0]           2  [0.244, 0.723, 0.627]   \n",
       " 4     290  [1.0, 0.0, 0.0]           0   [0.928, 0.478, 0.09]   \n",
       " 5     290  [1.0, 0.0, 0.0]           0  [0.983, 0.521, 0.037]   \n",
       " 6     290  [1.0, 0.0, 0.0]           0  [0.941, 0.064, 0.054]   \n",
       " 7     290  [1.0, 0.0, 0.0]           0     [0.92, 0.0, 0.035]   \n",
       " 8     290  [0.0, 0.0, 1.0]           2  [0.125, 0.654, 0.786]   \n",
       " 9     290  [0.0, 0.0, 1.0]           2  [0.201, 0.874, 0.683]   \n",
       " 10    290  [0.0, 0.0, 1.0]           2  [0.091, 0.682, 0.806]   \n",
       " 11    290  [0.0, 0.0, 1.0]           2   [0.09, 0.672, 0.797]   \n",
       " 12    290  [0.0, 0.0, 1.0]           2  [0.123, 0.717, 0.787]   \n",
       " 13    290  [1.0, 0.0, 0.0]           0  [0.904, 0.264, 0.067]   \n",
       " 14    290  [1.0, 0.0, 0.0]           0      [1.0, 0.303, 0.0]   \n",
       " 15    290  [0.0, 0.0, 1.0]           2  [0.166, 0.708, 0.729]   \n",
       " 16    290  [0.0, 0.0, 1.0]           2  [0.228, 0.753, 0.635]   \n",
       " 17    290  [1.0, 0.0, 0.0]           0  [0.881, 0.477, 0.127]   \n",
       " 18    290  [1.0, 0.0, 0.0]           0  [0.915, 0.292, 0.078]   \n",
       " 19    290  [1.0, 0.0, 0.0]           0   [0.861, 0.32, 0.129]   \n",
       " 20    290  [1.0, 0.0, 0.0]           0  [0.976, 0.106, 0.002]   \n",
       " 21    290  [1.0, 0.0, 0.0]           0   [0.931, 0.39, 0.083]   \n",
       " 22    290  [1.0, 0.0, 0.0]           0  [0.913, 0.246, 0.093]   \n",
       " 23    290  [1.0, 0.0, 0.0]           0  [0.945, 0.532, 0.063]   \n",
       " 24    290  [1.0, 0.0, 0.0]           0  [0.914, 0.334, 0.082]   \n",
       " 25    290  [1.0, 0.0, 0.0]           0  [0.929, 0.314, 0.063]   \n",
       " 26    290  [1.0, 0.0, 0.0]           0   [0.868, 0.757, 0.17]   \n",
       " 27    290  [1.0, 0.0, 0.0]           0  [0.852, 0.363, 0.109]   \n",
       " 28    290  [1.0, 0.0, 0.0]           0   [0.924, 0.22, 0.063]   \n",
       " 29    290  [1.0, 0.0, 0.0]           0  [0.921, 0.371, 0.081]   \n",
       " 30    290  [0.0, 1.0, 0.0]           2  [0.347, 0.751, 0.539]   \n",
       " 31    290  [0.0, 1.0, 0.0]           2  [0.284, 0.561, 0.579]   \n",
       " 32    290  [0.0, 1.0, 0.0]           1    [0.529, 0.873, 0.4]   \n",
       " 33    290  [0.0, 1.0, 0.0]           2  [0.291, 0.613, 0.626]   \n",
       " 34    290  [0.0, 1.0, 0.0]           1    [0.469, 1.0, 0.486]   \n",
       " 35    290  [0.0, 1.0, 0.0]           2    [0.3, 0.536, 0.607]   \n",
       " 36    290  [0.0, 1.0, 0.0]           2  [0.336, 0.721, 0.524]   \n",
       " 37    290  [0.0, 1.0, 0.0]           2  [0.263, 0.671, 0.585]   \n",
       " 38    290  [0.0, 1.0, 0.0]           2   [0.327, 0.62, 0.586]   \n",
       " 39    290  [0.0, 1.0, 0.0]           2  [0.304, 0.704, 0.583]   \n",
       " 40    290  [0.0, 1.0, 0.0]           1  [0.471, 0.722, 0.467]   \n",
       " 41    290  [0.0, 1.0, 0.0]           1   [0.423, 0.84, 0.509]   \n",
       " 42    290  [0.0, 1.0, 0.0]           2  [0.401, 0.724, 0.515]   \n",
       " 43    290  [0.0, 1.0, 0.0]           2  [0.231, 0.816, 0.664]   \n",
       " 44    290  [0.0, 1.0, 0.0]           2  [0.325, 0.554, 0.519]   \n",
       " 45    290  [0.0, 1.0, 0.0]           2  [0.262, 0.565, 0.639]   \n",
       " 46    290  [0.0, 1.0, 0.0]           1  [0.369, 0.823, 0.529]   \n",
       " 47    290  [0.0, 1.0, 0.0]           2  [0.313, 0.654, 0.571]   \n",
       " 48    290  [0.0, 1.0, 0.0]           1  [0.371, 0.778, 0.525]   \n",
       " 49    290  [0.0, 1.0, 0.0]           2     [0.38, 0.7, 0.505]   \n",
       " 50    290  [0.0, 1.0, 0.0]           1   [0.533, 0.81, 0.402]   \n",
       " 51    290  [0.0, 0.0, 1.0]           2   [0.169, 0.766, 0.72]   \n",
       " 52    290  [0.0, 0.0, 1.0]           2  [0.088, 0.685, 0.854]   \n",
       " 53    290  [0.0, 0.0, 1.0]           2  [0.118, 0.858, 0.826]   \n",
       " 54    290  [0.0, 0.0, 1.0]           2  [0.066, 0.538, 0.824]   \n",
       " 55    290  [0.0, 0.0, 1.0]           2   [0.171, 0.703, 0.72]   \n",
       " 56    290  [0.0, 0.0, 1.0]           2      [0.0, 0.825, 1.0]   \n",
       " 57    290  [0.0, 0.0, 1.0]           2   [0.22, 0.959, 0.719]   \n",
       " 58    290  [0.0, 0.0, 1.0]           2   [0.202, 0.789, 0.71]   \n",
       " 59    290  [0.0, 0.0, 1.0]           2  [0.132, 0.627, 0.746]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     300  [1.0, 0.0, 0.0]           0   [0.93, 0.296, 0.065]   \n",
       " 1     300  [0.0, 0.0, 1.0]           2  [0.142, 0.619, 0.781]   \n",
       " 2     300  [1.0, 0.0, 0.0]           0  [0.938, 0.395, 0.047]   \n",
       " 3     300  [0.0, 0.0, 1.0]           2  [0.241, 0.726, 0.629]   \n",
       " 4     300  [1.0, 0.0, 0.0]           0  [0.927, 0.476, 0.088]   \n",
       " 5     300  [1.0, 0.0, 0.0]           0  [0.982, 0.518, 0.036]   \n",
       " 6     300  [1.0, 0.0, 0.0]           0  [0.942, 0.064, 0.053]   \n",
       " 7     300  [1.0, 0.0, 0.0]           0    [0.921, 0.0, 0.035]   \n",
       " 8     300  [0.0, 0.0, 1.0]           2  [0.123, 0.659, 0.788]   \n",
       " 9     300  [0.0, 0.0, 1.0]           2  [0.198, 0.876, 0.685]   \n",
       " 10    300  [0.0, 0.0, 1.0]           2   [0.09, 0.687, 0.808]   \n",
       " 11    300  [0.0, 0.0, 1.0]           2  [0.089, 0.676, 0.799]   \n",
       " 12    300  [0.0, 0.0, 1.0]           2  [0.121, 0.721, 0.788]   \n",
       " 13    300  [1.0, 0.0, 0.0]           0  [0.905, 0.262, 0.067]   \n",
       " 14    300  [1.0, 0.0, 0.0]           0        [1.0, 0.3, 0.0]   \n",
       " 15    300  [0.0, 0.0, 1.0]           2  [0.163, 0.712, 0.731]   \n",
       " 16    300  [0.0, 0.0, 1.0]           2  [0.225, 0.756, 0.636]   \n",
       " 17    300  [1.0, 0.0, 0.0]           0   [0.88, 0.476, 0.126]   \n",
       " 18    300  [1.0, 0.0, 0.0]           0  [0.916, 0.291, 0.077]   \n",
       " 19    300  [1.0, 0.0, 0.0]           0   [0.861, 0.32, 0.128]   \n",
       " 20    300  [1.0, 0.0, 0.0]           0  [0.977, 0.105, 0.002]   \n",
       " 21    300  [1.0, 0.0, 0.0]           0   [0.93, 0.389, 0.082]   \n",
       " 22    300  [1.0, 0.0, 0.0]           0  [0.913, 0.246, 0.092]   \n",
       " 23    300  [1.0, 0.0, 0.0]           0  [0.944, 0.529, 0.062]   \n",
       " 24    300  [1.0, 0.0, 0.0]           0  [0.914, 0.333, 0.081]   \n",
       " 25    300  [1.0, 0.0, 0.0]           0  [0.929, 0.313, 0.063]   \n",
       " 26    300  [1.0, 0.0, 0.0]           0  [0.866, 0.755, 0.168]   \n",
       " 27    300  [1.0, 0.0, 0.0]           0  [0.852, 0.362, 0.109]   \n",
       " 28    300  [1.0, 0.0, 0.0]           0   [0.925, 0.22, 0.062]   \n",
       " 29    300  [1.0, 0.0, 0.0]           0   [0.921, 0.369, 0.08]   \n",
       " 30    300  [0.0, 1.0, 0.0]           2  [0.344, 0.753, 0.539]   \n",
       " 31    300  [0.0, 1.0, 0.0]           2   [0.281, 0.564, 0.58]   \n",
       " 32    300  [0.0, 1.0, 0.0]           1    [0.525, 0.873, 0.4]   \n",
       " 33    300  [0.0, 1.0, 0.0]           2  [0.288, 0.618, 0.626]   \n",
       " 34    300  [0.0, 1.0, 0.0]           1    [0.465, 1.0, 0.485]   \n",
       " 35    300  [0.0, 1.0, 0.0]           2   [0.297, 0.54, 0.607]   \n",
       " 36    300  [0.0, 1.0, 0.0]           2  [0.333, 0.722, 0.525]   \n",
       " 37    300  [0.0, 1.0, 0.0]           2   [0.26, 0.673, 0.587]   \n",
       " 38    300  [0.0, 1.0, 0.0]           2  [0.324, 0.624, 0.586]   \n",
       " 39    300  [0.0, 1.0, 0.0]           2  [0.301, 0.706, 0.584]   \n",
       " 40    300  [0.0, 1.0, 0.0]           1  [0.467, 0.724, 0.466]   \n",
       " 41    300  [0.0, 1.0, 0.0]           1  [0.419, 0.842, 0.509]   \n",
       " 42    300  [0.0, 1.0, 0.0]           2  [0.397, 0.726, 0.514]   \n",
       " 43    300  [0.0, 1.0, 0.0]           2  [0.228, 0.819, 0.665]   \n",
       " 44    300  [0.0, 1.0, 0.0]           2   [0.322, 0.556, 0.52]   \n",
       " 45    300  [0.0, 1.0, 0.0]           2  [0.259, 0.569, 0.639]   \n",
       " 46    300  [0.0, 1.0, 0.0]           1  [0.366, 0.824, 0.529]   \n",
       " 47    300  [0.0, 1.0, 0.0]           2   [0.31, 0.657, 0.571]   \n",
       " 48    300  [0.0, 1.0, 0.0]           1  [0.367, 0.779, 0.526]   \n",
       " 49    300  [0.0, 1.0, 0.0]           2  [0.377, 0.702, 0.505]   \n",
       " 50    300  [0.0, 1.0, 0.0]           1   [0.53, 0.809, 0.401]   \n",
       " 51    300  [0.0, 0.0, 1.0]           2   [0.166, 0.77, 0.722]   \n",
       " 52    300  [0.0, 0.0, 1.0]           2  [0.086, 0.692, 0.854]   \n",
       " 53    300  [0.0, 0.0, 1.0]           2  [0.116, 0.864, 0.827]   \n",
       " 54    300  [0.0, 0.0, 1.0]           2  [0.066, 0.543, 0.826]   \n",
       " 55    300  [0.0, 0.0, 1.0]           2  [0.169, 0.708, 0.721]   \n",
       " 56    300  [0.0, 0.0, 1.0]           2      [0.0, 0.833, 1.0]   \n",
       " 57    300  [0.0, 0.0, 1.0]           2   [0.216, 0.962, 0.72]   \n",
       " 58    300  [0.0, 0.0, 1.0]           2  [0.199, 0.793, 0.711]   \n",
       " 59    300  [0.0, 0.0, 1.0]           2   [0.13, 0.632, 0.749]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     310  [1.0, 0.0, 0.0]           0  [0.931, 0.295, 0.064]   \n",
       " 1     310  [0.0, 0.0, 1.0]           2   [0.14, 0.625, 0.782]   \n",
       " 2     310  [1.0, 0.0, 0.0]           0  [0.938, 0.393, 0.047]   \n",
       " 3     310  [0.0, 0.0, 1.0]           2   [0.238, 0.728, 0.63]   \n",
       " 4     310  [1.0, 0.0, 0.0]           0  [0.926, 0.475, 0.087]   \n",
       " 5     310  [1.0, 0.0, 0.0]           0  [0.981, 0.515, 0.035]   \n",
       " 6     310  [1.0, 0.0, 0.0]           0  [0.943, 0.065, 0.053]   \n",
       " 7     310  [1.0, 0.0, 0.0]           0    [0.923, 0.0, 0.035]   \n",
       " 8     310  [0.0, 0.0, 1.0]           2   [0.121, 0.663, 0.79]   \n",
       " 9     310  [0.0, 0.0, 1.0]           2  [0.195, 0.878, 0.687]   \n",
       " 10    310  [0.0, 0.0, 1.0]           2  [0.089, 0.691, 0.811]   \n",
       " 11    310  [0.0, 0.0, 1.0]           2   [0.088, 0.68, 0.802]   \n",
       " 12    310  [0.0, 0.0, 1.0]           2   [0.119, 0.725, 0.79]   \n",
       " 13    310  [1.0, 0.0, 0.0]           0  [0.905, 0.261, 0.067]   \n",
       " 14    310  [1.0, 0.0, 0.0]           0      [1.0, 0.297, 0.0]   \n",
       " 15    310  [0.0, 0.0, 1.0]           2  [0.161, 0.715, 0.732]   \n",
       " 16    310  [0.0, 0.0, 1.0]           2  [0.222, 0.758, 0.638]   \n",
       " 17    310  [1.0, 0.0, 0.0]           0   [0.88, 0.475, 0.124]   \n",
       " 18    310  [1.0, 0.0, 0.0]           0   [0.916, 0.29, 0.076]   \n",
       " 19    310  [1.0, 0.0, 0.0]           0  [0.861, 0.319, 0.127]   \n",
       " 20    310  [1.0, 0.0, 0.0]           0  [0.978, 0.104, 0.002]   \n",
       " 21    310  [1.0, 0.0, 0.0]           0   [0.93, 0.388, 0.081]   \n",
       " 22    310  [1.0, 0.0, 0.0]           0  [0.914, 0.246, 0.091]   \n",
       " 23    310  [1.0, 0.0, 0.0]           0  [0.944, 0.526, 0.061]   \n",
       " 24    310  [1.0, 0.0, 0.0]           0   [0.914, 0.332, 0.08]   \n",
       " 25    310  [1.0, 0.0, 0.0]           0  [0.929, 0.311, 0.062]   \n",
       " 26    310  [1.0, 0.0, 0.0]           0  [0.864, 0.753, 0.166]   \n",
       " 27    310  [1.0, 0.0, 0.0]           0   [0.852, 0.36, 0.108]   \n",
       " 28    310  [1.0, 0.0, 0.0]           0  [0.925, 0.219, 0.061]   \n",
       " 29    310  [1.0, 0.0, 0.0]           0  [0.921, 0.368, 0.079]   \n",
       " 30    310  [0.0, 1.0, 0.0]           2    [0.34, 0.754, 0.54]   \n",
       " 31    310  [0.0, 1.0, 0.0]           2  [0.279, 0.567, 0.581]   \n",
       " 32    310  [0.0, 1.0, 0.0]           1  [0.522, 0.872, 0.399]   \n",
       " 33    310  [0.0, 1.0, 0.0]           2  [0.285, 0.622, 0.626]   \n",
       " 34    310  [0.0, 1.0, 0.0]           1    [0.461, 1.0, 0.484]   \n",
       " 35    310  [0.0, 1.0, 0.0]           2  [0.294, 0.544, 0.608]   \n",
       " 36    310  [0.0, 1.0, 0.0]           2   [0.33, 0.723, 0.527]   \n",
       " 37    310  [0.0, 1.0, 0.0]           2  [0.257, 0.674, 0.589]   \n",
       " 38    310  [0.0, 1.0, 0.0]           2  [0.321, 0.627, 0.586]   \n",
       " 39    310  [0.0, 1.0, 0.0]           2  [0.298, 0.709, 0.585]   \n",
       " 40    310  [0.0, 1.0, 0.0]           1  [0.464, 0.725, 0.466]   \n",
       " 41    310  [0.0, 1.0, 0.0]           1  [0.415, 0.843, 0.508]   \n",
       " 42    310  [0.0, 1.0, 0.0]           2  [0.394, 0.727, 0.514]   \n",
       " 43    310  [0.0, 1.0, 0.0]           2  [0.224, 0.821, 0.666]   \n",
       " 44    310  [0.0, 1.0, 0.0]           2   [0.32, 0.557, 0.521]   \n",
       " 45    310  [0.0, 1.0, 0.0]           2   [0.257, 0.573, 0.64]   \n",
       " 46    310  [0.0, 1.0, 0.0]           1  [0.362, 0.826, 0.529]   \n",
       " 47    310  [0.0, 1.0, 0.0]           2  [0.307, 0.659, 0.572]   \n",
       " 48    310  [0.0, 1.0, 0.0]           1   [0.364, 0.78, 0.526]   \n",
       " 49    310  [0.0, 1.0, 0.0]           2  [0.374, 0.703, 0.505]   \n",
       " 50    310  [0.0, 1.0, 0.0]           1  [0.527, 0.809, 0.401]   \n",
       " 51    310  [0.0, 0.0, 1.0]           2  [0.164, 0.773, 0.723]   \n",
       " 52    310  [0.0, 0.0, 1.0]           2  [0.085, 0.698, 0.855]   \n",
       " 53    310  [0.0, 0.0, 1.0]           2  [0.114, 0.868, 0.828]   \n",
       " 54    310  [0.0, 0.0, 1.0]           2  [0.065, 0.548, 0.829]   \n",
       " 55    310  [0.0, 0.0, 1.0]           2  [0.166, 0.711, 0.723]   \n",
       " 56    310  [0.0, 0.0, 1.0]           2       [0.0, 0.84, 1.0]   \n",
       " 57    310  [0.0, 0.0, 1.0]           2   [0.212, 0.965, 0.72]   \n",
       " 58    310  [0.0, 0.0, 1.0]           2  [0.196, 0.796, 0.712]   \n",
       " 59    310  [0.0, 0.0, 1.0]           2  [0.128, 0.635, 0.751]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     320  [1.0, 0.0, 0.0]           0  [0.931, 0.294, 0.063]   \n",
       " 1     320  [0.0, 0.0, 1.0]           2  [0.138, 0.631, 0.783]   \n",
       " 2     320  [1.0, 0.0, 0.0]           0  [0.938, 0.391, 0.047]   \n",
       " 3     320  [0.0, 0.0, 1.0]           2   [0.235, 0.73, 0.632]   \n",
       " 4     320  [1.0, 0.0, 0.0]           0  [0.926, 0.473, 0.086]   \n",
       " 5     320  [1.0, 0.0, 0.0]           0  [0.981, 0.512, 0.034]   \n",
       " 6     320  [1.0, 0.0, 0.0]           0  [0.943, 0.065, 0.052]   \n",
       " 7     320  [1.0, 0.0, 0.0]           0    [0.924, 0.0, 0.035]   \n",
       " 8     320  [0.0, 0.0, 1.0]           2  [0.119, 0.667, 0.792]   \n",
       " 9     320  [0.0, 0.0, 1.0]           2   [0.192, 0.88, 0.688]   \n",
       " 10    320  [0.0, 0.0, 1.0]           2  [0.087, 0.695, 0.813]   \n",
       " 11    320  [0.0, 0.0, 1.0]           2  [0.086, 0.684, 0.804]   \n",
       " 12    320  [0.0, 0.0, 1.0]           2  [0.117, 0.729, 0.792]   \n",
       " 13    320  [1.0, 0.0, 0.0]           0   [0.906, 0.26, 0.066]   \n",
       " 14    320  [1.0, 0.0, 0.0]           0      [1.0, 0.295, 0.0]   \n",
       " 15    320  [0.0, 0.0, 1.0]           2  [0.158, 0.719, 0.734]   \n",
       " 16    320  [0.0, 0.0, 1.0]           2     [0.22, 0.76, 0.64]   \n",
       " 17    320  [1.0, 0.0, 0.0]           0  [0.879, 0.474, 0.123]   \n",
       " 18    320  [1.0, 0.0, 0.0]           0  [0.916, 0.289, 0.075]   \n",
       " 19    320  [1.0, 0.0, 0.0]           0  [0.861, 0.319, 0.126]   \n",
       " 20    320  [1.0, 0.0, 0.0]           0  [0.979, 0.103, 0.002]   \n",
       " 21    320  [1.0, 0.0, 0.0]           0    [0.93, 0.387, 0.08]   \n",
       " 22    320  [1.0, 0.0, 0.0]           0   [0.914, 0.246, 0.09]   \n",
       " 23    320  [1.0, 0.0, 0.0]           0  [0.943, 0.524, 0.061]   \n",
       " 24    320  [1.0, 0.0, 0.0]           0  [0.914, 0.331, 0.079]   \n",
       " 25    320  [1.0, 0.0, 0.0]           0   [0.929, 0.31, 0.061]   \n",
       " 26    320  [1.0, 0.0, 0.0]           0  [0.862, 0.751, 0.164]   \n",
       " 27    320  [1.0, 0.0, 0.0]           0  [0.852, 0.359, 0.108]   \n",
       " 28    320  [1.0, 0.0, 0.0]           0  [0.926, 0.218, 0.061]   \n",
       " 29    320  [1.0, 0.0, 0.0]           0  [0.921, 0.367, 0.078]   \n",
       " 30    320  [0.0, 1.0, 0.0]           2   [0.337, 0.756, 0.54]   \n",
       " 31    320  [0.0, 1.0, 0.0]           2  [0.276, 0.569, 0.583]   \n",
       " 32    320  [0.0, 1.0, 0.0]           1  [0.519, 0.872, 0.398]   \n",
       " 33    320  [0.0, 1.0, 0.0]           2  [0.282, 0.626, 0.626]   \n",
       " 34    320  [0.0, 1.0, 0.0]           1    [0.457, 1.0, 0.483]   \n",
       " 35    320  [0.0, 1.0, 0.0]           2  [0.292, 0.548, 0.608]   \n",
       " 36    320  [0.0, 1.0, 0.0]           2  [0.327, 0.724, 0.528]   \n",
       " 37    320  [0.0, 1.0, 0.0]           2  [0.255, 0.676, 0.591]   \n",
       " 38    320  [0.0, 1.0, 0.0]           2   [0.318, 0.63, 0.587]   \n",
       " 39    320  [0.0, 1.0, 0.0]           2  [0.295, 0.711, 0.586]   \n",
       " 40    320  [0.0, 1.0, 0.0]           1  [0.461, 0.727, 0.465]   \n",
       " 41    320  [0.0, 1.0, 0.0]           1  [0.412, 0.844, 0.508]   \n",
       " 42    320  [0.0, 1.0, 0.0]           2  [0.391, 0.729, 0.514]   \n",
       " 43    320  [0.0, 1.0, 0.0]           2  [0.221, 0.824, 0.667]   \n",
       " 44    320  [0.0, 1.0, 0.0]           2  [0.318, 0.559, 0.523]   \n",
       " 45    320  [0.0, 1.0, 0.0]           2  [0.254, 0.577, 0.641]   \n",
       " 46    320  [0.0, 1.0, 0.0]           1  [0.359, 0.827, 0.529]   \n",
       " 47    320  [0.0, 1.0, 0.0]           2  [0.304, 0.662, 0.573]   \n",
       " 48    320  [0.0, 1.0, 0.0]           1  [0.361, 0.782, 0.526]   \n",
       " 49    320  [0.0, 1.0, 0.0]           2  [0.371, 0.705, 0.506]   \n",
       " 50    320  [0.0, 1.0, 0.0]           1    [0.524, 0.809, 0.4]   \n",
       " 51    320  [0.0, 0.0, 1.0]           2  [0.161, 0.777, 0.725]   \n",
       " 52    320  [0.0, 0.0, 1.0]           2  [0.083, 0.704, 0.856]   \n",
       " 53    320  [0.0, 0.0, 1.0]           2  [0.111, 0.873, 0.829]   \n",
       " 54    320  [0.0, 0.0, 1.0]           2  [0.064, 0.553, 0.831]   \n",
       " 55    320  [0.0, 0.0, 1.0]           2  [0.164, 0.715, 0.724]   \n",
       " 56    320  [0.0, 0.0, 1.0]           2      [0.0, 0.848, 1.0]   \n",
       " 57    320  [0.0, 0.0, 1.0]           2   [0.209, 0.968, 0.72]   \n",
       " 58    320  [0.0, 0.0, 1.0]           2  [0.193, 0.799, 0.713]   \n",
       " 59    320  [0.0, 0.0, 1.0]           2   [0.126, 0.64, 0.753]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     330  [1.0, 0.0, 0.0]           0  [0.931, 0.293, 0.062]   \n",
       " 1     330  [0.0, 0.0, 1.0]           2  [0.136, 0.635, 0.784]   \n",
       " 2     330  [1.0, 0.0, 0.0]           0  [0.938, 0.388, 0.046]   \n",
       " 3     330  [0.0, 0.0, 1.0]           2  [0.233, 0.731, 0.633]   \n",
       " 4     330  [1.0, 0.0, 0.0]           0  [0.926, 0.472, 0.084]   \n",
       " 5     330  [1.0, 0.0, 0.0]           0    [0.98, 0.51, 0.034]   \n",
       " 6     330  [1.0, 0.0, 0.0]           0  [0.944, 0.065, 0.052]   \n",
       " 7     330  [1.0, 0.0, 0.0]           0    [0.926, 0.0, 0.035]   \n",
       " 8     330  [0.0, 0.0, 1.0]           2   [0.117, 0.67, 0.794]   \n",
       " 9     330  [0.0, 0.0, 1.0]           2   [0.189, 0.881, 0.69]   \n",
       " 10    330  [0.0, 0.0, 1.0]           2  [0.086, 0.698, 0.815]   \n",
       " 11    330  [0.0, 0.0, 1.0]           2  [0.085, 0.686, 0.807]   \n",
       " 12    330  [0.0, 0.0, 1.0]           2  [0.115, 0.732, 0.794]   \n",
       " 13    330  [1.0, 0.0, 0.0]           0  [0.906, 0.259, 0.066]   \n",
       " 14    330  [1.0, 0.0, 0.0]           0      [1.0, 0.292, 0.0]   \n",
       " 15    330  [0.0, 0.0, 1.0]           2  [0.156, 0.721, 0.736]   \n",
       " 16    330  [0.0, 0.0, 1.0]           2  [0.217, 0.761, 0.641]   \n",
       " 17    330  [1.0, 0.0, 0.0]           0  [0.879, 0.473, 0.121]   \n",
       " 18    330  [1.0, 0.0, 0.0]           0  [0.916, 0.288, 0.074]   \n",
       " 19    330  [1.0, 0.0, 0.0]           0  [0.861, 0.319, 0.125]   \n",
       " 20    330  [1.0, 0.0, 0.0]           0   [0.98, 0.102, 0.002]   \n",
       " 21    330  [1.0, 0.0, 0.0]           0   [0.93, 0.385, 0.079]   \n",
       " 22    330  [1.0, 0.0, 0.0]           0  [0.914, 0.246, 0.089]   \n",
       " 23    330  [1.0, 0.0, 0.0]           0   [0.942, 0.522, 0.06]   \n",
       " 24    330  [1.0, 0.0, 0.0]           0   [0.914, 0.33, 0.078]   \n",
       " 25    330  [1.0, 0.0, 0.0]           0  [0.929, 0.308, 0.061]   \n",
       " 26    330  [1.0, 0.0, 0.0]           0   [0.861, 0.75, 0.162]   \n",
       " 27    330  [1.0, 0.0, 0.0]           0  [0.852, 0.357, 0.107]   \n",
       " 28    330  [1.0, 0.0, 0.0]           0   [0.926, 0.218, 0.06]   \n",
       " 29    330  [1.0, 0.0, 0.0]           0  [0.921, 0.366, 0.077]   \n",
       " 30    330  [0.0, 1.0, 0.0]           2  [0.334, 0.757, 0.541]   \n",
       " 31    330  [0.0, 1.0, 0.0]           2  [0.274, 0.571, 0.584]   \n",
       " 32    330  [0.0, 1.0, 0.0]           1  [0.516, 0.871, 0.398]   \n",
       " 33    330  [0.0, 1.0, 0.0]           2   [0.28, 0.629, 0.626]   \n",
       " 34    330  [0.0, 1.0, 0.0]           1    [0.453, 1.0, 0.482]   \n",
       " 35    330  [0.0, 1.0, 0.0]           2  [0.289, 0.551, 0.609]   \n",
       " 36    330  [0.0, 1.0, 0.0]           2  [0.324, 0.725, 0.529]   \n",
       " 37    330  [0.0, 1.0, 0.0]           2  [0.252, 0.677, 0.593]   \n",
       " 38    330  [0.0, 1.0, 0.0]           2  [0.315, 0.633, 0.587]   \n",
       " 39    330  [0.0, 1.0, 0.0]           2  [0.292, 0.713, 0.586]   \n",
       " 40    330  [0.0, 1.0, 0.0]           1  [0.458, 0.728, 0.464]   \n",
       " 41    330  [0.0, 1.0, 0.0]           1  [0.408, 0.845, 0.508]   \n",
       " 42    330  [0.0, 1.0, 0.0]           1  [0.388, 0.731, 0.514]   \n",
       " 43    330  [0.0, 1.0, 0.0]           2  [0.219, 0.826, 0.668]   \n",
       " 44    330  [0.0, 1.0, 0.0]           2   [0.316, 0.56, 0.524]   \n",
       " 45    330  [0.0, 1.0, 0.0]           2  [0.251, 0.581, 0.642]   \n",
       " 46    330  [0.0, 1.0, 0.0]           1  [0.356, 0.828, 0.529]   \n",
       " 47    330  [0.0, 1.0, 0.0]           2  [0.301, 0.664, 0.573]   \n",
       " 48    330  [0.0, 1.0, 0.0]           1  [0.358, 0.783, 0.527]   \n",
       " 49    330  [0.0, 1.0, 0.0]           2  [0.368, 0.706, 0.506]   \n",
       " 50    330  [0.0, 1.0, 0.0]           1  [0.521, 0.809, 0.399]   \n",
       " 51    330  [0.0, 0.0, 1.0]           2  [0.159, 0.779, 0.726]   \n",
       " 52    330  [0.0, 0.0, 1.0]           2  [0.082, 0.709, 0.857]   \n",
       " 53    330  [0.0, 0.0, 1.0]           2  [0.109, 0.877, 0.829]   \n",
       " 54    330  [0.0, 0.0, 1.0]           2  [0.063, 0.557, 0.834]   \n",
       " 55    330  [0.0, 0.0, 1.0]           2  [0.162, 0.718, 0.726]   \n",
       " 56    330  [0.0, 0.0, 1.0]           2      [0.0, 0.854, 1.0]   \n",
       " 57    330  [0.0, 0.0, 1.0]           2   [0.205, 0.97, 0.721]   \n",
       " 58    330  [0.0, 0.0, 1.0]           2   [0.19, 0.802, 0.714]   \n",
       " 59    330  [0.0, 0.0, 1.0]           2  [0.125, 0.642, 0.755]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     340  [1.0, 0.0, 0.0]           0  [0.931, 0.292, 0.062]   \n",
       " 1     340  [0.0, 0.0, 1.0]           2   [0.134, 0.64, 0.784]   \n",
       " 2     340  [1.0, 0.0, 0.0]           0  [0.938, 0.386, 0.046]   \n",
       " 3     340  [0.0, 0.0, 1.0]           2   [0.23, 0.733, 0.635]   \n",
       " 4     340  [1.0, 0.0, 0.0]           0  [0.925, 0.471, 0.083]   \n",
       " 5     340  [1.0, 0.0, 0.0]           0  [0.979, 0.507, 0.033]   \n",
       " 6     340  [1.0, 0.0, 0.0]           0  [0.945, 0.066, 0.051]   \n",
       " 7     340  [1.0, 0.0, 0.0]           0    [0.927, 0.0, 0.035]   \n",
       " 8     340  [0.0, 0.0, 1.0]           2  [0.115, 0.674, 0.795]   \n",
       " 9     340  [0.0, 0.0, 1.0]           2  [0.186, 0.882, 0.691]   \n",
       " 10    340  [0.0, 0.0, 1.0]           2  [0.085, 0.702, 0.817]   \n",
       " 11    340  [0.0, 0.0, 1.0]           2  [0.084, 0.689, 0.809]   \n",
       " 12    340  [0.0, 0.0, 1.0]           2  [0.113, 0.735, 0.796]   \n",
       " 13    340  [1.0, 0.0, 0.0]           0  [0.907, 0.257, 0.065]   \n",
       " 14    340  [1.0, 0.0, 0.0]           0       [1.0, 0.29, 0.0]   \n",
       " 15    340  [0.0, 0.0, 1.0]           2  [0.154, 0.724, 0.737]   \n",
       " 16    340  [0.0, 0.0, 1.0]           2  [0.214, 0.763, 0.643]   \n",
       " 17    340  [1.0, 0.0, 0.0]           0   [0.878, 0.472, 0.12]   \n",
       " 18    340  [1.0, 0.0, 0.0]           0  [0.917, 0.288, 0.074]   \n",
       " 19    340  [1.0, 0.0, 0.0]           0  [0.861, 0.318, 0.123]   \n",
       " 20    340  [1.0, 0.0, 0.0]           0   [0.98, 0.101, 0.002]   \n",
       " 21    340  [1.0, 0.0, 0.0]           0  [0.929, 0.385, 0.078]   \n",
       " 22    340  [1.0, 0.0, 0.0]           0  [0.914, 0.245, 0.088]   \n",
       " 23    340  [1.0, 0.0, 0.0]           0  [0.942, 0.519, 0.059]   \n",
       " 24    340  [1.0, 0.0, 0.0]           0  [0.915, 0.329, 0.077]   \n",
       " 25    340  [1.0, 0.0, 0.0]           0    [0.93, 0.307, 0.06]   \n",
       " 26    340  [1.0, 0.0, 0.0]           0   [0.859, 0.748, 0.16]   \n",
       " 27    340  [1.0, 0.0, 0.0]           0  [0.853, 0.356, 0.107]   \n",
       " 28    340  [1.0, 0.0, 0.0]           0  [0.927, 0.217, 0.059]   \n",
       " 29    340  [1.0, 0.0, 0.0]           0  [0.921, 0.365, 0.076]   \n",
       " 30    340  [0.0, 1.0, 0.0]           2  [0.331, 0.759, 0.541]   \n",
       " 31    340  [0.0, 1.0, 0.0]           2  [0.272, 0.574, 0.585]   \n",
       " 32    340  [0.0, 1.0, 0.0]           1  [0.513, 0.871, 0.397]   \n",
       " 33    340  [0.0, 1.0, 0.0]           2  [0.277, 0.632, 0.627]   \n",
       " 34    340  [0.0, 1.0, 0.0]           1    [0.449, 1.0, 0.482]   \n",
       " 35    340  [0.0, 1.0, 0.0]           2  [0.287, 0.555, 0.609]   \n",
       " 36    340  [0.0, 1.0, 0.0]           2   [0.322, 0.726, 0.53]   \n",
       " 37    340  [0.0, 1.0, 0.0]           2   [0.25, 0.679, 0.594]   \n",
       " 38    340  [0.0, 1.0, 0.0]           2  [0.313, 0.636, 0.587]   \n",
       " 39    340  [0.0, 1.0, 0.0]           2  [0.289, 0.715, 0.587]   \n",
       " 40    340  [0.0, 1.0, 0.0]           1   [0.455, 0.73, 0.464]   \n",
       " 41    340  [0.0, 1.0, 0.0]           1  [0.405, 0.846, 0.507]   \n",
       " 42    340  [0.0, 1.0, 0.0]           1  [0.385, 0.732, 0.514]   \n",
       " 43    340  [0.0, 1.0, 0.0]           2  [0.216, 0.828, 0.669]   \n",
       " 44    340  [0.0, 1.0, 0.0]           2  [0.313, 0.562, 0.525]   \n",
       " 45    340  [0.0, 1.0, 0.0]           2  [0.249, 0.584, 0.642]   \n",
       " 46    340  [0.0, 1.0, 0.0]           1  [0.353, 0.829, 0.529]   \n",
       " 47    340  [0.0, 1.0, 0.0]           2  [0.299, 0.666, 0.574]   \n",
       " 48    340  [0.0, 1.0, 0.0]           1  [0.355, 0.784, 0.527]   \n",
       " 49    340  [0.0, 1.0, 0.0]           2  [0.366, 0.707, 0.507]   \n",
       " 50    340  [0.0, 1.0, 0.0]           1  [0.518, 0.809, 0.399]   \n",
       " 51    340  [0.0, 0.0, 1.0]           2  [0.157, 0.782, 0.728]   \n",
       " 52    340  [0.0, 0.0, 1.0]           2   [0.08, 0.715, 0.858]   \n",
       " 53    340  [0.0, 0.0, 1.0]           2   [0.107, 0.881, 0.83]   \n",
       " 54    340  [0.0, 0.0, 1.0]           2  [0.062, 0.561, 0.836]   \n",
       " 55    340  [0.0, 0.0, 1.0]           2  [0.159, 0.721, 0.727]   \n",
       " 56    340  [0.0, 0.0, 1.0]           2       [0.0, 0.86, 1.0]   \n",
       " 57    340  [0.0, 0.0, 1.0]           2  [0.202, 0.972, 0.721]   \n",
       " 58    340  [0.0, 0.0, 1.0]           2  [0.188, 0.804, 0.715]   \n",
       " 59    340  [0.0, 0.0, 1.0]           2  [0.123, 0.646, 0.757]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     350  [1.0, 0.0, 0.0]           0  [0.931, 0.291, 0.061]   \n",
       " 1     350  [0.0, 0.0, 1.0]           2  [0.132, 0.645, 0.785]   \n",
       " 2     350  [1.0, 0.0, 0.0]           0  [0.938, 0.384, 0.046]   \n",
       " 3     350  [0.0, 0.0, 1.0]           2  [0.227, 0.735, 0.636]   \n",
       " 4     350  [1.0, 0.0, 0.0]           0   [0.925, 0.47, 0.082]   \n",
       " 5     350  [1.0, 0.0, 0.0]           0  [0.979, 0.505, 0.032]   \n",
       " 6     350  [1.0, 0.0, 0.0]           0   [0.946, 0.066, 0.05]   \n",
       " 7     350  [1.0, 0.0, 0.0]           0    [0.928, 0.0, 0.035]   \n",
       " 8     350  [0.0, 0.0, 1.0]           2  [0.114, 0.677, 0.797]   \n",
       " 9     350  [0.0, 0.0, 1.0]           2  [0.184, 0.884, 0.693]   \n",
       " 10    350  [0.0, 0.0, 1.0]           2  [0.083, 0.705, 0.818]   \n",
       " 11    350  [0.0, 0.0, 1.0]           2  [0.082, 0.692, 0.811]   \n",
       " 12    350  [0.0, 0.0, 1.0]           2  [0.111, 0.738, 0.797]   \n",
       " 13    350  [1.0, 0.0, 0.0]           0  [0.907, 0.256, 0.065]   \n",
       " 14    350  [1.0, 0.0, 0.0]           0      [1.0, 0.288, 0.0]   \n",
       " 15    350  [0.0, 0.0, 1.0]           2  [0.152, 0.727, 0.739]   \n",
       " 16    350  [0.0, 0.0, 1.0]           2  [0.212, 0.764, 0.644]   \n",
       " 17    350  [1.0, 0.0, 0.0]           0  [0.878, 0.472, 0.119]   \n",
       " 18    350  [1.0, 0.0, 0.0]           0  [0.917, 0.287, 0.073]   \n",
       " 19    350  [1.0, 0.0, 0.0]           0  [0.861, 0.318, 0.122]   \n",
       " 20    350  [1.0, 0.0, 0.0]           0    [0.981, 0.1, 0.002]   \n",
       " 21    350  [1.0, 0.0, 0.0]           0  [0.929, 0.384, 0.077]   \n",
       " 22    350  [1.0, 0.0, 0.0]           0  [0.915, 0.245, 0.087]   \n",
       " 23    350  [1.0, 0.0, 0.0]           0  [0.942, 0.517, 0.058]   \n",
       " 24    350  [1.0, 0.0, 0.0]           0  [0.915, 0.328, 0.076]   \n",
       " 25    350  [1.0, 0.0, 0.0]           0    [0.93, 0.306, 0.06]   \n",
       " 26    350  [1.0, 0.0, 0.0]           0  [0.858, 0.747, 0.159]   \n",
       " 27    350  [1.0, 0.0, 0.0]           0  [0.853, 0.355, 0.106]   \n",
       " 28    350  [1.0, 0.0, 0.0]           0  [0.927, 0.216, 0.059]   \n",
       " 29    350  [1.0, 0.0, 0.0]           0  [0.921, 0.364, 0.075]   \n",
       " 30    350  [0.0, 1.0, 0.0]           2   [0.329, 0.76, 0.542]   \n",
       " 31    350  [0.0, 1.0, 0.0]           2  [0.269, 0.576, 0.586]   \n",
       " 32    350  [0.0, 1.0, 0.0]           1    [0.51, 0.87, 0.397]   \n",
       " 33    350  [0.0, 1.0, 0.0]           2  [0.274, 0.636, 0.627]   \n",
       " 34    350  [0.0, 1.0, 0.0]           1    [0.445, 1.0, 0.481]   \n",
       " 35    350  [0.0, 1.0, 0.0]           2  [0.284, 0.558, 0.609]   \n",
       " 36    350  [0.0, 1.0, 0.0]           2  [0.319, 0.727, 0.531]   \n",
       " 37    350  [0.0, 1.0, 0.0]           2   [0.247, 0.68, 0.596]   \n",
       " 38    350  [0.0, 1.0, 0.0]           2   [0.31, 0.639, 0.587]   \n",
       " 39    350  [0.0, 1.0, 0.0]           2  [0.286, 0.716, 0.588]   \n",
       " 40    350  [0.0, 1.0, 0.0]           1  [0.453, 0.731, 0.463]   \n",
       " 41    350  [0.0, 1.0, 0.0]           1  [0.402, 0.847, 0.507]   \n",
       " 42    350  [0.0, 1.0, 0.0]           1  [0.382, 0.734, 0.514]   \n",
       " 43    350  [0.0, 1.0, 0.0]           2    [0.213, 0.83, 0.67]   \n",
       " 44    350  [0.0, 1.0, 0.0]           2  [0.311, 0.563, 0.527]   \n",
       " 45    350  [0.0, 1.0, 0.0]           2  [0.246, 0.587, 0.643]   \n",
       " 46    350  [0.0, 1.0, 0.0]           1    [0.349, 0.83, 0.53]   \n",
       " 47    350  [0.0, 1.0, 0.0]           2  [0.296, 0.668, 0.575]   \n",
       " 48    350  [0.0, 1.0, 0.0]           1  [0.352, 0.785, 0.527]   \n",
       " 49    350  [0.0, 1.0, 0.0]           1  [0.363, 0.708, 0.507]   \n",
       " 50    350  [0.0, 1.0, 0.0]           1  [0.516, 0.808, 0.398]   \n",
       " 51    350  [0.0, 0.0, 1.0]           2  [0.154, 0.785, 0.729]   \n",
       " 52    350  [0.0, 0.0, 1.0]           2   [0.079, 0.72, 0.859]   \n",
       " 53    350  [0.0, 0.0, 1.0]           2  [0.105, 0.884, 0.831]   \n",
       " 54    350  [0.0, 0.0, 1.0]           2  [0.062, 0.564, 0.838]   \n",
       " 55    350  [0.0, 0.0, 1.0]           2  [0.157, 0.724, 0.729]   \n",
       " 56    350  [0.0, 0.0, 1.0]           2      [0.0, 0.866, 1.0]   \n",
       " 57    350  [0.0, 0.0, 1.0]           2  [0.199, 0.974, 0.722]   \n",
       " 58    350  [0.0, 0.0, 1.0]           2  [0.185, 0.807, 0.716]   \n",
       " 59    350  [0.0, 0.0, 1.0]           2  [0.121, 0.649, 0.759]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     360  [1.0, 0.0, 0.0]           0   [0.932, 0.291, 0.06]   \n",
       " 1     360  [0.0, 0.0, 1.0]           2  [0.131, 0.649, 0.786]   \n",
       " 2     360  [1.0, 0.0, 0.0]           0  [0.938, 0.383, 0.045]   \n",
       " 3     360  [0.0, 0.0, 1.0]           2  [0.225, 0.736, 0.638]   \n",
       " 4     360  [1.0, 0.0, 0.0]           0  [0.924, 0.469, 0.081]   \n",
       " 5     360  [1.0, 0.0, 0.0]           0  [0.978, 0.503, 0.032]   \n",
       " 6     360  [1.0, 0.0, 0.0]           0   [0.947, 0.067, 0.05]   \n",
       " 7     360  [1.0, 0.0, 0.0]           0     [0.93, 0.0, 0.035]   \n",
       " 8     360  [0.0, 0.0, 1.0]           2   [0.112, 0.68, 0.799]   \n",
       " 9     360  [0.0, 0.0, 1.0]           2  [0.181, 0.885, 0.694]   \n",
       " 10    360  [0.0, 0.0, 1.0]           2   [0.082, 0.708, 0.82]   \n",
       " 11    360  [0.0, 0.0, 1.0]           2  [0.081, 0.694, 0.814]   \n",
       " 12    360  [0.0, 0.0, 1.0]           2  [0.109, 0.741, 0.799]   \n",
       " 13    360  [1.0, 0.0, 0.0]           0  [0.908, 0.255, 0.065]   \n",
       " 14    360  [1.0, 0.0, 0.0]           0      [1.0, 0.286, 0.0]   \n",
       " 15    360  [0.0, 0.0, 1.0]           2    [0.15, 0.729, 0.74]   \n",
       " 16    360  [0.0, 0.0, 1.0]           2  [0.209, 0.765, 0.646]   \n",
       " 17    360  [1.0, 0.0, 0.0]           0  [0.878, 0.471, 0.117]   \n",
       " 18    360  [1.0, 0.0, 0.0]           0  [0.917, 0.287, 0.072]   \n",
       " 19    360  [1.0, 0.0, 0.0]           0  [0.861, 0.318, 0.121]   \n",
       " 20    360  [1.0, 0.0, 0.0]           0  [0.982, 0.099, 0.002]   \n",
       " 21    360  [1.0, 0.0, 0.0]           0  [0.929, 0.383, 0.076]   \n",
       " 22    360  [1.0, 0.0, 0.0]           0  [0.915, 0.245, 0.086]   \n",
       " 23    360  [1.0, 0.0, 0.0]           0  [0.941, 0.516, 0.058]   \n",
       " 24    360  [1.0, 0.0, 0.0]           0  [0.915, 0.328, 0.075]   \n",
       " 25    360  [1.0, 0.0, 0.0]           0   [0.93, 0.305, 0.059]   \n",
       " 26    360  [1.0, 0.0, 0.0]           0  [0.856, 0.746, 0.157]   \n",
       " 27    360  [1.0, 0.0, 0.0]           0  [0.853, 0.353, 0.106]   \n",
       " 28    360  [1.0, 0.0, 0.0]           0  [0.928, 0.216, 0.058]   \n",
       " 29    360  [1.0, 0.0, 0.0]           0  [0.921, 0.363, 0.074]   \n",
       " 30    360  [0.0, 1.0, 0.0]           2  [0.326, 0.761, 0.542]   \n",
       " 31    360  [0.0, 1.0, 0.0]           2  [0.267, 0.578, 0.587]   \n",
       " 32    360  [0.0, 1.0, 0.0]           1   [0.508, 0.87, 0.396]   \n",
       " 33    360  [0.0, 1.0, 0.0]           2  [0.272, 0.639, 0.627]   \n",
       " 34    360  [0.0, 1.0, 0.0]           1     [0.442, 1.0, 0.48]   \n",
       " 35    360  [0.0, 1.0, 0.0]           2   [0.282, 0.561, 0.61]   \n",
       " 36    360  [0.0, 1.0, 0.0]           2  [0.316, 0.727, 0.532]   \n",
       " 37    360  [0.0, 1.0, 0.0]           2  [0.245, 0.681, 0.597]   \n",
       " 38    360  [0.0, 1.0, 0.0]           2  [0.307, 0.641, 0.587]   \n",
       " 39    360  [0.0, 1.0, 0.0]           2  [0.284, 0.718, 0.589]   \n",
       " 40    360  [0.0, 1.0, 0.0]           1   [0.45, 0.732, 0.462]   \n",
       " 41    360  [0.0, 1.0, 0.0]           1  [0.399, 0.848, 0.506]   \n",
       " 42    360  [0.0, 1.0, 0.0]           1  [0.379, 0.735, 0.514]   \n",
       " 43    360  [0.0, 1.0, 0.0]           2   [0.21, 0.832, 0.671]   \n",
       " 44    360  [0.0, 1.0, 0.0]           2  [0.309, 0.564, 0.528]   \n",
       " 45    360  [0.0, 1.0, 0.0]           2   [0.244, 0.59, 0.644]   \n",
       " 46    360  [0.0, 1.0, 0.0]           1   [0.347, 0.831, 0.53]   \n",
       " 47    360  [0.0, 1.0, 0.0]           2   [0.294, 0.67, 0.575]   \n",
       " 48    360  [0.0, 1.0, 0.0]           1  [0.349, 0.786, 0.527]   \n",
       " 49    360  [0.0, 1.0, 0.0]           1   [0.36, 0.709, 0.507]   \n",
       " 50    360  [0.0, 1.0, 0.0]           1  [0.513, 0.808, 0.398]   \n",
       " 51    360  [0.0, 0.0, 1.0]           2  [0.152, 0.787, 0.731]   \n",
       " 52    360  [0.0, 0.0, 1.0]           2  [0.078, 0.724, 0.859]   \n",
       " 53    360  [0.0, 0.0, 1.0]           2  [0.103, 0.888, 0.832]   \n",
       " 54    360  [0.0, 0.0, 1.0]           2  [0.061, 0.568, 0.841]   \n",
       " 55    360  [0.0, 0.0, 1.0]           2   [0.155, 0.726, 0.73]   \n",
       " 56    360  [0.0, 0.0, 1.0]           2      [0.0, 0.871, 1.0]   \n",
       " 57    360  [0.0, 0.0, 1.0]           2  [0.196, 0.976, 0.722]   \n",
       " 58    360  [0.0, 0.0, 1.0]           2  [0.182, 0.809, 0.717]   \n",
       " 59    360  [0.0, 0.0, 1.0]           2   [0.12, 0.651, 0.761]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     370  [1.0, 0.0, 0.0]           0    [0.932, 0.29, 0.06]   \n",
       " 1     370  [0.0, 0.0, 1.0]           2  [0.129, 0.654, 0.786]   \n",
       " 2     370  [1.0, 0.0, 0.0]           0  [0.938, 0.381, 0.045]   \n",
       " 3     370  [0.0, 0.0, 1.0]           2  [0.222, 0.738, 0.639]   \n",
       " 4     370  [1.0, 0.0, 0.0]           0   [0.924, 0.468, 0.08]   \n",
       " 5     370  [1.0, 0.0, 0.0]           0  [0.978, 0.501, 0.031]   \n",
       " 6     370  [1.0, 0.0, 0.0]           0  [0.947, 0.067, 0.049]   \n",
       " 7     370  [1.0, 0.0, 0.0]           0    [0.931, 0.0, 0.035]   \n",
       " 8     370  [0.0, 0.0, 1.0]           2     [0.11, 0.683, 0.8]   \n",
       " 9     370  [0.0, 0.0, 1.0]           2  [0.179, 0.886, 0.696]   \n",
       " 10    370  [0.0, 0.0, 1.0]           2  [0.081, 0.711, 0.822]   \n",
       " 11    370  [0.0, 0.0, 1.0]           2   [0.08, 0.697, 0.816]   \n",
       " 12    370  [0.0, 0.0, 1.0]           2  [0.107, 0.743, 0.801]   \n",
       " 13    370  [1.0, 0.0, 0.0]           0  [0.908, 0.254, 0.064]   \n",
       " 14    370  [1.0, 0.0, 0.0]           0      [1.0, 0.284, 0.0]   \n",
       " 15    370  [0.0, 0.0, 1.0]           2  [0.148, 0.732, 0.742]   \n",
       " 16    370  [0.0, 0.0, 1.0]           2  [0.207, 0.767, 0.647]   \n",
       " 17    370  [1.0, 0.0, 0.0]           0  [0.877, 0.471, 0.116]   \n",
       " 18    370  [1.0, 0.0, 0.0]           0  [0.917, 0.286, 0.071]   \n",
       " 19    370  [1.0, 0.0, 0.0]           0   [0.861, 0.318, 0.12]   \n",
       " 20    370  [1.0, 0.0, 0.0]           0  [0.983, 0.099, 0.002]   \n",
       " 21    370  [1.0, 0.0, 0.0]           0  [0.929, 0.382, 0.075]   \n",
       " 22    370  [1.0, 0.0, 0.0]           0  [0.915, 0.245, 0.085]   \n",
       " 23    370  [1.0, 0.0, 0.0]           0  [0.941, 0.514, 0.057]   \n",
       " 24    370  [1.0, 0.0, 0.0]           0  [0.915, 0.327, 0.075]   \n",
       " 25    370  [1.0, 0.0, 0.0]           0   [0.93, 0.304, 0.059]   \n",
       " 26    370  [1.0, 0.0, 0.0]           0  [0.855, 0.745, 0.155]   \n",
       " 27    370  [1.0, 0.0, 0.0]           0  [0.853, 0.352, 0.105]   \n",
       " 28    370  [1.0, 0.0, 0.0]           0  [0.928, 0.216, 0.058]   \n",
       " 29    370  [1.0, 0.0, 0.0]           0  [0.921, 0.362, 0.073]   \n",
       " 30    370  [0.0, 1.0, 0.0]           2  [0.323, 0.762, 0.542]   \n",
       " 31    370  [0.0, 1.0, 0.0]           2   [0.265, 0.58, 0.588]   \n",
       " 32    370  [0.0, 1.0, 0.0]           1   [0.505, 0.87, 0.395]   \n",
       " 33    370  [0.0, 1.0, 0.0]           2  [0.269, 0.642, 0.627]   \n",
       " 34    370  [0.0, 1.0, 0.0]           1    [0.438, 1.0, 0.479]   \n",
       " 35    370  [0.0, 1.0, 0.0]           2   [0.279, 0.564, 0.61]   \n",
       " 36    370  [0.0, 1.0, 0.0]           2  [0.314, 0.728, 0.532]   \n",
       " 37    370  [0.0, 1.0, 0.0]           2  [0.243, 0.682, 0.599]   \n",
       " 38    370  [0.0, 1.0, 0.0]           2  [0.305, 0.644, 0.588]   \n",
       " 39    370  [0.0, 1.0, 0.0]           2   [0.281, 0.72, 0.589]   \n",
       " 40    370  [0.0, 1.0, 0.0]           1  [0.447, 0.734, 0.462]   \n",
       " 41    370  [0.0, 1.0, 0.0]           1  [0.396, 0.849, 0.506]   \n",
       " 42    370  [0.0, 1.0, 0.0]           1  [0.377, 0.737, 0.514]   \n",
       " 43    370  [0.0, 1.0, 0.0]           2  [0.208, 0.833, 0.672]   \n",
       " 44    370  [0.0, 1.0, 0.0]           2  [0.307, 0.565, 0.529]   \n",
       " 45    370  [0.0, 1.0, 0.0]           2  [0.242, 0.593, 0.644]   \n",
       " 46    370  [0.0, 1.0, 0.0]           1   [0.344, 0.832, 0.53]   \n",
       " 47    370  [0.0, 1.0, 0.0]           2  [0.291, 0.672, 0.576]   \n",
       " 48    370  [0.0, 1.0, 0.0]           1  [0.346, 0.787, 0.528]   \n",
       " 49    370  [0.0, 1.0, 0.0]           1  [0.358, 0.711, 0.508]   \n",
       " 50    370  [0.0, 1.0, 0.0]           1   [0.51, 0.808, 0.397]   \n",
       " 51    370  [0.0, 0.0, 1.0]           2   [0.15, 0.789, 0.732]   \n",
       " 52    370  [0.0, 0.0, 1.0]           2   [0.076, 0.729, 0.86]   \n",
       " 53    370  [0.0, 0.0, 1.0]           2  [0.101, 0.891, 0.832]   \n",
       " 54    370  [0.0, 0.0, 1.0]           2   [0.06, 0.571, 0.843]   \n",
       " 55    370  [0.0, 0.0, 1.0]           2  [0.153, 0.729, 0.731]   \n",
       " 56    370  [0.0, 0.0, 1.0]           2      [0.0, 0.877, 1.0]   \n",
       " 57    370  [0.0, 0.0, 1.0]           2  [0.193, 0.978, 0.722]   \n",
       " 58    370  [0.0, 0.0, 1.0]           2   [0.18, 0.811, 0.718]   \n",
       " 59    370  [0.0, 0.0, 1.0]           2  [0.118, 0.654, 0.762]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     380  [1.0, 0.0, 0.0]           0  [0.932, 0.289, 0.059]   \n",
       " 1     380  [0.0, 0.0, 1.0]           2  [0.127, 0.658, 0.787]   \n",
       " 2     380  [1.0, 0.0, 0.0]           0  [0.938, 0.379, 0.045]   \n",
       " 3     380  [0.0, 0.0, 1.0]           2    [0.22, 0.739, 0.64]   \n",
       " 4     380  [1.0, 0.0, 0.0]           0  [0.924, 0.467, 0.079]   \n",
       " 5     380  [1.0, 0.0, 0.0]           0  [0.977, 0.499, 0.031]   \n",
       " 6     380  [1.0, 0.0, 0.0]           0  [0.948, 0.067, 0.049]   \n",
       " 7     380  [1.0, 0.0, 0.0]           0    [0.932, 0.0, 0.035]   \n",
       " 8     380  [0.0, 0.0, 1.0]           2  [0.108, 0.686, 0.802]   \n",
       " 9     380  [0.0, 0.0, 1.0]           2  [0.176, 0.887, 0.697]   \n",
       " 10    380  [0.0, 0.0, 1.0]           2  [0.079, 0.714, 0.824]   \n",
       " 11    380  [0.0, 0.0, 1.0]           2    [0.079, 0.7, 0.818]   \n",
       " 12    380  [0.0, 0.0, 1.0]           2  [0.106, 0.746, 0.802]   \n",
       " 13    380  [1.0, 0.0, 0.0]           0  [0.908, 0.253, 0.064]   \n",
       " 14    380  [1.0, 0.0, 0.0]           0      [1.0, 0.283, 0.0]   \n",
       " 15    380  [0.0, 0.0, 1.0]           2  [0.145, 0.734, 0.743]   \n",
       " 16    380  [0.0, 0.0, 1.0]           2  [0.205, 0.768, 0.649]   \n",
       " 17    380  [1.0, 0.0, 0.0]           0   [0.877, 0.47, 0.115]   \n",
       " 18    380  [1.0, 0.0, 0.0]           0  [0.918, 0.286, 0.071]   \n",
       " 19    380  [1.0, 0.0, 0.0]           0   [0.861, 0.318, 0.12]   \n",
       " 20    380  [1.0, 0.0, 0.0]           0  [0.983, 0.098, 0.002]   \n",
       " 21    380  [1.0, 0.0, 0.0]           0  [0.929, 0.381, 0.074]   \n",
       " 22    380  [1.0, 0.0, 0.0]           0  [0.915, 0.246, 0.084]   \n",
       " 23    380  [1.0, 0.0, 0.0]           0   [0.94, 0.512, 0.056]   \n",
       " 24    380  [1.0, 0.0, 0.0]           0  [0.915, 0.326, 0.074]   \n",
       " 25    380  [1.0, 0.0, 0.0]           0   [0.93, 0.303, 0.058]   \n",
       " 26    380  [1.0, 0.0, 0.0]           0  [0.854, 0.744, 0.154]   \n",
       " 27    380  [1.0, 0.0, 0.0]           0  [0.853, 0.351, 0.105]   \n",
       " 28    380  [1.0, 0.0, 0.0]           0  [0.929, 0.215, 0.057]   \n",
       " 29    380  [1.0, 0.0, 0.0]           0  [0.921, 0.362, 0.073]   \n",
       " 30    380  [0.0, 1.0, 0.0]           1  [0.321, 0.764, 0.543]   \n",
       " 31    380  [0.0, 1.0, 0.0]           2  [0.263, 0.581, 0.589]   \n",
       " 32    380  [0.0, 1.0, 0.0]           1  [0.502, 0.869, 0.395]   \n",
       " 33    380  [0.0, 1.0, 0.0]           2  [0.266, 0.645, 0.627]   \n",
       " 34    380  [0.0, 1.0, 0.0]           1    [0.435, 1.0, 0.479]   \n",
       " 35    380  [0.0, 1.0, 0.0]           2  [0.277, 0.567, 0.611]   \n",
       " 36    380  [0.0, 1.0, 0.0]           2  [0.311, 0.729, 0.533]   \n",
       " 37    380  [0.0, 1.0, 0.0]           2     [0.24, 0.683, 0.6]   \n",
       " 38    380  [0.0, 1.0, 0.0]           2  [0.302, 0.646, 0.588]   \n",
       " 39    380  [0.0, 1.0, 0.0]           2   [0.279, 0.721, 0.59]   \n",
       " 40    380  [0.0, 1.0, 0.0]           1  [0.445, 0.735, 0.461]   \n",
       " 41    380  [0.0, 1.0, 0.0]           1   [0.393, 0.85, 0.506]   \n",
       " 42    380  [0.0, 1.0, 0.0]           1  [0.374, 0.738, 0.514]   \n",
       " 43    380  [0.0, 1.0, 0.0]           2  [0.205, 0.835, 0.673]   \n",
       " 44    380  [0.0, 1.0, 0.0]           2   [0.305, 0.567, 0.53]   \n",
       " 45    380  [0.0, 1.0, 0.0]           2  [0.239, 0.596, 0.645]   \n",
       " 46    380  [0.0, 1.0, 0.0]           1   [0.341, 0.833, 0.53]   \n",
       " 47    380  [0.0, 1.0, 0.0]           2  [0.289, 0.673, 0.576]   \n",
       " 48    380  [0.0, 1.0, 0.0]           1  [0.343, 0.788, 0.528]   \n",
       " 49    380  [0.0, 1.0, 0.0]           1  [0.355, 0.712, 0.508]   \n",
       " 50    380  [0.0, 1.0, 0.0]           1  [0.508, 0.808, 0.397]   \n",
       " 51    380  [0.0, 0.0, 1.0]           2  [0.148, 0.792, 0.733]   \n",
       " 52    380  [0.0, 0.0, 1.0]           2  [0.075, 0.734, 0.861]   \n",
       " 53    380  [0.0, 0.0, 1.0]           2  [0.099, 0.894, 0.833]   \n",
       " 54    380  [0.0, 0.0, 1.0]           2  [0.059, 0.574, 0.845]   \n",
       " 55    380  [0.0, 0.0, 1.0]           2  [0.151, 0.732, 0.732]   \n",
       " 56    380  [0.0, 0.0, 1.0]           2      [0.0, 0.882, 1.0]   \n",
       " 57    380  [0.0, 0.0, 1.0]           2    [0.19, 0.98, 0.723]   \n",
       " 58    380  [0.0, 0.0, 1.0]           2  [0.177, 0.814, 0.719]   \n",
       " 59    380  [0.0, 0.0, 1.0]           2  [0.116, 0.657, 0.764]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     390  [1.0, 0.0, 0.0]           0  [0.932, 0.289, 0.058]   \n",
       " 1     390  [0.0, 0.0, 1.0]           2  [0.125, 0.662, 0.788]   \n",
       " 2     390  [1.0, 0.0, 0.0]           0  [0.938, 0.378, 0.044]   \n",
       " 3     390  [0.0, 0.0, 1.0]           2   [0.218, 0.74, 0.641]   \n",
       " 4     390  [1.0, 0.0, 0.0]           0  [0.924, 0.467, 0.078]   \n",
       " 5     390  [1.0, 0.0, 0.0]           0   [0.977, 0.497, 0.03]   \n",
       " 6     390  [1.0, 0.0, 0.0]           0  [0.949, 0.068, 0.048]   \n",
       " 7     390  [1.0, 0.0, 0.0]           0    [0.933, 0.0, 0.035]   \n",
       " 8     390  [0.0, 0.0, 1.0]           2  [0.107, 0.689, 0.803]   \n",
       " 9     390  [0.0, 0.0, 1.0]           2  [0.174, 0.888, 0.698]   \n",
       " 10    390  [0.0, 0.0, 1.0]           2  [0.078, 0.716, 0.825]   \n",
       " 11    390  [0.0, 0.0, 1.0]           2  [0.078, 0.702, 0.819]   \n",
       " 12    390  [0.0, 0.0, 1.0]           2  [0.104, 0.749, 0.804]   \n",
       " 13    390  [1.0, 0.0, 0.0]           0  [0.909, 0.253, 0.064]   \n",
       " 14    390  [1.0, 0.0, 0.0]           0      [1.0, 0.281, 0.0]   \n",
       " 15    390  [0.0, 0.0, 1.0]           2  [0.143, 0.737, 0.744]   \n",
       " 16    390  [0.0, 0.0, 1.0]           2   [0.202, 0.769, 0.65]   \n",
       " 17    390  [1.0, 0.0, 0.0]           0   [0.877, 0.47, 0.114]   \n",
       " 18    390  [1.0, 0.0, 0.0]           0   [0.918, 0.285, 0.07]   \n",
       " 19    390  [1.0, 0.0, 0.0]           0  [0.861, 0.317, 0.119]   \n",
       " 20    390  [1.0, 0.0, 0.0]           0  [0.984, 0.097, 0.002]   \n",
       " 21    390  [1.0, 0.0, 0.0]           0  [0.929, 0.381, 0.073]   \n",
       " 22    390  [1.0, 0.0, 0.0]           0  [0.916, 0.246, 0.083]   \n",
       " 23    390  [1.0, 0.0, 0.0]           0    [0.94, 0.51, 0.055]   \n",
       " 24    390  [1.0, 0.0, 0.0]           0  [0.915, 0.326, 0.073]   \n",
       " 25    390  [1.0, 0.0, 0.0]           0   [0.93, 0.302, 0.057]   \n",
       " 26    390  [1.0, 0.0, 0.0]           0  [0.853, 0.743, 0.152]   \n",
       " 27    390  [1.0, 0.0, 0.0]           0   [0.854, 0.35, 0.104]   \n",
       " 28    390  [1.0, 0.0, 0.0]           0  [0.929, 0.215, 0.056]   \n",
       " 29    390  [1.0, 0.0, 0.0]           0  [0.921, 0.361, 0.072]   \n",
       " 30    390  [0.0, 1.0, 0.0]           1  [0.318, 0.765, 0.543]   \n",
       " 31    390  [0.0, 1.0, 0.0]           2   [0.261, 0.583, 0.59]   \n",
       " 32    390  [0.0, 1.0, 0.0]           1    [0.5, 0.869, 0.394]   \n",
       " 33    390  [0.0, 1.0, 0.0]           2  [0.264, 0.648, 0.628]   \n",
       " 34    390  [0.0, 1.0, 0.0]           1    [0.432, 1.0, 0.478]   \n",
       " 35    390  [0.0, 1.0, 0.0]           2  [0.275, 0.569, 0.611]   \n",
       " 36    390  [0.0, 1.0, 0.0]           2   [0.309, 0.73, 0.534]   \n",
       " 37    390  [0.0, 1.0, 0.0]           2  [0.238, 0.684, 0.601]   \n",
       " 38    390  [0.0, 1.0, 0.0]           2    [0.3, 0.649, 0.588]   \n",
       " 39    390  [0.0, 1.0, 0.0]           2  [0.276, 0.723, 0.591]   \n",
       " 40    390  [0.0, 1.0, 0.0]           1   [0.442, 0.736, 0.46]   \n",
       " 41    390  [0.0, 1.0, 0.0]           1   [0.39, 0.851, 0.505]   \n",
       " 42    390  [0.0, 1.0, 0.0]           1  [0.371, 0.739, 0.513]   \n",
       " 43    390  [0.0, 1.0, 0.0]           2  [0.202, 0.837, 0.673]   \n",
       " 44    390  [0.0, 1.0, 0.0]           2  [0.303, 0.568, 0.531]   \n",
       " 45    390  [0.0, 1.0, 0.0]           2  [0.237, 0.599, 0.645]   \n",
       " 46    390  [0.0, 1.0, 0.0]           1   [0.338, 0.834, 0.53]   \n",
       " 47    390  [0.0, 1.0, 0.0]           2  [0.286, 0.675, 0.577]   \n",
       " 48    390  [0.0, 1.0, 0.0]           1  [0.341, 0.789, 0.528]   \n",
       " 49    390  [0.0, 1.0, 0.0]           1  [0.353, 0.713, 0.508]   \n",
       " 50    390  [0.0, 1.0, 0.0]           1  [0.505, 0.808, 0.396]   \n",
       " 51    390  [0.0, 0.0, 1.0]           2  [0.146, 0.794, 0.734]   \n",
       " 52    390  [0.0, 0.0, 1.0]           2  [0.074, 0.738, 0.861]   \n",
       " 53    390  [0.0, 0.0, 1.0]           2  [0.097, 0.898, 0.834]   \n",
       " 54    390  [0.0, 0.0, 1.0]           2  [0.059, 0.578, 0.846]   \n",
       " 55    390  [0.0, 0.0, 1.0]           2  [0.149, 0.734, 0.734]   \n",
       " 56    390  [0.0, 0.0, 1.0]           2      [0.0, 0.887, 1.0]   \n",
       " 57    390  [0.0, 0.0, 1.0]           2  [0.187, 0.982, 0.723]   \n",
       " 58    390  [0.0, 0.0, 1.0]           2   [0.175, 0.816, 0.72]   \n",
       " 59    390  [0.0, 0.0, 1.0]           2  [0.115, 0.659, 0.766]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     400  [1.0, 0.0, 0.0]           0  [0.933, 0.288, 0.058]   \n",
       " 1     400  [0.0, 0.0, 1.0]           2  [0.123, 0.666, 0.788]   \n",
       " 2     400  [1.0, 0.0, 0.0]           0  [0.938, 0.376, 0.044]   \n",
       " 3     400  [0.0, 0.0, 1.0]           2  [0.215, 0.742, 0.642]   \n",
       " 4     400  [1.0, 0.0, 0.0]           0  [0.923, 0.466, 0.077]   \n",
       " 5     400  [1.0, 0.0, 0.0]           0   [0.977, 0.496, 0.03]   \n",
       " 6     400  [1.0, 0.0, 0.0]           0  [0.949, 0.068, 0.048]   \n",
       " 7     400  [1.0, 0.0, 0.0]           0    [0.934, 0.0, 0.035]   \n",
       " 8     400  [0.0, 0.0, 1.0]           2  [0.105, 0.692, 0.804]   \n",
       " 9     400  [0.0, 0.0, 1.0]           2  [0.171, 0.889, 0.699]   \n",
       " 10    400  [0.0, 0.0, 1.0]           2  [0.077, 0.719, 0.827]   \n",
       " 11    400  [0.0, 0.0, 1.0]           2  [0.076, 0.704, 0.821]   \n",
       " 12    400  [0.0, 0.0, 1.0]           2  [0.102, 0.751, 0.805]   \n",
       " 13    400  [1.0, 0.0, 0.0]           0  [0.909, 0.252, 0.063]   \n",
       " 14    400  [1.0, 0.0, 0.0]           0      [1.0, 0.279, 0.0]   \n",
       " 15    400  [0.0, 0.0, 1.0]           2  [0.142, 0.739, 0.746]   \n",
       " 16    400  [0.0, 0.0, 1.0]           2     [0.2, 0.77, 0.651]   \n",
       " 17    400  [1.0, 0.0, 0.0]           0  [0.877, 0.469, 0.113]   \n",
       " 18    400  [1.0, 0.0, 0.0]           0  [0.918, 0.285, 0.069]   \n",
       " 19    400  [1.0, 0.0, 0.0]           0  [0.862, 0.317, 0.118]   \n",
       " 20    400  [1.0, 0.0, 0.0]           0  [0.984, 0.097, 0.002]   \n",
       " 21    400  [1.0, 0.0, 0.0]           0   [0.929, 0.38, 0.072]   \n",
       " 22    400  [1.0, 0.0, 0.0]           0  [0.916, 0.246, 0.082]   \n",
       " 23    400  [1.0, 0.0, 0.0]           0   [0.94, 0.509, 0.055]   \n",
       " 24    400  [1.0, 0.0, 0.0]           0  [0.915, 0.325, 0.072]   \n",
       " 25    400  [1.0, 0.0, 0.0]           0  [0.931, 0.301, 0.057]   \n",
       " 26    400  [1.0, 0.0, 0.0]           0   [0.852, 0.742, 0.15]   \n",
       " 27    400  [1.0, 0.0, 0.0]           0  [0.854, 0.349, 0.104]   \n",
       " 28    400  [1.0, 0.0, 0.0]           0   [0.93, 0.215, 0.056]   \n",
       " 29    400  [1.0, 0.0, 0.0]           0   [0.921, 0.36, 0.071]   \n",
       " 30    400  [0.0, 1.0, 0.0]           1  [0.315, 0.766, 0.543]   \n",
       " 31    400  [0.0, 1.0, 0.0]           2  [0.259, 0.585, 0.591]   \n",
       " 32    400  [0.0, 1.0, 0.0]           1  [0.497, 0.869, 0.394]   \n",
       " 33    400  [0.0, 1.0, 0.0]           2   [0.261, 0.65, 0.628]   \n",
       " 34    400  [0.0, 1.0, 0.0]           1    [0.429, 1.0, 0.477]   \n",
       " 35    400  [0.0, 1.0, 0.0]           2  [0.272, 0.572, 0.611]   \n",
       " 36    400  [0.0, 1.0, 0.0]           2   [0.306, 0.73, 0.535]   \n",
       " 37    400  [0.0, 1.0, 0.0]           2  [0.236, 0.685, 0.603]   \n",
       " 38    400  [0.0, 1.0, 0.0]           2  [0.297, 0.651, 0.588]   \n",
       " 39    400  [0.0, 1.0, 0.0]           2  [0.274, 0.724, 0.591]   \n",
       " 40    400  [0.0, 1.0, 0.0]           1    [0.44, 0.737, 0.46]   \n",
       " 41    400  [0.0, 1.0, 0.0]           1  [0.387, 0.852, 0.505]   \n",
       " 42    400  [0.0, 1.0, 0.0]           1  [0.369, 0.741, 0.513]   \n",
       " 43    400  [0.0, 1.0, 0.0]           2    [0.2, 0.839, 0.674]   \n",
       " 44    400  [0.0, 1.0, 0.0]           2  [0.301, 0.569, 0.532]   \n",
       " 45    400  [0.0, 1.0, 0.0]           2  [0.235, 0.601, 0.646]   \n",
       " 46    400  [0.0, 1.0, 0.0]           1   [0.335, 0.834, 0.53]   \n",
       " 47    400  [0.0, 1.0, 0.0]           2  [0.284, 0.677, 0.577]   \n",
       " 48    400  [0.0, 1.0, 0.0]           1   [0.338, 0.79, 0.528]   \n",
       " 49    400  [0.0, 1.0, 0.0]           1   [0.35, 0.714, 0.508]   \n",
       " 50    400  [0.0, 1.0, 0.0]           1  [0.503, 0.808, 0.396]   \n",
       " 51    400  [0.0, 0.0, 1.0]           2  [0.144, 0.796, 0.735]   \n",
       " 52    400  [0.0, 0.0, 1.0]           2  [0.072, 0.742, 0.862]   \n",
       " 53    400  [0.0, 0.0, 1.0]           2    [0.096, 0.9, 0.834]   \n",
       " 54    400  [0.0, 0.0, 1.0]           2   [0.058, 0.58, 0.848]   \n",
       " 55    400  [0.0, 0.0, 1.0]           2  [0.147, 0.737, 0.735]   \n",
       " 56    400  [0.0, 0.0, 1.0]           2      [0.0, 0.891, 1.0]   \n",
       " 57    400  [0.0, 0.0, 1.0]           2  [0.184, 0.984, 0.723]   \n",
       " 58    400  [0.0, 0.0, 1.0]           2  [0.173, 0.818, 0.721]   \n",
       " 59    400  [0.0, 0.0, 1.0]           2  [0.113, 0.662, 0.767]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     410  [1.0, 0.0, 0.0]           0  [0.933, 0.288, 0.057]   \n",
       " 1     410  [0.0, 0.0, 1.0]           2  [0.122, 0.669, 0.789]   \n",
       " 2     410  [1.0, 0.0, 0.0]           0  [0.938, 0.375, 0.043]   \n",
       " 3     410  [0.0, 0.0, 1.0]           2  [0.213, 0.743, 0.644]   \n",
       " 4     410  [1.0, 0.0, 0.0]           0  [0.923, 0.465, 0.076]   \n",
       " 5     410  [1.0, 0.0, 0.0]           0  [0.976, 0.494, 0.029]   \n",
       " 6     410  [1.0, 0.0, 0.0]           0   [0.95, 0.068, 0.047]   \n",
       " 7     410  [1.0, 0.0, 0.0]           0    [0.935, 0.0, 0.035]   \n",
       " 8     410  [0.0, 0.0, 1.0]           2  [0.104, 0.694, 0.806]   \n",
       " 9     410  [0.0, 0.0, 1.0]           2   [0.169, 0.89, 0.701]   \n",
       " 10    410  [0.0, 0.0, 1.0]           2  [0.076, 0.721, 0.829]   \n",
       " 11    410  [0.0, 0.0, 1.0]           2  [0.075, 0.706, 0.823]   \n",
       " 12    410  [0.0, 0.0, 1.0]           2  [0.101, 0.753, 0.807]   \n",
       " 13    410  [1.0, 0.0, 0.0]           0   [0.91, 0.251, 0.063]   \n",
       " 14    410  [1.0, 0.0, 0.0]           0      [1.0, 0.278, 0.0]   \n",
       " 15    410  [0.0, 0.0, 1.0]           2   [0.14, 0.741, 0.747]   \n",
       " 16    410  [0.0, 0.0, 1.0]           2  [0.198, 0.771, 0.653]   \n",
       " 17    410  [1.0, 0.0, 0.0]           0  [0.876, 0.469, 0.111]   \n",
       " 18    410  [1.0, 0.0, 0.0]           0  [0.918, 0.284, 0.068]   \n",
       " 19    410  [1.0, 0.0, 0.0]           0  [0.862, 0.317, 0.117]   \n",
       " 20    410  [1.0, 0.0, 0.0]           0  [0.985, 0.096, 0.003]   \n",
       " 21    410  [1.0, 0.0, 0.0]           0   [0.929, 0.38, 0.071]   \n",
       " 22    410  [1.0, 0.0, 0.0]           0  [0.916, 0.246, 0.081]   \n",
       " 23    410  [1.0, 0.0, 0.0]           0   [0.94, 0.508, 0.054]   \n",
       " 24    410  [1.0, 0.0, 0.0]           0  [0.916, 0.325, 0.072]   \n",
       " 25    410  [1.0, 0.0, 0.0]           0    [0.931, 0.3, 0.056]   \n",
       " 26    410  [1.0, 0.0, 0.0]           0  [0.851, 0.741, 0.149]   \n",
       " 27    410  [1.0, 0.0, 0.0]           0  [0.854, 0.348, 0.103]   \n",
       " 28    410  [1.0, 0.0, 0.0]           0   [0.93, 0.214, 0.055]   \n",
       " 29    410  [1.0, 0.0, 0.0]           0    [0.921, 0.36, 0.07]   \n",
       " 30    410  [0.0, 1.0, 0.0]           1  [0.313, 0.767, 0.544]   \n",
       " 31    410  [0.0, 1.0, 0.0]           2  [0.256, 0.587, 0.592]   \n",
       " 32    410  [0.0, 1.0, 0.0]           1  [0.495, 0.868, 0.393]   \n",
       " 33    410  [0.0, 1.0, 0.0]           2  [0.259, 0.653, 0.628]   \n",
       " 34    410  [0.0, 1.0, 0.0]           1    [0.426, 1.0, 0.476]   \n",
       " 35    410  [0.0, 1.0, 0.0]           2   [0.27, 0.575, 0.611]   \n",
       " 36    410  [0.0, 1.0, 0.0]           2  [0.304, 0.731, 0.535]   \n",
       " 37    410  [0.0, 1.0, 0.0]           2  [0.234, 0.686, 0.604]   \n",
       " 38    410  [0.0, 1.0, 0.0]           2  [0.295, 0.653, 0.588]   \n",
       " 39    410  [0.0, 1.0, 0.0]           2  [0.271, 0.726, 0.592]   \n",
       " 40    410  [0.0, 1.0, 0.0]           1  [0.437, 0.738, 0.459]   \n",
       " 41    410  [0.0, 1.0, 0.0]           1  [0.384, 0.853, 0.504]   \n",
       " 42    410  [0.0, 1.0, 0.0]           1  [0.366, 0.742, 0.513]   \n",
       " 43    410  [0.0, 1.0, 0.0]           2   [0.198, 0.84, 0.675]   \n",
       " 44    410  [0.0, 1.0, 0.0]           2   [0.299, 0.57, 0.533]   \n",
       " 45    410  [0.0, 1.0, 0.0]           2  [0.233, 0.604, 0.646]   \n",
       " 46    410  [0.0, 1.0, 0.0]           1   [0.333, 0.835, 0.53]   \n",
       " 47    410  [0.0, 1.0, 0.0]           2  [0.282, 0.679, 0.578]   \n",
       " 48    410  [0.0, 1.0, 0.0]           1  [0.336, 0.791, 0.528]   \n",
       " 49    410  [0.0, 1.0, 0.0]           1  [0.348, 0.715, 0.509]   \n",
       " 50    410  [0.0, 1.0, 0.0]           1  [0.501, 0.808, 0.395]   \n",
       " 51    410  [0.0, 0.0, 1.0]           2  [0.142, 0.798, 0.737]   \n",
       " 52    410  [0.0, 0.0, 1.0]           2  [0.071, 0.746, 0.863]   \n",
       " 53    410  [0.0, 0.0, 1.0]           2  [0.094, 0.903, 0.835]   \n",
       " 54    410  [0.0, 0.0, 1.0]           2   [0.057, 0.583, 0.85]   \n",
       " 55    410  [0.0, 0.0, 1.0]           2  [0.145, 0.739, 0.736]   \n",
       " 56    410  [0.0, 0.0, 1.0]           2      [0.0, 0.896, 1.0]   \n",
       " 57    410  [0.0, 0.0, 1.0]           2  [0.182, 0.985, 0.724]   \n",
       " 58    410  [0.0, 0.0, 1.0]           2    [0.17, 0.82, 0.722]   \n",
       " 59    410  [0.0, 0.0, 1.0]           2  [0.112, 0.664, 0.769]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     420  [1.0, 0.0, 0.0]           0  [0.933, 0.287, 0.056]   \n",
       " 1     420  [0.0, 0.0, 1.0]           2    [0.12, 0.672, 0.79]   \n",
       " 2     420  [1.0, 0.0, 0.0]           0  [0.938, 0.374, 0.043]   \n",
       " 3     420  [0.0, 0.0, 1.0]           2  [0.211, 0.744, 0.645]   \n",
       " 4     420  [1.0, 0.0, 0.0]           0  [0.923, 0.465, 0.075]   \n",
       " 5     420  [1.0, 0.0, 0.0]           0  [0.976, 0.493, 0.029]   \n",
       " 6     420  [1.0, 0.0, 0.0]           0  [0.951, 0.069, 0.047]   \n",
       " 7     420  [1.0, 0.0, 0.0]           0    [0.936, 0.0, 0.035]   \n",
       " 8     420  [0.0, 0.0, 1.0]           2  [0.102, 0.696, 0.807]   \n",
       " 9     420  [0.0, 0.0, 1.0]           2   [0.167, 0.89, 0.702]   \n",
       " 10    420  [0.0, 0.0, 1.0]           2   [0.075, 0.723, 0.83]   \n",
       " 11    420  [0.0, 0.0, 1.0]           2  [0.074, 0.707, 0.825]   \n",
       " 12    420  [0.0, 0.0, 1.0]           2  [0.099, 0.755, 0.808]   \n",
       " 13    420  [1.0, 0.0, 0.0]           0    [0.91, 0.25, 0.063]   \n",
       " 14    420  [1.0, 0.0, 0.0]           0      [1.0, 0.277, 0.0]   \n",
       " 15    420  [0.0, 0.0, 1.0]           2  [0.138, 0.742, 0.748]   \n",
       " 16    420  [0.0, 0.0, 1.0]           2  [0.196, 0.772, 0.654]   \n",
       " 17    420  [1.0, 0.0, 0.0]           0   [0.876, 0.469, 0.11]   \n",
       " 18    420  [1.0, 0.0, 0.0]           0  [0.919, 0.284, 0.068]   \n",
       " 19    420  [1.0, 0.0, 0.0]           0  [0.862, 0.317, 0.116]   \n",
       " 20    420  [1.0, 0.0, 0.0]           0  [0.985, 0.096, 0.003]   \n",
       " 21    420  [1.0, 0.0, 0.0]           0  [0.929, 0.379, 0.071]   \n",
       " 22    420  [1.0, 0.0, 0.0]           0   [0.916, 0.246, 0.08]   \n",
       " 23    420  [1.0, 0.0, 0.0]           0  [0.939, 0.506, 0.054]   \n",
       " 24    420  [1.0, 0.0, 0.0]           0  [0.916, 0.325, 0.071]   \n",
       " 25    420  [1.0, 0.0, 0.0]           0    [0.931, 0.3, 0.056]   \n",
       " 26    420  [1.0, 0.0, 0.0]           0    [0.85, 0.74, 0.147]   \n",
       " 27    420  [1.0, 0.0, 0.0]           0  [0.854, 0.347, 0.103]   \n",
       " 28    420  [1.0, 0.0, 0.0]           0  [0.931, 0.214, 0.055]   \n",
       " 29    420  [1.0, 0.0, 0.0]           0  [0.921, 0.359, 0.069]   \n",
       " 30    420  [0.0, 1.0, 0.0]           1   [0.31, 0.768, 0.544]   \n",
       " 31    420  [0.0, 1.0, 0.0]           2  [0.254, 0.588, 0.593]   \n",
       " 32    420  [0.0, 1.0, 0.0]           1  [0.493, 0.868, 0.393]   \n",
       " 33    420  [0.0, 1.0, 0.0]           2  [0.257, 0.655, 0.628]   \n",
       " 34    420  [0.0, 1.0, 0.0]           1    [0.423, 1.0, 0.476]   \n",
       " 35    420  [0.0, 1.0, 0.0]           2  [0.268, 0.577, 0.612]   \n",
       " 36    420  [0.0, 1.0, 0.0]           2  [0.302, 0.731, 0.536]   \n",
       " 37    420  [0.0, 1.0, 0.0]           2  [0.232, 0.686, 0.606]   \n",
       " 38    420  [0.0, 1.0, 0.0]           2  [0.292, 0.655, 0.588]   \n",
       " 39    420  [0.0, 1.0, 0.0]           2  [0.269, 0.727, 0.592]   \n",
       " 40    420  [0.0, 1.0, 0.0]           1  [0.435, 0.739, 0.459]   \n",
       " 41    420  [0.0, 1.0, 0.0]           1  [0.382, 0.854, 0.504]   \n",
       " 42    420  [0.0, 1.0, 0.0]           1  [0.364, 0.743, 0.513]   \n",
       " 43    420  [0.0, 1.0, 0.0]           2  [0.195, 0.841, 0.676]   \n",
       " 44    420  [0.0, 1.0, 0.0]           2   [0.297, 0.57, 0.534]   \n",
       " 45    420  [0.0, 1.0, 0.0]           2   [0.23, 0.606, 0.647]   \n",
       " 46    420  [0.0, 1.0, 0.0]           1    [0.33, 0.836, 0.53]   \n",
       " 47    420  [0.0, 1.0, 0.0]           2   [0.279, 0.68, 0.578]   \n",
       " 48    420  [0.0, 1.0, 0.0]           1  [0.333, 0.791, 0.529]   \n",
       " 49    420  [0.0, 1.0, 0.0]           1  [0.346, 0.715, 0.509]   \n",
       " 50    420  [0.0, 1.0, 0.0]           1  [0.498, 0.808, 0.394]   \n",
       " 51    420  [0.0, 0.0, 1.0]           2   [0.14, 0.799, 0.738]   \n",
       " 52    420  [0.0, 0.0, 1.0]           2   [0.07, 0.749, 0.864]   \n",
       " 53    420  [0.0, 0.0, 1.0]           2  [0.092, 0.905, 0.836]   \n",
       " 54    420  [0.0, 0.0, 1.0]           2  [0.056, 0.585, 0.852]   \n",
       " 55    420  [0.0, 0.0, 1.0]           2  [0.143, 0.741, 0.737]   \n",
       " 56    420  [0.0, 0.0, 1.0]           2      [0.0, 0.899, 1.0]   \n",
       " 57    420  [0.0, 0.0, 1.0]           2  [0.179, 0.987, 0.724]   \n",
       " 58    420  [0.0, 0.0, 1.0]           2  [0.168, 0.821, 0.723]   \n",
       " 59    420  [0.0, 0.0, 1.0]           2   [0.11, 0.666, 0.771]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     430  [1.0, 0.0, 0.0]           0  [0.933, 0.287, 0.056]   \n",
       " 1     430  [0.0, 0.0, 1.0]           2  [0.118, 0.675, 0.791]   \n",
       " 2     430  [1.0, 0.0, 0.0]           0  [0.939, 0.373, 0.043]   \n",
       " 3     430  [0.0, 0.0, 1.0]           2  [0.209, 0.744, 0.646]   \n",
       " 4     430  [1.0, 0.0, 0.0]           0  [0.923, 0.465, 0.074]   \n",
       " 5     430  [1.0, 0.0, 0.0]           0  [0.976, 0.492, 0.028]   \n",
       " 6     430  [1.0, 0.0, 0.0]           0  [0.951, 0.069, 0.046]   \n",
       " 7     430  [1.0, 0.0, 0.0]           0    [0.937, 0.0, 0.035]   \n",
       " 8     430  [0.0, 0.0, 1.0]           2  [0.101, 0.697, 0.809]   \n",
       " 9     430  [0.0, 0.0, 1.0]           2  [0.165, 0.891, 0.704]   \n",
       " 10    430  [0.0, 0.0, 1.0]           2  [0.074, 0.724, 0.832]   \n",
       " 11    430  [0.0, 0.0, 1.0]           2  [0.073, 0.708, 0.827]   \n",
       " 12    430  [0.0, 0.0, 1.0]           2   [0.098, 0.756, 0.81]   \n",
       " 13    430  [1.0, 0.0, 0.0]           0   [0.911, 0.25, 0.062]   \n",
       " 14    430  [1.0, 0.0, 0.0]           0      [1.0, 0.275, 0.0]   \n",
       " 15    430  [0.0, 0.0, 1.0]           2   [0.136, 0.743, 0.75]   \n",
       " 16    430  [0.0, 0.0, 1.0]           2  [0.193, 0.772, 0.656]   \n",
       " 17    430  [1.0, 0.0, 0.0]           0  [0.876, 0.469, 0.109]   \n",
       " 18    430  [1.0, 0.0, 0.0]           0  [0.919, 0.284, 0.067]   \n",
       " 19    430  [1.0, 0.0, 0.0]           0  [0.862, 0.317, 0.115]   \n",
       " 20    430  [1.0, 0.0, 0.0]           0  [0.986, 0.095, 0.003]   \n",
       " 21    430  [1.0, 0.0, 0.0]           0   [0.929, 0.379, 0.07]   \n",
       " 22    430  [1.0, 0.0, 0.0]           0  [0.917, 0.246, 0.079]   \n",
       " 23    430  [1.0, 0.0, 0.0]           0  [0.939, 0.505, 0.053]   \n",
       " 24    430  [1.0, 0.0, 0.0]           0   [0.916, 0.324, 0.07]   \n",
       " 25    430  [1.0, 0.0, 0.0]           0  [0.931, 0.299, 0.055]   \n",
       " 26    430  [1.0, 0.0, 0.0]           0   [0.849, 0.74, 0.146]   \n",
       " 27    430  [1.0, 0.0, 0.0]           0  [0.854, 0.347, 0.103]   \n",
       " 28    430  [1.0, 0.0, 0.0]           0  [0.931, 0.214, 0.054]   \n",
       " 29    430  [1.0, 0.0, 0.0]           0  [0.921, 0.359, 0.069]   \n",
       " 30    430  [0.0, 1.0, 0.0]           1  [0.308, 0.768, 0.545]   \n",
       " 31    430  [0.0, 1.0, 0.0]           2  [0.252, 0.589, 0.595]   \n",
       " 32    430  [0.0, 1.0, 0.0]           1   [0.49, 0.868, 0.392]   \n",
       " 33    430  [0.0, 1.0, 0.0]           2  [0.255, 0.657, 0.629]   \n",
       " 34    430  [0.0, 1.0, 0.0]           1     [0.42, 1.0, 0.475]   \n",
       " 35    430  [0.0, 1.0, 0.0]           2  [0.266, 0.578, 0.613]   \n",
       " 36    430  [0.0, 1.0, 0.0]           2    [0.3, 0.731, 0.537]   \n",
       " 37    430  [0.0, 1.0, 0.0]           2   [0.23, 0.687, 0.607]   \n",
       " 38    430  [0.0, 1.0, 0.0]           2   [0.29, 0.657, 0.589]   \n",
       " 39    430  [0.0, 1.0, 0.0]           2  [0.266, 0.727, 0.593]   \n",
       " 40    430  [0.0, 1.0, 0.0]           1   [0.433, 0.74, 0.458]   \n",
       " 41    430  [0.0, 1.0, 0.0]           1  [0.379, 0.854, 0.504]   \n",
       " 42    430  [0.0, 1.0, 0.0]           1  [0.361, 0.744, 0.513]   \n",
       " 43    430  [0.0, 1.0, 0.0]           2  [0.193, 0.842, 0.677]   \n",
       " 44    430  [0.0, 1.0, 0.0]           2   [0.295, 0.57, 0.535]   \n",
       " 45    430  [0.0, 1.0, 0.0]           2  [0.228, 0.608, 0.648]   \n",
       " 46    430  [0.0, 1.0, 0.0]           1  [0.327, 0.836, 0.531]   \n",
       " 47    430  [0.0, 1.0, 0.0]           2  [0.277, 0.681, 0.579]   \n",
       " 48    430  [0.0, 1.0, 0.0]           1  [0.331, 0.792, 0.529]   \n",
       " 49    430  [0.0, 1.0, 0.0]           1   [0.343, 0.716, 0.51]   \n",
       " 50    430  [0.0, 1.0, 0.0]           1  [0.496, 0.808, 0.394]   \n",
       " 51    430  [0.0, 0.0, 1.0]           2    [0.138, 0.8, 0.739]   \n",
       " 52    430  [0.0, 0.0, 1.0]           2  [0.069, 0.752, 0.864]   \n",
       " 53    430  [0.0, 0.0, 1.0]           2  [0.091, 0.907, 0.836]   \n",
       " 54    430  [0.0, 0.0, 1.0]           2  [0.056, 0.586, 0.854]   \n",
       " 55    430  [0.0, 0.0, 1.0]           2  [0.141, 0.742, 0.738]   \n",
       " 56    430  [0.0, 0.0, 1.0]           2      [0.0, 0.902, 1.0]   \n",
       " 57    430  [0.0, 0.0, 1.0]           2  [0.176, 0.988, 0.725]   \n",
       " 58    430  [0.0, 0.0, 1.0]           2  [0.166, 0.822, 0.724]   \n",
       " 59    430  [0.0, 0.0, 1.0]           2  [0.109, 0.667, 0.772]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     440  [1.0, 0.0, 0.0]           0  [0.933, 0.286, 0.055]   \n",
       " 1     440  [0.0, 0.0, 1.0]           2  [0.117, 0.678, 0.792]   \n",
       " 2     440  [1.0, 0.0, 0.0]           0  [0.939, 0.372, 0.043]   \n",
       " 3     440  [0.0, 0.0, 1.0]           2  [0.206, 0.745, 0.648]   \n",
       " 4     440  [1.0, 0.0, 0.0]           0  [0.923, 0.464, 0.073]   \n",
       " 5     440  [1.0, 0.0, 0.0]           0  [0.975, 0.491, 0.028]   \n",
       " 6     440  [1.0, 0.0, 0.0]           0  [0.952, 0.069, 0.046]   \n",
       " 7     440  [1.0, 0.0, 0.0]           0    [0.938, 0.0, 0.035]   \n",
       " 8     440  [0.0, 0.0, 1.0]           2   [0.099, 0.699, 0.81]   \n",
       " 9     440  [0.0, 0.0, 1.0]           2  [0.162, 0.891, 0.705]   \n",
       " 10    440  [0.0, 0.0, 1.0]           2  [0.073, 0.726, 0.834]   \n",
       " 11    440  [0.0, 0.0, 1.0]           2  [0.072, 0.709, 0.829]   \n",
       " 12    440  [0.0, 0.0, 1.0]           2  [0.096, 0.757, 0.811]   \n",
       " 13    440  [1.0, 0.0, 0.0]           0  [0.911, 0.249, 0.062]   \n",
       " 14    440  [1.0, 0.0, 0.0]           0      [1.0, 0.274, 0.0]   \n",
       " 15    440  [0.0, 0.0, 1.0]           2  [0.134, 0.745, 0.751]   \n",
       " 16    440  [0.0, 0.0, 1.0]           2  [0.191, 0.773, 0.657]   \n",
       " 17    440  [1.0, 0.0, 0.0]           0  [0.876, 0.469, 0.108]   \n",
       " 18    440  [1.0, 0.0, 0.0]           0  [0.919, 0.284, 0.066]   \n",
       " 19    440  [1.0, 0.0, 0.0]           0  [0.862, 0.317, 0.114]   \n",
       " 20    440  [1.0, 0.0, 0.0]           0  [0.986, 0.095, 0.003]   \n",
       " 21    440  [1.0, 0.0, 0.0]           0  [0.929, 0.378, 0.069]   \n",
       " 22    440  [1.0, 0.0, 0.0]           0  [0.917, 0.246, 0.078]   \n",
       " 23    440  [1.0, 0.0, 0.0]           0  [0.939, 0.504, 0.052]   \n",
       " 24    440  [1.0, 0.0, 0.0]           0  [0.916, 0.324, 0.069]   \n",
       " 25    440  [1.0, 0.0, 0.0]           0  [0.931, 0.299, 0.055]   \n",
       " 26    440  [1.0, 0.0, 0.0]           0  [0.848, 0.739, 0.145]   \n",
       " 27    440  [1.0, 0.0, 0.0]           0  [0.855, 0.346, 0.102]   \n",
       " 28    440  [1.0, 0.0, 0.0]           0  [0.931, 0.214, 0.054]   \n",
       " 29    440  [1.0, 0.0, 0.0]           0  [0.921, 0.359, 0.068]   \n",
       " 30    440  [0.0, 1.0, 0.0]           1  [0.306, 0.769, 0.545]   \n",
       " 31    440  [0.0, 1.0, 0.0]           2    [0.25, 0.59, 0.596]   \n",
       " 32    440  [0.0, 1.0, 0.0]           1  [0.488, 0.868, 0.392]   \n",
       " 33    440  [0.0, 1.0, 0.0]           2  [0.252, 0.659, 0.629]   \n",
       " 34    440  [0.0, 1.0, 0.0]           1    [0.417, 1.0, 0.474]   \n",
       " 35    440  [0.0, 1.0, 0.0]           2   [0.264, 0.58, 0.613]   \n",
       " 36    440  [0.0, 1.0, 0.0]           2  [0.297, 0.732, 0.538]   \n",
       " 37    440  [0.0, 1.0, 0.0]           2  [0.228, 0.687, 0.609]   \n",
       " 38    440  [0.0, 1.0, 0.0]           2  [0.288, 0.658, 0.589]   \n",
       " 39    440  [0.0, 1.0, 0.0]           2  [0.264, 0.728, 0.594]   \n",
       " 40    440  [0.0, 1.0, 0.0]           1  [0.431, 0.741, 0.458]   \n",
       " 41    440  [0.0, 1.0, 0.0]           1  [0.376, 0.855, 0.504]   \n",
       " 42    440  [0.0, 1.0, 0.0]           1  [0.359, 0.745, 0.513]   \n",
       " 43    440  [0.0, 1.0, 0.0]           2  [0.191, 0.843, 0.678]   \n",
       " 44    440  [0.0, 1.0, 0.0]           2  [0.293, 0.571, 0.537]   \n",
       " 45    440  [0.0, 1.0, 0.0]           2   [0.226, 0.61, 0.649]   \n",
       " 46    440  [0.0, 1.0, 0.0]           1  [0.325, 0.837, 0.531]   \n",
       " 47    440  [0.0, 1.0, 0.0]           2   [0.275, 0.682, 0.58]   \n",
       " 48    440  [0.0, 1.0, 0.0]           1  [0.328, 0.792, 0.529]   \n",
       " 49    440  [0.0, 1.0, 0.0]           1   [0.341, 0.716, 0.51]   \n",
       " 50    440  [0.0, 1.0, 0.0]           1  [0.494, 0.808, 0.394]   \n",
       " 51    440  [0.0, 0.0, 1.0]           2  [0.136, 0.802, 0.741]   \n",
       " 52    440  [0.0, 0.0, 1.0]           2  [0.068, 0.755, 0.865]   \n",
       " 53    440  [0.0, 0.0, 1.0]           2  [0.089, 0.909, 0.837]   \n",
       " 54    440  [0.0, 0.0, 1.0]           2  [0.055, 0.588, 0.856]   \n",
       " 55    440  [0.0, 0.0, 1.0]           2    [0.14, 0.744, 0.74]   \n",
       " 56    440  [0.0, 0.0, 1.0]           2      [0.0, 0.906, 1.0]   \n",
       " 57    440  [0.0, 0.0, 1.0]           2  [0.174, 0.989, 0.725]   \n",
       " 58    440  [0.0, 0.0, 1.0]           2  [0.164, 0.823, 0.725]   \n",
       " 59    440  [0.0, 0.0, 1.0]           2  [0.108, 0.668, 0.774]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     450  [1.0, 0.0, 0.0]           0  [0.934, 0.286, 0.055]   \n",
       " 1     450  [0.0, 0.0, 1.0]           2  [0.115, 0.681, 0.792]   \n",
       " 2     450  [1.0, 0.0, 0.0]           0   [0.939, 0.37, 0.042]   \n",
       " 3     450  [0.0, 0.0, 1.0]           2  [0.204, 0.745, 0.649]   \n",
       " 4     450  [1.0, 0.0, 0.0]           0  [0.923, 0.464, 0.072]   \n",
       " 5     450  [1.0, 0.0, 0.0]           0  [0.975, 0.489, 0.027]   \n",
       " 6     450  [1.0, 0.0, 0.0]           0   [0.952, 0.07, 0.045]   \n",
       " 7     450  [1.0, 0.0, 0.0]           0    [0.939, 0.0, 0.035]   \n",
       " 8     450  [0.0, 0.0, 1.0]           2    [0.098, 0.7, 0.812]   \n",
       " 9     450  [0.0, 0.0, 1.0]           2   [0.16, 0.892, 0.706]   \n",
       " 10    450  [0.0, 0.0, 1.0]           2  [0.071, 0.727, 0.835]   \n",
       " 11    450  [0.0, 0.0, 1.0]           2   [0.071, 0.71, 0.831]   \n",
       " 12    450  [0.0, 0.0, 1.0]           2  [0.095, 0.758, 0.813]   \n",
       " 13    450  [1.0, 0.0, 0.0]           0  [0.911, 0.249, 0.062]   \n",
       " 14    450  [1.0, 0.0, 0.0]           0      [1.0, 0.273, 0.0]   \n",
       " 15    450  [0.0, 0.0, 1.0]           2  [0.132, 0.746, 0.752]   \n",
       " 16    450  [0.0, 0.0, 1.0]           2  [0.189, 0.773, 0.658]   \n",
       " 17    450  [1.0, 0.0, 0.0]           0  [0.876, 0.469, 0.107]   \n",
       " 18    450  [1.0, 0.0, 0.0]           0   [0.92, 0.284, 0.066]   \n",
       " 19    450  [1.0, 0.0, 0.0]           0  [0.863, 0.317, 0.113]   \n",
       " 20    450  [1.0, 0.0, 0.0]           0  [0.987, 0.095, 0.003]   \n",
       " 21    450  [1.0, 0.0, 0.0]           0  [0.929, 0.378, 0.068]   \n",
       " 22    450  [1.0, 0.0, 0.0]           0  [0.917, 0.247, 0.077]   \n",
       " 23    450  [1.0, 0.0, 0.0]           0  [0.939, 0.503, 0.052]   \n",
       " 24    450  [1.0, 0.0, 0.0]           0  [0.916, 0.324, 0.069]   \n",
       " 25    450  [1.0, 0.0, 0.0]           0  [0.932, 0.298, 0.055]   \n",
       " 26    450  [1.0, 0.0, 0.0]           0  [0.848, 0.739, 0.143]   \n",
       " 27    450  [1.0, 0.0, 0.0]           0  [0.855, 0.345, 0.102]   \n",
       " 28    450  [1.0, 0.0, 0.0]           0  [0.932, 0.213, 0.053]   \n",
       " 29    450  [1.0, 0.0, 0.0]           0  [0.922, 0.358, 0.067]   \n",
       " 30    450  [0.0, 1.0, 0.0]           1  [0.303, 0.769, 0.546]   \n",
       " 31    450  [0.0, 1.0, 0.0]           2  [0.248, 0.591, 0.597]   \n",
       " 32    450  [0.0, 1.0, 0.0]           1  [0.486, 0.867, 0.391]   \n",
       " 33    450  [0.0, 1.0, 0.0]           2   [0.25, 0.661, 0.629]   \n",
       " 34    450  [0.0, 1.0, 0.0]           1    [0.414, 1.0, 0.474]   \n",
       " 35    450  [0.0, 1.0, 0.0]           2  [0.262, 0.582, 0.614]   \n",
       " 36    450  [0.0, 1.0, 0.0]           1  [0.295, 0.732, 0.539]   \n",
       " 37    450  [0.0, 1.0, 0.0]           2   [0.226, 0.687, 0.61]   \n",
       " 38    450  [0.0, 1.0, 0.0]           2   [0.286, 0.66, 0.589]   \n",
       " 39    450  [0.0, 1.0, 0.0]           2  [0.262, 0.729, 0.595]   \n",
       " 40    450  [0.0, 1.0, 0.0]           1  [0.428, 0.742, 0.457]   \n",
       " 41    450  [0.0, 1.0, 0.0]           1  [0.374, 0.856, 0.503]   \n",
       " 42    450  [0.0, 1.0, 0.0]           1  [0.357, 0.745, 0.513]   \n",
       " 43    450  [0.0, 1.0, 0.0]           2  [0.188, 0.844, 0.679]   \n",
       " 44    450  [0.0, 1.0, 0.0]           2  [0.291, 0.571, 0.538]   \n",
       " 45    450  [0.0, 1.0, 0.0]           2  [0.224, 0.612, 0.649]   \n",
       " 46    450  [0.0, 1.0, 0.0]           1  [0.323, 0.837, 0.531]   \n",
       " 47    450  [0.0, 1.0, 0.0]           2   [0.273, 0.683, 0.58]   \n",
       " 48    450  [0.0, 1.0, 0.0]           1   [0.326, 0.793, 0.53]   \n",
       " 49    450  [0.0, 1.0, 0.0]           1   [0.339, 0.717, 0.51]   \n",
       " 50    450  [0.0, 1.0, 0.0]           1  [0.492, 0.808, 0.393]   \n",
       " 51    450  [0.0, 0.0, 1.0]           2  [0.135, 0.803, 0.742]   \n",
       " 52    450  [0.0, 0.0, 1.0]           2  [0.067, 0.758, 0.866]   \n",
       " 53    450  [0.0, 0.0, 1.0]           2  [0.088, 0.911, 0.838]   \n",
       " 54    450  [0.0, 0.0, 1.0]           2   [0.054, 0.59, 0.858]   \n",
       " 55    450  [0.0, 0.0, 1.0]           2  [0.138, 0.745, 0.741]   \n",
       " 56    450  [0.0, 0.0, 1.0]           2      [0.0, 0.909, 1.0]   \n",
       " 57    450  [0.0, 0.0, 1.0]           2   [0.172, 0.99, 0.726]   \n",
       " 58    450  [0.0, 0.0, 1.0]           2  [0.162, 0.825, 0.726]   \n",
       " 59    450  [0.0, 0.0, 1.0]           2   [0.106, 0.67, 0.776]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     460  [1.0, 0.0, 0.0]           0  [0.934, 0.286, 0.054]   \n",
       " 1     460  [0.0, 0.0, 1.0]           2  [0.114, 0.684, 0.793]   \n",
       " 2     460  [1.0, 0.0, 0.0]           0  [0.939, 0.369, 0.042]   \n",
       " 3     460  [0.0, 0.0, 1.0]           2   [0.202, 0.746, 0.65]   \n",
       " 4     460  [1.0, 0.0, 0.0]           0  [0.922, 0.464, 0.071]   \n",
       " 5     460  [1.0, 0.0, 0.0]           0  [0.975, 0.488, 0.027]   \n",
       " 6     460  [1.0, 0.0, 0.0]           0   [0.953, 0.07, 0.045]   \n",
       " 7     460  [1.0, 0.0, 0.0]           0     [0.94, 0.0, 0.035]   \n",
       " 8     460  [0.0, 0.0, 1.0]           2  [0.096, 0.703, 0.813]   \n",
       " 9     460  [0.0, 0.0, 1.0]           2  [0.158, 0.892, 0.707]   \n",
       " 10    460  [0.0, 0.0, 1.0]           2   [0.07, 0.729, 0.837]   \n",
       " 11    460  [0.0, 0.0, 1.0]           2   [0.07, 0.712, 0.833]   \n",
       " 12    460  [0.0, 0.0, 1.0]           2   [0.094, 0.76, 0.814]   \n",
       " 13    460  [1.0, 0.0, 0.0]           0  [0.912, 0.248, 0.061]   \n",
       " 14    460  [1.0, 0.0, 0.0]           0      [1.0, 0.272, 0.0]   \n",
       " 15    460  [0.0, 0.0, 1.0]           2  [0.131, 0.748, 0.754]   \n",
       " 16    460  [0.0, 0.0, 1.0]           2   [0.187, 0.774, 0.66]   \n",
       " 17    460  [1.0, 0.0, 0.0]           0  [0.876, 0.469, 0.106]   \n",
       " 18    460  [1.0, 0.0, 0.0]           0   [0.92, 0.283, 0.065]   \n",
       " 19    460  [1.0, 0.0, 0.0]           0  [0.863, 0.317, 0.112]   \n",
       " 20    460  [1.0, 0.0, 0.0]           0  [0.987, 0.094, 0.003]   \n",
       " 21    460  [1.0, 0.0, 0.0]           0  [0.929, 0.378, 0.067]   \n",
       " 22    460  [1.0, 0.0, 0.0]           0  [0.918, 0.247, 0.077]   \n",
       " 23    460  [1.0, 0.0, 0.0]           0  [0.939, 0.502, 0.051]   \n",
       " 24    460  [1.0, 0.0, 0.0]           0  [0.917, 0.324, 0.068]   \n",
       " 25    460  [1.0, 0.0, 0.0]           0  [0.932, 0.298, 0.054]   \n",
       " 26    460  [1.0, 0.0, 0.0]           0  [0.847, 0.739, 0.142]   \n",
       " 27    460  [1.0, 0.0, 0.0]           0  [0.855, 0.344, 0.101]   \n",
       " 28    460  [1.0, 0.0, 0.0]           0  [0.932, 0.213, 0.053]   \n",
       " 29    460  [1.0, 0.0, 0.0]           0  [0.922, 0.358, 0.067]   \n",
       " 30    460  [0.0, 1.0, 0.0]           1   [0.301, 0.77, 0.546]   \n",
       " 31    460  [0.0, 1.0, 0.0]           2  [0.247, 0.592, 0.597]   \n",
       " 32    460  [0.0, 1.0, 0.0]           1   [0.484, 0.867, 0.39]   \n",
       " 33    460  [0.0, 1.0, 0.0]           2  [0.248, 0.663, 0.629]   \n",
       " 34    460  [0.0, 1.0, 0.0]           1    [0.411, 1.0, 0.473]   \n",
       " 35    460  [0.0, 1.0, 0.0]           2   [0.26, 0.584, 0.614]   \n",
       " 36    460  [0.0, 1.0, 0.0]           1   [0.293, 0.732, 0.54]   \n",
       " 37    460  [0.0, 1.0, 0.0]           2  [0.223, 0.688, 0.611]   \n",
       " 38    460  [0.0, 1.0, 0.0]           2  [0.283, 0.662, 0.589]   \n",
       " 39    460  [0.0, 1.0, 0.0]           2    [0.26, 0.73, 0.595]   \n",
       " 40    460  [0.0, 1.0, 0.0]           1  [0.426, 0.743, 0.456]   \n",
       " 41    460  [0.0, 1.0, 0.0]           1  [0.371, 0.856, 0.503]   \n",
       " 42    460  [0.0, 1.0, 0.0]           1  [0.354, 0.747, 0.513]   \n",
       " 43    460  [0.0, 1.0, 0.0]           2   [0.186, 0.845, 0.68]   \n",
       " 44    460  [0.0, 1.0, 0.0]           2  [0.289, 0.572, 0.538]   \n",
       " 45    460  [0.0, 1.0, 0.0]           2   [0.222, 0.614, 0.65]   \n",
       " 46    460  [0.0, 1.0, 0.0]           1   [0.32, 0.838, 0.531]   \n",
       " 47    460  [0.0, 1.0, 0.0]           2  [0.271, 0.684, 0.581]   \n",
       " 48    460  [0.0, 1.0, 0.0]           1   [0.323, 0.793, 0.53]   \n",
       " 49    460  [0.0, 1.0, 0.0]           1   [0.337, 0.718, 0.51]   \n",
       " 50    460  [0.0, 1.0, 0.0]           1   [0.49, 0.808, 0.393]   \n",
       " 51    460  [0.0, 0.0, 1.0]           2  [0.133, 0.804, 0.743]   \n",
       " 52    460  [0.0, 0.0, 1.0]           2  [0.066, 0.761, 0.866]   \n",
       " 53    460  [0.0, 0.0, 1.0]           2  [0.086, 0.914, 0.838]   \n",
       " 54    460  [0.0, 0.0, 1.0]           2  [0.054, 0.592, 0.859]   \n",
       " 55    460  [0.0, 0.0, 1.0]           2  [0.136, 0.747, 0.742]   \n",
       " 56    460  [0.0, 0.0, 1.0]           2      [0.0, 0.913, 1.0]   \n",
       " 57    460  [0.0, 0.0, 1.0]           2  [0.169, 0.991, 0.726]   \n",
       " 58    460  [0.0, 0.0, 1.0]           2   [0.16, 0.826, 0.727]   \n",
       " 59    460  [0.0, 0.0, 1.0]           2  [0.105, 0.671, 0.777]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     470  [1.0, 0.0, 0.0]           0  [0.934, 0.285, 0.054]   \n",
       " 1     470  [0.0, 0.0, 1.0]           2  [0.112, 0.687, 0.794]   \n",
       " 2     470  [1.0, 0.0, 0.0]           0  [0.939, 0.369, 0.042]   \n",
       " 3     470  [0.0, 0.0, 1.0]           2    [0.2, 0.747, 0.651]   \n",
       " 4     470  [1.0, 0.0, 0.0]           0   [0.922, 0.463, 0.07]   \n",
       " 5     470  [1.0, 0.0, 0.0]           0  [0.974, 0.487, 0.026]   \n",
       " 6     470  [1.0, 0.0, 0.0]           0   [0.953, 0.07, 0.044]   \n",
       " 7     470  [1.0, 0.0, 0.0]           0     [0.94, 0.0, 0.035]   \n",
       " 8     470  [0.0, 0.0, 1.0]           2  [0.095, 0.704, 0.814]   \n",
       " 9     470  [0.0, 0.0, 1.0]           2  [0.156, 0.893, 0.708]   \n",
       " 10    470  [0.0, 0.0, 1.0]           2  [0.069, 0.731, 0.838]   \n",
       " 11    470  [0.0, 0.0, 1.0]           2  [0.069, 0.713, 0.834]   \n",
       " 12    470  [0.0, 0.0, 1.0]           2  [0.092, 0.762, 0.815]   \n",
       " 13    470  [1.0, 0.0, 0.0]           0  [0.912, 0.248, 0.061]   \n",
       " 14    470  [1.0, 0.0, 0.0]           0      [1.0, 0.271, 0.0]   \n",
       " 15    470  [0.0, 0.0, 1.0]           2  [0.129, 0.749, 0.755]   \n",
       " 16    470  [0.0, 0.0, 1.0]           2  [0.185, 0.775, 0.661]   \n",
       " 17    470  [1.0, 0.0, 0.0]           0  [0.876, 0.469, 0.105]   \n",
       " 18    470  [1.0, 0.0, 0.0]           0   [0.92, 0.283, 0.065]   \n",
       " 19    470  [1.0, 0.0, 0.0]           0  [0.863, 0.317, 0.112]   \n",
       " 20    470  [1.0, 0.0, 0.0]           0  [0.988, 0.094, 0.003]   \n",
       " 21    470  [1.0, 0.0, 0.0]           0  [0.929, 0.378, 0.067]   \n",
       " 22    470  [1.0, 0.0, 0.0]           0  [0.918, 0.247, 0.076]   \n",
       " 23    470  [1.0, 0.0, 0.0]           0  [0.938, 0.501, 0.051]   \n",
       " 24    470  [1.0, 0.0, 0.0]           0  [0.917, 0.323, 0.067]   \n",
       " 25    470  [1.0, 0.0, 0.0]           0  [0.932, 0.297, 0.054]   \n",
       " 26    470  [1.0, 0.0, 0.0]           0   [0.846, 0.738, 0.14]   \n",
       " 27    470  [1.0, 0.0, 0.0]           0  [0.855, 0.344, 0.101]   \n",
       " 28    470  [1.0, 0.0, 0.0]           0  [0.932, 0.213, 0.052]   \n",
       " 29    470  [1.0, 0.0, 0.0]           0  [0.922, 0.358, 0.066]   \n",
       " 30    470  [0.0, 1.0, 0.0]           1  [0.299, 0.771, 0.546]   \n",
       " 31    470  [0.0, 1.0, 0.0]           2  [0.245, 0.593, 0.598]   \n",
       " 32    470  [0.0, 1.0, 0.0]           1   [0.482, 0.867, 0.39]   \n",
       " 33    470  [0.0, 1.0, 0.0]           2   [0.246, 0.665, 0.63]   \n",
       " 34    470  [0.0, 1.0, 0.0]           1    [0.408, 1.0, 0.472]   \n",
       " 35    470  [0.0, 1.0, 0.0]           2  [0.258, 0.586, 0.614]   \n",
       " 36    470  [0.0, 1.0, 0.0]           1   [0.291, 0.733, 0.54]   \n",
       " 37    470  [0.0, 1.0, 0.0]           2  [0.222, 0.689, 0.612]   \n",
       " 38    470  [0.0, 1.0, 0.0]           2  [0.281, 0.664, 0.589]   \n",
       " 39    470  [0.0, 1.0, 0.0]           2  [0.257, 0.731, 0.596]   \n",
       " 40    470  [0.0, 1.0, 0.0]           1  [0.424, 0.744, 0.456]   \n",
       " 41    470  [0.0, 1.0, 0.0]           1  [0.369, 0.857, 0.502]   \n",
       " 42    470  [0.0, 1.0, 0.0]           1  [0.352, 0.748, 0.513]   \n",
       " 43    470  [0.0, 1.0, 0.0]           2   [0.184, 0.846, 0.68]   \n",
       " 44    470  [0.0, 1.0, 0.0]           2  [0.287, 0.573, 0.539]   \n",
       " 45    470  [0.0, 1.0, 0.0]           2    [0.22, 0.616, 0.65]   \n",
       " 46    470  [0.0, 1.0, 0.0]           1  [0.318, 0.839, 0.531]   \n",
       " 47    470  [0.0, 1.0, 0.0]           2  [0.268, 0.685, 0.581]   \n",
       " 48    470  [0.0, 1.0, 0.0]           1   [0.321, 0.794, 0.53]   \n",
       " 49    470  [0.0, 1.0, 0.0]           1  [0.335, 0.718, 0.511]   \n",
       " 50    470  [0.0, 1.0, 0.0]           1  [0.488, 0.808, 0.392]   \n",
       " 51    470  [0.0, 0.0, 1.0]           2  [0.131, 0.806, 0.744]   \n",
       " 52    470  [0.0, 0.0, 1.0]           2  [0.065, 0.764, 0.867]   \n",
       " 53    470  [0.0, 0.0, 1.0]           2  [0.085, 0.916, 0.839]   \n",
       " 54    470  [0.0, 0.0, 1.0]           2  [0.053, 0.594, 0.861]   \n",
       " 55    470  [0.0, 0.0, 1.0]           2  [0.135, 0.748, 0.743]   \n",
       " 56    470  [0.0, 0.0, 1.0]           2      [0.0, 0.916, 1.0]   \n",
       " 57    470  [0.0, 0.0, 1.0]           2  [0.167, 0.992, 0.726]   \n",
       " 58    470  [0.0, 0.0, 1.0]           2  [0.158, 0.828, 0.727]   \n",
       " 59    470  [0.0, 0.0, 1.0]           2  [0.104, 0.673, 0.779]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     480  [1.0, 0.0, 0.0]           0  [0.934, 0.285, 0.053]   \n",
       " 1     480  [0.0, 0.0, 1.0]           2  [0.111, 0.689, 0.794]   \n",
       " 2     480  [1.0, 0.0, 0.0]           0  [0.939, 0.368, 0.041]   \n",
       " 3     480  [0.0, 0.0, 1.0]           2  [0.198, 0.748, 0.652]   \n",
       " 4     480  [1.0, 0.0, 0.0]           0  [0.922, 0.463, 0.069]   \n",
       " 5     480  [1.0, 0.0, 0.0]           0  [0.974, 0.486, 0.026]   \n",
       " 6     480  [1.0, 0.0, 0.0]           0  [0.954, 0.071, 0.044]   \n",
       " 7     480  [1.0, 0.0, 0.0]           0    [0.941, 0.0, 0.035]   \n",
       " 8     480  [0.0, 0.0, 1.0]           2  [0.094, 0.706, 0.816]   \n",
       " 9     480  [0.0, 0.0, 1.0]           2   [0.154, 0.893, 0.71]   \n",
       " 10    480  [0.0, 0.0, 1.0]           2  [0.068, 0.732, 0.839]   \n",
       " 11    480  [0.0, 0.0, 1.0]           2  [0.068, 0.714, 0.836]   \n",
       " 12    480  [0.0, 0.0, 1.0]           2  [0.091, 0.763, 0.816]   \n",
       " 13    480  [1.0, 0.0, 0.0]           0   [0.913, 0.247, 0.06]   \n",
       " 14    480  [1.0, 0.0, 0.0]           0       [1.0, 0.27, 0.0]   \n",
       " 15    480  [0.0, 0.0, 1.0]           2   [0.127, 0.75, 0.756]   \n",
       " 16    480  [0.0, 0.0, 1.0]           2  [0.184, 0.775, 0.662]   \n",
       " 17    480  [1.0, 0.0, 0.0]           0  [0.876, 0.469, 0.104]   \n",
       " 18    480  [1.0, 0.0, 0.0]           0   [0.92, 0.283, 0.064]   \n",
       " 19    480  [1.0, 0.0, 0.0]           0  [0.863, 0.318, 0.111]   \n",
       " 20    480  [1.0, 0.0, 0.0]           0  [0.988, 0.094, 0.003]   \n",
       " 21    480  [1.0, 0.0, 0.0]           0  [0.929, 0.377, 0.066]   \n",
       " 22    480  [1.0, 0.0, 0.0]           0  [0.918, 0.247, 0.075]   \n",
       " 23    480  [1.0, 0.0, 0.0]           0   [0.938, 0.501, 0.05]   \n",
       " 24    480  [1.0, 0.0, 0.0]           0  [0.917, 0.323, 0.067]   \n",
       " 25    480  [1.0, 0.0, 0.0]           0  [0.932, 0.297, 0.053]   \n",
       " 26    480  [1.0, 0.0, 0.0]           0  [0.846, 0.738, 0.139]   \n",
       " 27    480  [1.0, 0.0, 0.0]           0    [0.856, 0.343, 0.1]   \n",
       " 28    480  [1.0, 0.0, 0.0]           0  [0.933, 0.213, 0.052]   \n",
       " 29    480  [1.0, 0.0, 0.0]           0  [0.922, 0.357, 0.065]   \n",
       " 30    480  [0.0, 1.0, 0.0]           1  [0.297, 0.772, 0.546]   \n",
       " 31    480  [0.0, 1.0, 0.0]           2  [0.243, 0.594, 0.599]   \n",
       " 32    480  [0.0, 1.0, 0.0]           1   [0.48, 0.867, 0.389]   \n",
       " 33    480  [0.0, 1.0, 0.0]           2   [0.244, 0.667, 0.63]   \n",
       " 34    480  [0.0, 1.0, 0.0]           1    [0.406, 1.0, 0.472]   \n",
       " 35    480  [0.0, 1.0, 0.0]           2  [0.256, 0.588, 0.614]   \n",
       " 36    480  [0.0, 1.0, 0.0]           1  [0.289, 0.733, 0.541]   \n",
       " 37    480  [0.0, 1.0, 0.0]           2   [0.22, 0.689, 0.614]   \n",
       " 38    480  [0.0, 1.0, 0.0]           2   [0.279, 0.665, 0.59]   \n",
       " 39    480  [0.0, 1.0, 0.0]           2  [0.255, 0.732, 0.596]   \n",
       " 40    480  [0.0, 1.0, 0.0]           1  [0.422, 0.745, 0.455]   \n",
       " 41    480  [0.0, 1.0, 0.0]           1  [0.367, 0.858, 0.502]   \n",
       " 42    480  [0.0, 1.0, 0.0]           1   [0.35, 0.748, 0.513]   \n",
       " 43    480  [0.0, 1.0, 0.0]           2  [0.182, 0.847, 0.681]   \n",
       " 44    480  [0.0, 1.0, 0.0]           2   [0.286, 0.573, 0.54]   \n",
       " 45    480  [0.0, 1.0, 0.0]           2  [0.218, 0.617, 0.651]   \n",
       " 46    480  [0.0, 1.0, 0.0]           1  [0.316, 0.839, 0.531]   \n",
       " 47    480  [0.0, 1.0, 0.0]           2  [0.266, 0.686, 0.582]   \n",
       " 48    480  [0.0, 1.0, 0.0]           1   [0.319, 0.795, 0.53]   \n",
       " 49    480  [0.0, 1.0, 0.0]           1  [0.333, 0.719, 0.511]   \n",
       " 50    480  [0.0, 1.0, 0.0]           1  [0.486, 0.808, 0.392]   \n",
       " 51    480  [0.0, 0.0, 1.0]           2   [0.13, 0.807, 0.745]   \n",
       " 52    480  [0.0, 0.0, 1.0]           2  [0.064, 0.766, 0.868]   \n",
       " 53    480  [0.0, 0.0, 1.0]           2   [0.083, 0.917, 0.84]   \n",
       " 54    480  [0.0, 0.0, 1.0]           2  [0.052, 0.595, 0.862]   \n",
       " 55    480  [0.0, 0.0, 1.0]           2   [0.133, 0.75, 0.744]   \n",
       " 56    480  [0.0, 0.0, 1.0]           2      [0.0, 0.918, 1.0]   \n",
       " 57    480  [0.0, 0.0, 1.0]           2  [0.165, 0.993, 0.727]   \n",
       " 58    480  [0.0, 0.0, 1.0]           2  [0.156, 0.829, 0.728]   \n",
       " 59    480  [0.0, 0.0, 1.0]           2   [0.102, 0.674, 0.78]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ,\n",
       "     epoch           actual  prediction       confidence_score  \\\n",
       " 0     490  [1.0, 0.0, 0.0]           0  [0.935, 0.285, 0.053]   \n",
       " 1     490  [0.0, 0.0, 1.0]           2   [0.11, 0.692, 0.795]   \n",
       " 2     490  [1.0, 0.0, 0.0]           0  [0.939, 0.367, 0.041]   \n",
       " 3     490  [0.0, 0.0, 1.0]           2  [0.196, 0.748, 0.653]   \n",
       " 4     490  [1.0, 0.0, 0.0]           0  [0.922, 0.463, 0.069]   \n",
       " 5     490  [1.0, 0.0, 0.0]           0  [0.974, 0.486, 0.025]   \n",
       " 6     490  [1.0, 0.0, 0.0]           0  [0.954, 0.071, 0.043]   \n",
       " 7     490  [1.0, 0.0, 0.0]           0    [0.942, 0.0, 0.035]   \n",
       " 8     490  [0.0, 0.0, 1.0]           2  [0.093, 0.707, 0.817]   \n",
       " 9     490  [0.0, 0.0, 1.0]           2  [0.152, 0.893, 0.711]   \n",
       " 10    490  [0.0, 0.0, 1.0]           2  [0.067, 0.733, 0.841]   \n",
       " 11    490  [0.0, 0.0, 1.0]           2  [0.067, 0.715, 0.837]   \n",
       " 12    490  [0.0, 0.0, 1.0]           2   [0.09, 0.764, 0.818]   \n",
       " 13    490  [1.0, 0.0, 0.0]           0   [0.913, 0.247, 0.06]   \n",
       " 14    490  [1.0, 0.0, 0.0]           0      [1.0, 0.269, 0.0]   \n",
       " 15    490  [0.0, 0.0, 1.0]           2  [0.126, 0.752, 0.757]   \n",
       " 16    490  [0.0, 0.0, 1.0]           2  [0.182, 0.776, 0.663]   \n",
       " 17    490  [1.0, 0.0, 0.0]           0  [0.876, 0.469, 0.103]   \n",
       " 18    490  [1.0, 0.0, 0.0]           0  [0.921, 0.283, 0.063]   \n",
       " 19    490  [1.0, 0.0, 0.0]           0   [0.863, 0.318, 0.11]   \n",
       " 20    490  [1.0, 0.0, 0.0]           0  [0.989, 0.093, 0.003]   \n",
       " 21    490  [1.0, 0.0, 0.0]           0  [0.929, 0.377, 0.065]   \n",
       " 22    490  [1.0, 0.0, 0.0]           0  [0.918, 0.248, 0.074]   \n",
       " 23    490  [1.0, 0.0, 0.0]           0    [0.938, 0.5, 0.049]   \n",
       " 24    490  [1.0, 0.0, 0.0]           0  [0.917, 0.323, 0.066]   \n",
       " 25    490  [1.0, 0.0, 0.0]           0  [0.933, 0.296, 0.053]   \n",
       " 26    490  [1.0, 0.0, 0.0]           0  [0.845, 0.738, 0.138]   \n",
       " 27    490  [1.0, 0.0, 0.0]           0    [0.856, 0.343, 0.1]   \n",
       " 28    490  [1.0, 0.0, 0.0]           0  [0.933, 0.213, 0.051]   \n",
       " 29    490  [1.0, 0.0, 0.0]           0  [0.922, 0.357, 0.065]   \n",
       " 30    490  [0.0, 1.0, 0.0]           1  [0.295, 0.772, 0.547]   \n",
       " 31    490  [0.0, 1.0, 0.0]           2    [0.241, 0.595, 0.6]   \n",
       " 32    490  [0.0, 1.0, 0.0]           1  [0.478, 0.867, 0.389]   \n",
       " 33    490  [0.0, 1.0, 0.0]           2   [0.242, 0.669, 0.63]   \n",
       " 34    490  [0.0, 1.0, 0.0]           1    [0.403, 1.0, 0.471]   \n",
       " 35    490  [0.0, 1.0, 0.0]           2   [0.254, 0.59, 0.615]   \n",
       " 36    490  [0.0, 1.0, 0.0]           1  [0.287, 0.733, 0.541]   \n",
       " 37    490  [0.0, 1.0, 0.0]           2  [0.218, 0.689, 0.615]   \n",
       " 38    490  [0.0, 1.0, 0.0]           2   [0.277, 0.667, 0.59]   \n",
       " 39    490  [0.0, 1.0, 0.0]           2  [0.253, 0.733, 0.597]   \n",
       " 40    490  [0.0, 1.0, 0.0]           1   [0.42, 0.746, 0.455]   \n",
       " 41    490  [0.0, 1.0, 0.0]           1  [0.364, 0.858, 0.502]   \n",
       " 42    490  [0.0, 1.0, 0.0]           1  [0.348, 0.749, 0.513]   \n",
       " 43    490  [0.0, 1.0, 0.0]           2   [0.18, 0.848, 0.682]   \n",
       " 44    490  [0.0, 1.0, 0.0]           2  [0.284, 0.574, 0.541]   \n",
       " 45    490  [0.0, 1.0, 0.0]           2  [0.217, 0.619, 0.652]   \n",
       " 46    490  [0.0, 1.0, 0.0]           1   [0.313, 0.84, 0.531]   \n",
       " 47    490  [0.0, 1.0, 0.0]           2  [0.265, 0.688, 0.582]   \n",
       " 48    490  [0.0, 1.0, 0.0]           1   [0.317, 0.795, 0.53]   \n",
       " 49    490  [0.0, 1.0, 0.0]           1  [0.331, 0.719, 0.511]   \n",
       " 50    490  [0.0, 1.0, 0.0]           1  [0.484, 0.808, 0.391]   \n",
       " 51    490  [0.0, 0.0, 1.0]           2  [0.128, 0.808, 0.746]   \n",
       " 52    490  [0.0, 0.0, 1.0]           2  [0.063, 0.769, 0.868]   \n",
       " 53    490  [0.0, 0.0, 1.0]           2   [0.082, 0.919, 0.84]   \n",
       " 54    490  [0.0, 0.0, 1.0]           2  [0.052, 0.597, 0.864]   \n",
       " 55    490  [0.0, 0.0, 1.0]           2  [0.131, 0.751, 0.745]   \n",
       " 56    490  [0.0, 0.0, 1.0]           2      [0.0, 0.921, 1.0]   \n",
       " 57    490  [0.0, 0.0, 1.0]           2  [0.163, 0.994, 0.727]   \n",
       " 58    490  [0.0, 0.0, 1.0]           2   [0.154, 0.83, 0.729]   \n",
       " 59    490  [0.0, 0.0, 1.0]           2  [0.101, 0.675, 0.781]   \n",
       " \n",
       "                                                 input  \n",
       " 0         [0.23529406, 0.625, 0.0677966, 0.041666664]  \n",
       " 1         [0.85294116, 0.41666666, 0.81355935, 0.625]  \n",
       " 2     [0.08823522, 0.5833334, 0.0677966, 0.083333336]  \n",
       " 3            [0.5, 0.41666666, 0.6440678, 0.70833325]  \n",
       " 4            [0.14705884, 0.41666666, 0.0677966, 0.0]  \n",
       " 5                 [0.0, 0.41666666, 0.016949156, 0.0]  \n",
       " 6   [0.44117653, 0.8333333, 0.033898313, 0.041666664]  \n",
       " 7               [0.41176465, 1.0, 0.084745765, 0.125]  \n",
       " 8     [0.76470596, 0.45833328, 0.69491524, 0.9166666]  \n",
       " 9     [0.44117653, 0.2916667, 0.69491524, 0.74999994]  \n",
       " 10             [0.7352942, 0.5, 0.8305085, 0.9166666]  \n",
       " 11             [0.7058823, 0.5416666, 0.7966101, 1.0]  \n",
       " 12     [0.7058823, 0.41666666, 0.71186435, 0.9166666]  \n",
       " 13        [0.23529406, 0.7083333, 0.084745765, 0.125]  \n",
       " 14          [0.08823522, 0.6666666, 0.0, 0.041666664]  \n",
       " 15    [0.64705884, 0.41666666, 0.71186435, 0.7916666]  \n",
       " 16   [0.47058827, 0.41666666, 0.69491524, 0.70833325]  \n",
       " 17  [0.20588233, 0.41666666, 0.10169492, 0.041666664]  \n",
       " 18       [0.2647058, 0.625, 0.084745765, 0.041666664]  \n",
       " 19        [0.32352942, 0.5833334, 0.084745765, 0.125]  \n",
       " 20          [0.2647058, 0.87499994, 0.084745765, 0.0]  \n",
       " 21        [0.20588233, 0.5, 0.033898313, 0.041666664]  \n",
       " 22       [0.35294116, 0.625, 0.05084745, 0.041666664]  \n",
       " 23  [0.02941174, 0.41666666, 0.05084745, 0.041666664]  \n",
       " 24  [0.23529406, 0.5833334, 0.084745765, 0.041666664]  \n",
       " 25       [0.20588233, 0.625, 0.05084745, 0.083333336]  \n",
       " 26  [0.05882348, 0.12499998, 0.05084745, 0.083333336]  \n",
       " 27        [0.20588233, 0.625, 0.10169492, 0.20833333]  \n",
       " 28   [0.2941177, 0.7083333, 0.084745765, 0.041666664]  \n",
       " 29    [0.20588233, 0.5416666, 0.0677966, 0.041666664]  \n",
       " 30    [0.41176465, 0.3333333, 0.59322035, 0.49999994]  \n",
       " 31           [0.5882354, 0.5416666, 0.6271186, 0.625]  \n",
       " 32     [0.17647058, 0.1666667, 0.3898305, 0.37499997]  \n",
       " 33     [0.6764706, 0.37500003, 0.6101695, 0.49999994]  \n",
       " 34           [0.20588233, 0.0, 0.4237288, 0.37499997]  \n",
       " 35      [0.7058823, 0.45833328, 0.5762712, 0.5416666]  \n",
       " 36     [0.3823529, 0.41666666, 0.59322035, 0.5833333]  \n",
       " 37           [0.47058827, 0.5, 0.6440678, 0.70833325]  \n",
       " 38    [0.6176471, 0.37500003, 0.55932206, 0.49999994]  \n",
       " 39           [0.5, 0.37500003, 0.59322035, 0.5833333]  \n",
       " 40    [0.41176465, 0.24999996, 0.4237288, 0.37499997]  \n",
       " 41    [0.35294116, 0.1666667, 0.47457626, 0.41666666]  \n",
       " 42     [0.44117653, 0.2916667, 0.49152544, 0.4583333]  \n",
       " 43                [0.5, 0.2916667, 0.69491524, 0.625]  \n",
       " 44                [0.5, 0.5833334, 0.59322035, 0.625]  \n",
       " 45      [0.7058823, 0.45833328, 0.6271186, 0.5833333]  \n",
       " 46     [0.35294116, 0.24999996, 0.5762712, 0.4583333]  \n",
       " 47     [0.52941173, 0.41666666, 0.6101695, 0.5416666]  \n",
       " 48      [0.3823529, 0.2916667, 0.5423728, 0.49999994]  \n",
       " 49    [0.41176465, 0.37500003, 0.5423728, 0.49999994]  \n",
       " 50   [0.23529406, 0.20833333, 0.33898306, 0.41666666]  \n",
       " 51      [0.5882354, 0.37500003, 0.779661, 0.70833325]  \n",
       " 52    [0.88235307, 0.37500003, 0.8983051, 0.70833325]  \n",
       " 53    [0.7058823, 0.20833333, 0.81355935, 0.70833325]  \n",
       " 54           [0.85294116, 0.6666666, 0.86440676, 1.0]  \n",
       " 55    [0.64705884, 0.41666666, 0.7627118, 0.70833325]  \n",
       " 56                  [1.0, 0.24999996, 1.0, 0.9166666]  \n",
       " 57            [0.5, 0.08333335, 0.6779661, 0.5833333]  \n",
       " 58     [0.5882354, 0.2916667, 0.66101694, 0.70833325]  \n",
       " 59      [0.7058823, 0.5416666, 0.7966101, 0.83333325]  ]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_output = extractor.get_testing_results()\n",
    "epoch_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-constitutional",
   "metadata": {},
   "source": [
    "Let's make a plot of the accuracy and loss per epoch from the model training. What trends do you see here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "continental-anchor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-feature",
   "metadata": {
    "id": "9EEGuDVuzb5r"
   },
   "source": [
    "## Evaluate the model\n",
    "\n",
    "Let's see how the model performs on brand new, unseen before data. Two values will be returned: loss (a number which represents our error, lower values are better), and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "incredible-maine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7555258  0.3721129  0.18546636]\n",
      " [0.13026562 0.5218655  0.70985407]\n",
      " [0.7590202  0.40183365 0.17739274]\n",
      " [0.1955777  0.54194653 0.61008203]\n",
      " [0.7459127  0.43727344 0.19650014]\n",
      " [0.78505176 0.44534478 0.16622378]\n",
      " [0.7705698  0.29390696 0.17891838]\n",
      " [0.76150954 0.2678019  0.1734559 ]\n",
      " [0.11743628 0.52710533 0.725692  ]\n",
      " [0.16238192 0.59500486 0.65082055]\n",
      " [0.09865963 0.5367218  0.7426784 ]\n",
      " [0.09836984 0.52981496 0.74047595]\n",
      " [0.1152427  0.5479149  0.72632676]\n",
      " [0.7392328  0.35793367 0.1908288 ]\n",
      " [0.80483687 0.3659477  0.14863752]\n",
      " [0.14249977 0.54330987 0.683565  ]\n",
      " [0.18446723 0.5520516  0.61721444]\n",
      " [0.7107148  0.43945798 0.2207055 ]\n",
      " [0.7449283  0.3714721  0.19294356]\n",
      " [0.7016601  0.38414612 0.22568567]\n",
      " [0.79641134 0.3018505  0.15063569]\n",
      " [0.7508959  0.4059311  0.19411145]\n",
      " [0.7432665  0.35854477 0.20048024]\n",
      " [0.75798243 0.45056915 0.18318522]\n",
      " [0.7423515  0.38604972 0.19490102]\n",
      " [0.7539728  0.37611783 0.1855442 ]\n",
      " [0.68721306 0.53784645 0.24495794]\n",
      " [0.6960352  0.39306453 0.21886042]\n",
      " [0.75457203 0.34579575 0.18466143]\n",
      " [0.745907   0.39855292 0.19377604]\n",
      " [0.26982644 0.5508292  0.53450334]\n",
      " [0.22943172 0.48602214 0.57245123]\n",
      " [0.40831223 0.5850825  0.42260015]\n",
      " [0.22985412 0.51338327 0.59332484]\n",
      " [0.35177514 0.63396436 0.4806572 ]\n",
      " [0.23897631 0.48431075 0.58260816]\n",
      " [0.26400372 0.5363319  0.53110856]\n",
      " [0.21181434 0.5202447  0.5830961 ]\n",
      " [0.25657958 0.51237833 0.56476575]\n",
      " [0.23855326 0.53650415 0.57011265]\n",
      " [0.3648179  0.54122    0.46916258]\n",
      " [0.3223329  0.582261   0.50248075]\n",
      " [0.3100977  0.5424117  0.5103298 ]\n",
      " [0.18324757 0.5786189  0.63016874]\n",
      " [0.26189452 0.47804132 0.53090227]\n",
      " [0.21084754 0.49502814 0.60870546]\n",
      " [0.28385985 0.57543695 0.5235598 ]\n",
      " [0.2470607  0.5198563  0.5597714 ]\n",
      " [0.2864994  0.5590855  0.52289206]\n",
      " [0.29712147 0.53140527 0.50938606]\n",
      " [0.41333935 0.5634787  0.4242247 ]\n",
      " [0.14397119 0.5639489  0.67568153]\n",
      " [0.09520072 0.5501407  0.7617571 ]\n",
      " [0.1094766  0.6049261  0.74188906]\n",
      " [0.08687337 0.48676726 0.7591061 ]\n",
      " [0.14662102 0.5432622  0.6749818 ]\n",
      " [0.04830244 0.60592234 0.85417485]\n",
      " [0.16983604 0.6322229  0.66190106]\n",
      " [0.16340244 0.57200795 0.66355205]\n",
      " [0.12393679 0.5154849  0.7008603 ]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "   Iris-setosa       1.00      1.00      1.00        21\n",
      "Iris-versicolo       1.00      0.52      0.69        21\n",
      "Iris-virginica       0.64      1.00      0.78        18\n",
      "\n",
      "      accuracy                           0.83        60\n",
      "     macro avg       0.88      0.84      0.82        60\n",
      "  weighted avg       0.89      0.83      0.83        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# make a prediction\n",
    "testingPredictions = model.predict(x_test)\n",
    "testingPredictions = list(testingPredictions.argmax(axis=-1))\n",
    "\n",
    "confidence_scores = model.predict(x_test, batch_size=32)\n",
    "print(confidence_scores)\n",
    "\n",
    "target_names = ['Iris-setosa', 'Iris-versicolo', 'Iris-virginica']\n",
    "print(classification_report(y_test.argmax(axis=-1), testingPredictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "civilian-savage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5311 - accuracy: 0.8333\n",
      "loss: 0.531\n",
      "accuracy: 0.833\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "  print(\"%s: %.3f\" % (name, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-macro",
   "metadata": {},
   "source": [
    "Now, we need to export the data in order to support some interactive visualizations that we've created. Feel free to skip over this code block and move to the interactive visualizations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "recreational-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "output_directory = \"libraries/stored_results\"\n",
    "output_filename = \"logistic_regression.json\"\n",
    "full_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "data = {}\n",
    "for i in range(len(epoch_output[0])):\n",
    "    data[i] = {}\n",
    "    data[i]['Num Epochs'] = len(epoch_output)\n",
    "    data[i]['Index'] = i\n",
    "    data[i]['Test Label'] = int(epoch_output[0]['actual'][i].argmax())\n",
    "    data[i]['Test Prediction'] = {}\n",
    "    data[i]['Test Confidence Score'] = {}\n",
    "#     data[i]['Intermediate Values'] = {}\n",
    "    for j in range(len(epoch_output)):\n",
    "        data[i]['Test Prediction'][j] = int(epoch_output[j]['prediction'][i])\n",
    "        data[i]['Test Confidence Score'][j] = epoch_output[j]['confidence_score'][i].tolist()\n",
    "#         data[i]['Intermediate Values'][j] = epoch_output[j]['intermediate_values'][i]\n",
    "    data[i]['Test Sentence'] = epoch_output[0]['input'][i].tolist()\n",
    "    \n",
    "with open(full_path, 'w') as outfile:\n",
    "    json.dump(data, outfile, indent=4, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "quarterly-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "output_directory = \"libraries/stored_results/\"\n",
    "collected_epoch_filename = 'logistic_regression.json'\n",
    "full_path_extended = os.path.join(output_directory, collected_epoch_filename)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import libraries.mlvislib as mlvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "enclosed-publicity",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"https://d3js.org/d3.v3.min.js\" charset=\"utf-8\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <style> \n",
       "        body {\n",
       "            font-family: Arial, sans-serif;\n",
       "            font-size: larger;\n",
       "        }\n",
       "        .box_highlighted {\n",
       "            background-color: #ffb;\n",
       "            border: 1px solid #b53;\n",
       "        }\n",
       "        .highlight{\n",
       "            background-color: yellow;\n",
       "        }\n",
       "        .lighthigh{\n",
       "            background-color: green;\n",
       "        }\n",
       "        #testList {                                           /* List in data Section */\n",
       "            list-style-type: none;\n",
       "            padding-left: 0;\n",
       "            margin: 0;\n",
       "        }\n",
       "        li.dataPoint{\n",
       "            font-size: smaller;\n",
       "            overflow: hidden;\n",
       "            white-space: nowrap;\n",
       "            text-overflow: ellipsis;\n",
       "        }\n",
       "        li.dataPoint.clicked{\n",
       "            overflow: visible;\n",
       "            white-space: normal;\n",
       "        }\n",
       "        li.dataPoint:nth-child(odd){                          /* Alternating list item color */\n",
       "            background: #999;\n",
       "        }\n",
       "        table.xaxis {                                         /* X-axis of table */\n",
       "            table-layout: auto;\n",
       "            width: 750px;\n",
       "            margin-left: 26px !important;\n",
       "            text-align: center;\n",
       "        }\n",
       "        td.xlabel {\n",
       "            text-align: center;\n",
       "        }\n",
       "        table.yaxis {                                         /* Y-axis of table */\n",
       "            float: left;\n",
       "            display: inline;\n",
       "            table-layout: auto;\n",
       "            height: 750px;\n",
       "            writing-mode: sideways-lr;\n",
       "        }\n",
       "        td.ylabel {\n",
       "            text-align: center;\n",
       "        }\n",
       "        #review{                                              /* Data Section */\n",
       "            border:1px solid blue;\n",
       "            padding: 5px;\n",
       "            float: left;\n",
       "            width: 750px;\n",
       "            height: 500px;\n",
       "            background-color: white;\n",
       "            margin: 20px;\n",
       "            overflow: scroll;\n",
       "            }\n",
       "        #matrix{\n",
       "            border:1px solid blue;\n",
       "            padding: 5px;\n",
       "            float: left;\n",
       "            width: 750px;\n",
       "            display: inline;\n",
       "        }\n",
       "        #slider {\n",
       "          -webkit-appearance: none;\n",
       "          width: 100%;\n",
       "          height: 15px;\n",
       "          border-radius: 5px;\n",
       "          background: #d3d3d3;\n",
       "          outline: none;\n",
       "          opacity: 0.7;\n",
       "          -webkit-transition: .2s;\n",
       "          transition: opacity .2s;\n",
       "        }\n",
       "        #slider::-webkit-slider-thumb {\n",
       "          -webkit-appearance: none;\n",
       "          appearance: none;\n",
       "          width: 25px;\n",
       "          height: 25px;\n",
       "          border-radius: 50%;\n",
       "          background: #4ca2af;\n",
       "          cursor: pointer;\n",
       "        }\n",
       "        #slider::-moz-range-thumb {\n",
       "          width: 25px;\n",
       "          height: 25px;\n",
       "          border-radius: 50%;\n",
       "          background: #4ca2af;\n",
       "          cursor: pointer;\n",
       "        }\n",
       "     </style>\n",
       "        <h1> Interactive Confusion Matrix </h1>\n",
       "        <h3 id=\"confidence_setting\"> Confidence: 0.5 </h3>\n",
       "        <input class=\"slider\" id=\"confidence_slider\" type=\"range\" min=\"0\" max=\"1\" step=\".1\" value=\".5\"/>\n",
       "        <h3 id=\"epoch_setting\"> Epoch: 1 </h3>\n",
       "        <input class=\"slider\" id=\"epoch_slider\" type=\"range\" min=\"1\" max=\"20\" step=\"1\" value=\"1\"/>\n",
       "        <table class=\"xaxis\">\n",
       "            <tr> </tr>\n",
       "        </table>\n",
       "        <div>\n",
       "        <div>\n",
       "            <div>\n",
       "                <table class=\"yaxis\">\n",
       "                    <tr> </tr>\n",
       "                </table>\n",
       "                <div id=\"matrix\"></div>\n",
       "                <div id=\"review\">\n",
       "                    Data For:\n",
       "                    <ul id = \"testList\"></ul>\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>\n",
       "        <script> \n",
       "        /*--------------------------------------------------------------------------------\n",
       "        Function: extractTypes\n",
       "        Behavior: Identifies what different types each data point can be identified as based\n",
       "            off of the 'true_label' attribute in JSON file.\n",
       "        Input: JSON file\n",
       "        Returns: Returns array of possible values for 'Test Label'.\n",
       "        --------------------------------------------------------------------------------*/\n",
       "        function extractTypes(data){\n",
       "            var lookup = {};\n",
       "            var items = data;\n",
       "            var result = [];\n",
       "            for (var item, i=0; item = items[i++];){\n",
       "                var name = item['Test Label'];\n",
       "                if(!(name in lookup)){\n",
       "                    lookup[name] = 1;\n",
       "                    result.push(name);\n",
       "                }\n",
       "            }\n",
       "            return result.sort();\n",
       "        }\n",
       "\n",
       "        /*--------------------------------------------------------------------------------\n",
       "        Function: fetchDataWindowResults\n",
       "        Behavior: Fetches subset of 'd' variable to be displayed in 'Data' window\n",
       "        Input: 'd' variable, testLabel, predLabel, epoch, conf\n",
       "        Returns: Subset of 'd' variable formatted the same as 'd'\n",
       "        --------------------------------------------------------------------------------*/\n",
       "        function fetchDataWindowResults(d, testLabel, predLabel, epoch, conf){\n",
       "            console.log(\"Visualization: Calling fetchDataWindowResults...\");\n",
       "            var fullDataSet = d;\n",
       "            var selectedTestLabel = testLabel;\n",
       "            var selectedPredictionLabel = predLabel;\n",
       "            var selectedEpoch = epoch;\n",
       "            var selectedConfMin = conf;\n",
       "            var selectedEntries = []\n",
       "            for (const dataPoint of Object.entries(d)){\n",
       "                var currentPrediction = dataPoint[1]['Test Prediction'][epoch];\n",
       "                var currentTestLabel = dataPoint[1]['Test Label'];\n",
       "                var currentSentence = dataPoint[1]['Test Sentence'];\n",
       "                var currentConfScore = dataPoint[1]['Test Confidence Score'][epoch];\n",
       "                var bestConfScore = Math.max.apply(Math, currentConfScore);\n",
       "                if(\n",
       "                currentPrediction == selectedPredictionLabel &&\n",
       "                currentTestLabel == selectedTestLabel &&\n",
       "                bestConfScore >= selectedConfMin){\n",
       "                    selectedEntries.push(dataPoint);\n",
       "                }\n",
       "            }\n",
       "            return selectedEntries;\n",
       "        }\n",
       "\n",
       "        /*--------------------------------------------------------------------------------\n",
       "        Function: addAxisLabels\n",
       "        Behavior: Adds x and y axis components to the DOM and text labels should be\n",
       "            placed next to div for matrix\n",
       "        Input: takes two string arrays, one for the x and one for the y axis\n",
       "        Returns: N/A\n",
       "        --------------------------------------------------------------------------------*/\n",
       "        function addAxisLabels(){\n",
       "            var xaxis_labels = ['0', '1', '2']\n",
       "            var yaxis_labels = ['2', '1', '0']\n",
       "            for(var i = 0; i < xaxis_labels.length; i++){\n",
       "                d3.select(\".xaxis\").selectAll(\"tr\").append(\"td\").text(xaxis_labels[i])\n",
       "                    .classed(\"xlabel\", true);\n",
       "            }\n",
       "            for(var i = 0; i < yaxis_labels.length; i++){\n",
       "                d3.select(\".yaxis\").selectAll(\"tr\").append(\"td\").text(yaxis_labels[i])\n",
       "                    .attr(\"height\", (750 / yaxis_labels.length)).classed(\"ylabel\", true);\n",
       "            }\n",
       "        }\n",
       "        addAxisLabels()                               // Calling function to add axis' to the DOM\n",
       "\n",
       "        /*--------------------------------------------------------------------------------\n",
       "        Function: fillMatrix\n",
       "        Behavior: Does the work of actually placing selected datapoints into 'rect' items\n",
       "            and inserting them into the matrix svg (redefines 'rect' variable)\n",
       "        Input: None\n",
       "        Output: None\n",
       "        --------------------------------------------------------------------------------*/\n",
       "        function fillMatrix(){\n",
       "            console.log(\"Debugging: Filling Matrix...\");\n",
       "            rect = svg.selectAll(\"rect\")                                                                                       // Defining rect as child of SVG\n",
       "                          .data(datasubset)\n",
       "                          .enter()\n",
       "                          .append(\"rect\");\n",
       "            /*--------------------------------------------------------------------------------\n",
       "            Format: d[trueLabel, predictedLabel, entryText, index, conf_scores]\n",
       "            --------------------------------------------------------------------------------*/\n",
       "            rect.attr(\"x\", function (d){                                                                                       // Define x coordinate to place rect\n",
       "                    var matrixnum = (parseInt(d[1][currentEpochSetting -1]) * tableDimension) + parseInt(d[0]);\n",
       "                    var inmatrixcol = counters[matrixnum] % blockStackDimension;\n",
       "                    counters[matrixnum]++;\n",
       "                    return (d[1][currentEpochSetting -1] * (cellDimension + marginBuffer)) + (inmatrixcol * (cubeDimension));\n",
       "                })\n",
       "                .attr(\"y\", function(d){                                                                                        // Define y coordinate to place rect\n",
       "                    var matrixnum = (parseInt(d[1][currentEpochSetting -1] * tableDimension) + parseInt(d[0]));\n",
       "                    var hm = Math.floor(ycounters[matrixnum]/blockStackDimension);\n",
       "                    ycounters[matrixnum]++;\n",
       "                    return (d[0] * (cellDimension + marginBuffer)) + (hm * (cubeDimension));\n",
       "                })\n",
       "                .attr(\"id\", function(d){                                                                                       // Define unique id of rect\n",
       "                    return \"rect\" + d[3];\n",
       "                })\n",
       "                .attr(\"width\", function(d){                                                                                    // Define width of rect\n",
       "                    return cubeDimension;\n",
       "                })\n",
       "                .attr(\"height\", function(d){                                                                                   // Define height of rect\n",
       "                    return cubeDimension;\n",
       "                })\n",
       "                .attr(\"opacity\", function(d){                                                                                  // Define opacity of rect\n",
       "                    return 1;\n",
       "                })\n",
       "                .attr(\"fill\", function(d){                                                                                     // Define color of rect\n",
       "                    return (\"black\");\n",
       "                })\n",
       "                .attr(\"class\", function(d){                                                                                    // Define class of rect ( currently unused )\n",
       "                    predicted_label = \"predicted_label_\" + d[1][currentEpochSetting -1];\n",
       "                    true_label = \"true_label_\" + d[0];\n",
       "                    return true_label + \" \" + predicted_label;\n",
       "            });\n",
       "        }\n",
       "\n",
       "        /*--------------------------------------------------------------------------------\n",
       "        Function: clickedRect\n",
       "        Behavior: Activates when a rect is clicked, will find all rect's in same matrix\n",
       "            cell and color these blue, and all other cells black. This will then make a\n",
       "            call to fetchDataWindowResults to provide entries to the data window that\n",
       "            correspond with the entries to this cell\n",
       "        Input: Reference to clicked rect as well as the entire dataset to parse through\n",
       "        Returns: N/A\n",
       "        --------------------------------------------------------------------------------*/\n",
       "        function clickedRect(d_on, d){\n",
       "            var actual = d_on[0];\n",
       "            var prediction = d_on[1][currentEpochSetting-1];\n",
       "            var selectedDataSet = fetchDataWindowResults(d, actual, prediction,                                                 // Fetch data points for selected cell\n",
       "                (currentEpochSetting - 1), currentConfSetting);\n",
       "            d3.selectAll('rect').style('fill', \"black\");                                                                        // Selecting all rects and coloring black\n",
       "            d3.selectAll('rect')                                                                                                // Coloring rects in selected quadrent blue\n",
       "                .filter(function(d) {\n",
       "                    if( d[0] == actual && d[1][currentEpochSetting-1] == prediction)\n",
       "                        return 1;\n",
       "                    else\n",
       "                        return 0;\n",
       "                })\n",
       "                .style('fill', \"blue\");\n",
       "            var data_section_title = \"Data for: Label (\" + d_on[0] + \") Prediction (\" + d_on[1][currentEpochSetting-1] + \")\";\n",
       "            d3.select('#review').text(data_section_title);                                                                      // Updating title of 'Data' window\n",
       "            d3.select('#review').append(\"ul\").attr(\"id\", \"testList\")                                                            // Creating a new list to display\n",
       "            d3.select(\"#testList\").selectAll(\"li\").remove();                                                                    // Removing all old list items\n",
       "            for (var i = 0; i < selectedDataSet.length; i++){                                                                   // Add list entries for 'Sentence' and Confidence score of selected data\n",
       "                var tableRowData = selectedDataSet[i][1];\n",
       "                var dataPointString = \" Confidence Score: \" +  tableRowData['Test Confidence Score'][currentEpochSetting - 1] +\n",
       "                    \" Input Data: \" + tableRowData['Test Sentence']\n",
       "                d3.select(\"#testList\").append(\"li\").text(dataPointString).classed(\"dataPoint\", true);\n",
       "            }\n",
       "            d3.selectAll(\".dataPoint\").on('click', function(){                                                                  // View overflowed data on click\n",
       "                d3.selectAll(\".dataPoint\").classed(\"clicked\", false);\n",
       "                d3.select(this).classed(\"clicked\", true);\n",
       "            });\n",
       "        }\n",
       "\n",
       "        /*--------------------------------------------------------------------------------\n",
       "        Function: emptyMatrix\n",
       "        Behavior: Empties out entire confusion matrix of any rect's, as well as resets\n",
       "            counters used when placing in new rects\n",
       "        Input: N/A\n",
       "        Returns: N/A\n",
       "        --------------------------------------------------------------------------------*/\n",
       "        function emptyMatrix(){\n",
       "            counters = new Array(tableDimension * tableDimension).fill(0);\n",
       "            ycounters = new Array(tableDimension * tableDimension).fill(0);\n",
       "            svg.selectAll(\"*\").remove();\n",
       "        }\n",
       "\n",
       "        /*--------------------------------------------------------------------------------\n",
       "        Function: refineChoice\n",
       "        Behavior: Filters the dataset referenced when adding rect's to the matrix to only\n",
       "            include data that has a confidence score greater than that selected on the\n",
       "            slider.\n",
       "        Input: N/A\n",
       "        Returns: N/A\n",
       "        --------------------------------------------------------------------------------*/\n",
       "        function refineChoice(){\n",
       "            datasubset = [];\n",
       "            for( var i = 0; i < dataset.length; i++ ){\n",
       "                datapoint = dataset[i];\n",
       "                cScore = Math.max.apply(Math, dataset[i][4][currentEpochSetting-1]);\n",
       "                if( cScore >= currentConfSetting ){\n",
       "                    datasubset.push(datapoint);\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "\n",
       "        /*--------------------------------------------------------------------------------\n",
       "        GLOBAL VARIABLES: DECLARATIONS\n",
       "        --------------------------------------------------------------------------------*/\n",
       "        console.log(\"Visualization: Running JavaScript...\");\n",
       "        var dname = \"libraries/stored_results/logistic_regression.json\";              // Path to local JSON file storing data\n",
       "        var rawJSONData = null;                       // Data from JSON read operation\n",
       "        var currentConfSetting = .5;                  // Set confidence score minimum to default\n",
       "        var currentEpochSetting = 1;                  // Set epoch setting to default\n",
       "        var lastEpochIndex = 0;                       // Max value that epoch slider can reach\n",
       "        var totalItems = null;                        // Total number of boxes that should appear\n",
       "        var possibleOutputValues = null;              // Possible values that predictions may produce\n",
       "        var tableDimension = null;                    // Dimensions of table (square so x and y are same)\n",
       "        var dataset = []                              // JSON is read into this array\n",
       "        var datasubset = []                           // Selected values from dataset\n",
       "        var table = null;                             // Reference for how many boxes should be in each matrix cell\n",
       "        var svg = null;                               // SVG graphic\n",
       "        var rect = null;                              // Rectangles which are put into above graphic\n",
       "        var w = 750;                                  // Width of matrix\n",
       "        var h = 750;                                  // Height of matrix\n",
       "        var counters = null;                          // Current 'x' position when placing rects\n",
       "        var ycounters = null;                         // Curent 'y' position when placing rects\n",
       "        var cellDimension = null;                     // Height and width to make each cell on matrix\n",
       "        var blockStackDimension = null;               // How many cubes to place on a single row in matrix\n",
       "        var marginBuffer = null;                      // How much room to leave between cells in matrix\n",
       "        var cubeDimension = null;                     // Dimensions of each rect\n",
       "\n",
       "        /*--------------------------------------------------------------------------------\n",
       "        Creating Visualization / Main\n",
       "        --------------------------------------------------------------------------------*/\n",
       "        d3.json( \"libraries/stored_results/logistic_regression.json\", function(d) {\n",
       "            rawJSONData = d;                                                                               // Storing JSON after read operation\n",
       "            /*--------------------------------------------------------------------------------\n",
       "            GLOBAL VARIABLES: DEFINITIONS\n",
       "            --------------------------------------------------------------------------------*/\n",
       "            totalItems = Object.keys(d).length                                                             // Defining totalItems\n",
       "            possibleOutputValues = extractTypes(d);                                                        // Defining possibleOutputValues\n",
       "            tableDimension = possibleOutputValues.length;                                                  // Defining tableDimension\n",
       "            /* NOTE: LIMITS AMOUNT OF DATA USED\n",
       "            if( totalItems > 2000 ){\n",
       "                var sliced = [];\n",
       "                for( var i = 0; i < 2000; i++ ){\n",
       "                    sliced[i] = rawJSONData[i];\n",
       "                }\n",
       "                rawJSONData = sliced;\n",
       "                d = sliced;\n",
       "                totalItems = 2000;\n",
       "            }\n",
       "            */\n",
       "            table = new Array(tableDimension);                                                             // Initializing table\n",
       "            for(var i=0; i<tableDimension; i++){\n",
       "                table[i] = new Array(tableDimension);\n",
       "                for(var j=0; j<tableDimension; j++){\n",
       "                    table[i][j] = 0;\n",
       "                }\n",
       "            }\n",
       "            lastEpochIndex = d[0]['Num Epochs']                                                            // Defining lastEpochIndex\n",
       "            d3.select(\"#epoch_slider\").attr(\"max\", lastEpochIndex);                                        // Embedding lastEpochIndex\n",
       "            for(var jsonEntry, i=0; jsonEntry = d[i++];){                                                  // Storing JSON data in memory\n",
       "                var index = i;\n",
       "                var entryText = jsonEntry[\"Test Sentence\"];\n",
       "                var confidenceScore = jsonEntry[\"Test Confidence Score\"];\n",
       "                var trueLabel = jsonEntry[\"Test Label\"];\n",
       "                var predictedLabel = jsonEntry[\"Test Prediction\"];\n",
       "                var tableXCoordinate = possibleOutputValues.indexOf(predictedLabel[currentEpochSetting-1]); //Predicted\n",
       "                var tableYCoordinate = possibleOutputValues.indexOf(trueLabel); // Actual\n",
       "                table[tableXCoordinate][tableYCoordinate]+=1;\n",
       "                dataset.push([trueLabel, predictedLabel, entryText, index, confidenceScore]);\n",
       "            }\n",
       "            svg = d3.select(\"body\")                                                                        // Defining svg\n",
       "                        .select(\"#matrix\")\n",
       "                        .append(\"svg\")\n",
       "                        .attr(\"width\", w)\n",
       "                        .attr(\"height\", h);\n",
       "            counters = new Array(tableDimension * tableDimension).fill(0);                                 // Defining conters\n",
       "            ycounters = new Array(tableDimension * tableDimension).fill(0);                                // Defining ycounters\n",
       "            cellDimension = h / tableDimension;                                                            // Defining celldimension\n",
       "            blockStackDimension = Math.round(Math.sqrt(totalItems)) + 1;                                   // Defining blockStackDimension\n",
       "            marginBuffer = 5;                                                                              // Defining marginBuffer\n",
       "            cubeDimension = ((cellDimension - marginBuffer) / blockStackDimension);                        // Defining cubeDimension\n",
       "            /*--------------------------------------------------------------------------------\n",
       "            INITIALIZING MATRIX\n",
       "            --------------------------------------------------------------------------------*/\n",
       "            refineChoice();                                                                                // Filter based on default confidence score\n",
       "            fillMatrix();                                                                                  // Place rects on matrix\n",
       "            rect.on(\"click\",function(d_on){ clickedRect(d_on, d) });                                       // Define 'click' behavior\n",
       "            /*--------------------------------------------------------------------------------\n",
       "            Function: Slider Re-Draw\n",
       "            Behavior: This d3 code will redraw each time there is a change in the slider.\n",
       "            Input: None\n",
       "            Returns: N/A\n",
       "            --------------------------------------------------------------------------------*/\n",
       "            d3.selectAll(\".slider\").on(\"change\", function() {\n",
       "                if(this.id == \"confidence_slider\"){\n",
       "                    currentConfSetting = this.value;\n",
       "                }\n",
       "                if(this.id == \"epoch_slider\"){\n",
       "                    currentEpochSetting = this.value;\n",
       "                }\n",
       "                d3.select(\"#confidence_setting\").text(\"Confidence: \" + currentConfSetting);                    // Update confidence slider header to reflect change\n",
       "                d3.select(\"#epoch_setting\").text(\"Epoch: \" + currentEpochSetting);                             // Update epoch slider header to reflect change\n",
       "                emptyMatrix();                                                                                 // Remove all current rects from matrix\n",
       "                refineChoice();                                                                                // Filter data for changes in min confidence score\n",
       "                fillMatrix();                                                                                  // Place new set of datapoints in matrix\n",
       "                rect.on(\"click\",function(d_on){ clickedRect(d_on, rawJSONData) });                             // Re-assign click function to rects\n",
       "            })\n",
       "        });\n",
       "\n",
       "     </script>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = mlvs.ConfusionMatrix(full_path_extended, x_labels=[\"0\", \"1\", \"2\"], y_labels=[\"2\", \"1\", \"0\"])\n",
    "cm.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-canal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
